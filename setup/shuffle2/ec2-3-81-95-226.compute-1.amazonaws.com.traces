--- 1551158138198857 us
  [ 0] java.lang.String.replace
  [ 1] java.net.URLClassLoader$1.run
  [ 2] java.net.URLClassLoader$1.run
  [ 3] java.security.AccessController.doPrivileged
  [ 4] java.net.URLClassLoader.findClass
  [ 5] java.lang.ClassLoader.loadClass
  [ 6] sun.misc.Launcher$AppClassLoader.loadClass
  [ 7] java.lang.ClassLoader.loadClass
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] java.lang.ClassLoader.defineClass1
  [21] java.lang.ClassLoader.defineClass
  [22] java.security.SecureClassLoader.defineClass
  [23] java.net.URLClassLoader.defineClass
  [24] java.net.URLClassLoader.access$100
  [25] java.net.URLClassLoader$1.run
  [26] java.net.URLClassLoader$1.run
  [27] java.security.AccessController.doPrivileged
  [28] java.net.URLClassLoader.findClass
  [29] java.lang.ClassLoader.loadClass
  [30] sun.misc.Launcher$AppClassLoader.loadClass
  [31] java.lang.ClassLoader.loadClass
  [32] java.lang.ClassLoader.defineClass1
  [33] java.lang.ClassLoader.defineClass
  [34] java.security.SecureClassLoader.defineClass
  [35] java.net.URLClassLoader.defineClass
  [36] java.net.URLClassLoader.access$100
  [37] java.net.URLClassLoader$1.run
  [38] java.net.URLClassLoader$1.run
  [39] java.security.AccessController.doPrivileged
  [40] java.net.URLClassLoader.findClass
  [41] java.lang.ClassLoader.loadClass
  [42] sun.misc.Launcher$AppClassLoader.loadClass
  [43] java.lang.ClassLoader.loadClass
  [44] java.lang.ClassLoader.defineClass1
  [45] java.lang.ClassLoader.defineClass
  [46] java.security.SecureClassLoader.defineClass
  [47] java.net.URLClassLoader.defineClass
  [48] java.net.URLClassLoader.access$100
  [49] java.net.URLClassLoader$1.run
  [50] java.net.URLClassLoader$1.run
  [51] java.security.AccessController.doPrivileged
  [52] java.net.URLClassLoader.findClass
  [53] java.lang.ClassLoader.loadClass
  [54] sun.misc.Launcher$AppClassLoader.loadClass
  [55] java.lang.ClassLoader.loadClass
  [56] java.lang.Class.getDeclaredMethods0
  [57] java.lang.Class.privateGetDeclaredMethods
  [58] java.lang.Class.privateGetMethodRecursive
  [59] java.lang.Class.getMethod0
  [60] java.lang.Class.getMethod
  [61] sun.launcher.LauncherHelper.validateMainClass
  [62] sun.launcher.LauncherHelper.checkAndLoadMain
  [63] [DestroyJavaVM tid=16065]

--- 1551158138205300 us
  [ 0] __radix_tree_lookup_[k]
  [ 1] radix_tree_lookup_slot_[k]
  [ 2] find_get_entry_[k]
  [ 3] pagecache_get_page_[k]
  [ 4] generic_file_read_iter_[k]
  [ 5] xfs_file_buffered_aio_read	[xfs]_[k]
  [ 6] xfs_file_read_iter	[xfs]_[k]
  [ 7] __vfs_read_[k]
  [ 8] vfs_read_[k]
  [ 9] sys_read_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] read
  [13] java.util.zip.ZipFile.read
  [14] java.util.zip.ZipFile.access$1400
  [15] java.util.zip.ZipFile$ZipFileInputStream.read
  [16] java.util.zip.ZipFile$ZipFileInflaterInputStream.fill
  [17] java.util.zip.InflaterInputStream.read
  [18] sun.misc.Resource.getBytes
  [19] java.net.URLClassLoader.defineClass
  [20] java.net.URLClassLoader.access$100
  [21] java.net.URLClassLoader$1.run
  [22] java.net.URLClassLoader$1.run
  [23] java.security.AccessController.doPrivileged
  [24] java.net.URLClassLoader.findClass
  [25] java.lang.ClassLoader.loadClass
  [26] sun.misc.Launcher$AppClassLoader.loadClass
  [27] java.lang.ClassLoader.loadClass
  [28] java.lang.ClassLoader.defineClass1
  [29] java.lang.ClassLoader.defineClass
  [30] java.security.SecureClassLoader.defineClass
  [31] java.net.URLClassLoader.defineClass
  [32] java.net.URLClassLoader.access$100
  [33] java.net.URLClassLoader$1.run
  [34] java.net.URLClassLoader$1.run
  [35] java.security.AccessController.doPrivileged
  [36] java.net.URLClassLoader.findClass
  [37] java.lang.ClassLoader.loadClass
  [38] sun.misc.Launcher$AppClassLoader.loadClass
  [39] java.lang.ClassLoader.loadClass
  [40] java.lang.Class.getDeclaredMethods0
  [41] java.lang.Class.privateGetDeclaredMethods
  [42] java.lang.Class.privateGetMethodRecursive
  [43] java.lang.Class.getMethod0
  [44] java.lang.Class.getMethod
  [45] sun.launcher.LauncherHelper.validateMainClass
  [46] sun.launcher.LauncherHelper.checkAndLoadMain
  [47] [DestroyJavaVM tid=16065]

--- 1551158138307835 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] scala.collection.AbstractTraversable.<init>
  [13] scala.collection.AbstractIterable.<init>
  [14] scala.collection.AbstractMap.<init>
  [15] scala.collection.mutable.AbstractMap.<init>
  [16] scala.collection.convert.Wrappers$JMapWrapper.<init>
  [17] scala.collection.convert.WrapAsScala$class.mapAsScalaMap
  [18] scala.collection.convert.WrapAsScala$.mapAsScalaMap
  [19] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [20] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [21] scala.collection.convert.Decorators$AsScala.asScala
  [22] scala.sys.package$.env
  [23] org.apache.spark.util.Utils$.<init>
  [24] org.apache.spark.util.Utils$.<clinit>
  [25] org.apache.spark.internal.Logging$.<init>
  [26] org.apache.spark.internal.Logging$.<clinit>
  [27] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [28] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [29] org.apache.spark.deploy.SparkSubmit.doSubmit
  [30] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [31] org.apache.spark.deploy.SparkSubmit$.main
  [32] org.apache.spark.deploy.SparkSubmit.main
  [33] [DestroyJavaVM tid=16065]

--- 1551158138313684 us
  [ 0] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle, Thread*)
  [ 1] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle)
  [ 2] JVM_FillInStackTrace
  [ 3] Java_java_lang_Throwable_fillInStackTrace
  [ 4] java.lang.Throwable.fillInStackTrace
  [ 5] java.lang.Throwable.fillInStackTrace
  [ 6] java.lang.Throwable.<init>
  [ 7] java.lang.Exception.<init>
  [ 8] java.lang.ReflectiveOperationException.<init>
  [ 9] java.lang.ClassNotFoundException.<init>
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.collection.AbstractIterable.<init>
  [16] scala.collection.AbstractMap.<init>
  [17] scala.collection.mutable.AbstractMap.<init>
  [18] scala.collection.convert.Wrappers$JMapWrapper.<init>
  [19] scala.collection.convert.WrapAsScala$class.mapAsScalaMap
  [20] scala.collection.convert.WrapAsScala$.mapAsScalaMap
  [21] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [22] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [23] scala.collection.convert.Decorators$AsScala.asScala
  [24] scala.sys.package$.env
  [25] org.apache.spark.util.Utils$.<init>
  [26] org.apache.spark.util.Utils$.<clinit>
  [27] org.apache.spark.internal.Logging$.<init>
  [28] org.apache.spark.internal.Logging$.<clinit>
  [29] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [30] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [31] org.apache.spark.deploy.SparkSubmit.doSubmit
  [32] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$.main
  [34] org.apache.spark.deploy.SparkSubmit.main
  [35] [DestroyJavaVM tid=16065]

--- 1551158138407782 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] scala.collection.mutable.HashTable$class.$init$
  [13] scala.collection.mutable.HashMap.<init>
  [14] scala.collection.mutable.HashMap.<init>
  [15] scala.collection.mutable.Map$.empty
  [16] scala.collection.mutable.Map$.empty
  [17] scala.collection.generic.MutableMapFactory.newBuilder
  [18] scala.collection.generic.GenMapFactory.apply
  [19] scala.sys.SystemProperties$.propertyHelp$lzycompute
  [20] scala.sys.SystemProperties$.propertyHelp
  [21] scala.sys.SystemProperties$.addHelp
  [22] scala.sys.SystemProperties$.bool
  [23] scala.sys.SystemProperties$.noTraceSupression$lzycompute
  [24] scala.sys.SystemProperties$.noTraceSupression
  [25] scala.util.control.NoStackTrace$.<init>
  [26] scala.util.control.NoStackTrace$.<clinit>
  [27] scala.util.control.NoStackTrace$class.fillInStackTrace
  [28] scala.util.control.BreakControl.fillInStackTrace
  [29] java.lang.Throwable.<init>
  [30] scala.util.control.BreakControl.<init>
  [31] scala.util.control.Breaks.<init>
  [32] scala.collection.Traversable$.<init>
  [33] scala.collection.Traversable$.<clinit>
  [34] scala.package$.<init>
  [35] scala.package$.<clinit>
  [36] scala.Predef$.<init>
  [37] scala.Predef$.<clinit>
  [38] scala.collection.immutable.Map$Map4.updated
  [39] scala.collection.immutable.Map$Map4.$plus
  [40] scala.collection.immutable.Map$Map4.$plus
  [41] scala.collection.mutable.MapBuilder.$plus$eq
  [42] scala.collection.mutable.MapBuilder.$plus$eq
  [43] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [44] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [45] scala.collection.mutable.ResizableArray$class.foreach
  [46] scala.collection.mutable.ArrayBuffer.foreach
  [47] scala.collection.generic.Growable$class.$plus$plus$eq
  [48] scala.collection.mutable.MapBuilder.$plus$plus$eq
  [49] scala.collection.generic.GenMapFactory.apply
  [50] scala.sys.package$.env
  [51] org.apache.spark.util.Utils$.<init>
  [52] org.apache.spark.util.Utils$.<clinit>
  [53] org.apache.spark.internal.Logging$.<init>
  [54] org.apache.spark.internal.Logging$.<clinit>
  [55] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [56] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [57] org.apache.spark.deploy.SparkSubmit.doSubmit
  [58] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [59] org.apache.spark.deploy.SparkSubmit$.main
  [60] org.apache.spark.deploy.SparkSubmit.main
  [61] [DestroyJavaVM tid=16065]

--- 1551158138413606 us
  [ 0] __do_page_fault_[k]
  [ 1] page_fault_[k]
  [ 2] java.util.zip.ZipCoder.getBytes
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.sys.SystemProperties.contains
  [16] scala.sys.SystemProperties.contains
  [17] scala.sys.PropImpl.isSet
  [18] scala.sys.PropImpl.value
  [19] scala.sys.BooleanProp$BooleanPropImpl.value
  [20] scala.util.control.NoStackTrace$.<init>
  [21] scala.util.control.NoStackTrace$.<clinit>
  [22] scala.util.control.NoStackTrace$class.fillInStackTrace
  [23] scala.util.control.BreakControl.fillInStackTrace
  [24] java.lang.Throwable.<init>
  [25] scala.util.control.BreakControl.<init>
  [26] scala.util.control.Breaks.<init>
  [27] scala.collection.Traversable$.<init>
  [28] scala.collection.Traversable$.<clinit>
  [29] scala.package$.<init>
  [30] scala.package$.<clinit>
  [31] scala.Predef$.<init>
  [32] scala.Predef$.<clinit>
  [33] scala.collection.immutable.Map$Map4.updated
  [34] scala.collection.immutable.Map$Map4.$plus
  [35] scala.collection.immutable.Map$Map4.$plus
  [36] scala.collection.mutable.MapBuilder.$plus$eq
  [37] scala.collection.mutable.MapBuilder.$plus$eq
  [38] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [39] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [40] scala.collection.mutable.ResizableArray$class.foreach
  [41] scala.collection.mutable.ArrayBuffer.foreach
  [42] scala.collection.generic.Growable$class.$plus$plus$eq
  [43] scala.collection.mutable.MapBuilder.$plus$plus$eq
  [44] scala.collection.generic.GenMapFactory.apply
  [45] scala.sys.package$.env
  [46] org.apache.spark.util.Utils$.<init>
  [47] org.apache.spark.util.Utils$.<clinit>
  [48] org.apache.spark.internal.Logging$.<init>
  [49] org.apache.spark.internal.Logging$.<clinit>
  [50] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [51] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [52] org.apache.spark.deploy.SparkSubmit.doSubmit
  [53] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [54] org.apache.spark.deploy.SparkSubmit$.main
  [55] org.apache.spark.deploy.SparkSubmit.main
  [56] [DestroyJavaVM tid=16065]

--- 1551158138508792 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] scala.collection.immutable.Set$.emptyInstance
  [13] scala.collection.generic.ImmutableSetFactory.empty
  [14] org.apache.spark.util.Utils$.<init>
  [15] org.apache.spark.util.Utils$.<clinit>
  [16] org.apache.spark.internal.Logging$.<init>
  [17] org.apache.spark.internal.Logging$.<clinit>
  [18] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [19] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [20] org.apache.spark.deploy.SparkSubmit.doSubmit
  [21] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [22] org.apache.spark.deploy.SparkSubmit$.main
  [23] org.apache.spark.deploy.SparkSubmit.main
  [24] [DestroyJavaVM tid=16065]

--- 1551158138514618 us
  [ 0] ZIP_Lock
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.util.Utils$.getContextOrSparkClassLoader
  [16] org.apache.spark.util.Utils$.classForName
  [17] org.apache.spark.internal.Logging$.<init>
  [18] org.apache.spark.internal.Logging$.<clinit>
  [19] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [20] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [21] org.apache.spark.deploy.SparkSubmit.doSubmit
  [22] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [23] org.apache.spark.deploy.SparkSubmit$.main
  [24] org.apache.spark.deploy.SparkSubmit.main
  [25] [DestroyJavaVM tid=16065]

--- 1551158138626735 us
  [ 0] vtable stub
  [ 1] java.util.concurrent.ConcurrentHashMap.putVal
  [ 2] java.util.concurrent.ConcurrentHashMap.putIfAbsent
  [ 3] java.lang.ClassLoader.getClassLoadingLock
  [ 4] java.lang.ClassLoader.loadClass
  [ 5] sun.misc.Launcher$AppClassLoader.loadClass
  [ 6] java.lang.ClassLoader.loadClass
  [ 7] java.lang.Class.getDeclaredMethods0
  [ 8] java.lang.Class.privateGetDeclaredMethods
  [ 9] java.lang.Class.privateGetPublicMethods
  [10] java.lang.Class.getMethods
  [11] java.beans.Introspector.getPublicDeclaredMethods
  [12] java.beans.Introspector.getTargetMethodInfo
  [13] java.beans.Introspector.getBeanInfo
  [14] java.beans.Introspector.getBeanInfo
  [15] java.beans.Introspector.getBeanInfo
  [16] java.beans.Introspector.<init>
  [17] java.beans.Introspector.getBeanInfo
  [18] java.beans.Introspector.getBeanInfo
  [19] java.beans.Introspector.<init>
  [20] java.beans.Introspector.getBeanInfo
  [21] org.apache.log4j.config.PropertySetter.introspect
  [22] org.apache.log4j.config.PropertySetter.getPropertyDescriptor
  [23] org.apache.log4j.config.PropertySetter.setProperties
  [24] org.apache.log4j.config.PropertySetter.setProperties
  [25] org.apache.log4j.PropertyConfigurator.parseAppender
  [26] org.apache.log4j.PropertyConfigurator.parseCategory
  [27] org.apache.log4j.PropertyConfigurator.configureRootCategory
  [28] org.apache.log4j.PropertyConfigurator.doConfigure
  [29] org.apache.log4j.PropertyConfigurator.doConfigure
  [30] org.apache.log4j.helpers.OptionConverter.selectAndConfigure
  [31] org.apache.log4j.LogManager.<clinit>
  [32] org.apache.spark.internal.Logging$class.initializeLogging
  [33] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [34] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [35] org.apache.spark.deploy.SparkSubmit.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [37] org.apache.spark.deploy.SparkSubmit$.main
  [38] org.apache.spark.deploy.SparkSubmit.main
  [39] [DestroyJavaVM tid=16065]

--- 1551158138632890 us
  [ 0] do_syscall_64_[k]
  [ 1] entry_SYSCALL_64_after_hwframe_[k]
  [ 2] __vdso_clock_gettime
  [ 3] __GI___clock_gettime
  [ 4] [unknown]
  [ 5] java.lang.ClassLoader.findBootstrapClass
  [ 6] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 7] java.lang.ClassLoader.loadClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] sun.misc.Launcher$AppClassLoader.loadClass
  [10] java.lang.ClassLoader.loadClass
  [11] org.apache.log4j.WriterAppender.setWriter
  [12] org.apache.log4j.ConsoleAppender.activateOptions
  [13] org.apache.log4j.config.PropertySetter.activate
  [14] org.apache.log4j.config.PropertySetter.setProperties
  [15] org.apache.log4j.config.PropertySetter.setProperties
  [16] org.apache.log4j.PropertyConfigurator.parseAppender
  [17] org.apache.log4j.PropertyConfigurator.parseCategory
  [18] org.apache.log4j.PropertyConfigurator.configureRootCategory
  [19] org.apache.log4j.PropertyConfigurator.doConfigure
  [20] org.apache.log4j.PropertyConfigurator.doConfigure
  [21] org.apache.log4j.helpers.OptionConverter.selectAndConfigure
  [22] org.apache.log4j.LogManager.<clinit>
  [23] org.apache.spark.internal.Logging$class.initializeLogging
  [24] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [25] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16065]

--- 1551158138737338 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments
  [13] org.apache.spark.deploy.SparkSubmitArguments.validateArguments
  [14] org.apache.spark.deploy.SparkSubmitArguments.<init>
  [15] org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$1.<init>
  [16] org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments
  [17] org.apache.spark.deploy.SparkSubmit.doSubmit
  [18] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [19] org.apache.spark.deploy.SparkSubmit$.main
  [20] org.apache.spark.deploy.SparkSubmit.main
  [21] [DestroyJavaVM tid=16065]

--- 1551158138743375 us
  [ 0] InstanceKlass::InstanceKlass(int, int, int, int, ReferenceType, AccessFlags, bool)
  [ 1] InstanceKlass::allocate_instance_klass(ClassLoaderData*, int, int, int, int, ReferenceType, AccessFlags, Symbol*, Klass*, bool, Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 4] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 5] JVM_DefineClassWithSource
  [ 6] Java_java_lang_ClassLoader_defineClass1
  [ 7] java.lang.ClassLoader.defineClass1
  [ 8] java.lang.ClassLoader.defineClass
  [ 9] java.security.SecureClassLoader.defineClass
  [10] java.net.URLClassLoader.defineClass
  [11] java.net.URLClassLoader.access$100
  [12] java.net.URLClassLoader$1.run
  [13] java.net.URLClassLoader$1.run
  [14] java.security.AccessController.doPrivileged
  [15] java.net.URLClassLoader.findClass
  [16] java.lang.ClassLoader.loadClass
  [17] sun.misc.Launcher$AppClassLoader.loadClass
  [18] java.lang.ClassLoader.loadClass
  [19] org.apache.spark.SparkConf.loadFromSystemProperties
  [20] org.apache.spark.SparkConf.<init>
  [21] org.apache.spark.SparkConf.<init>
  [22] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [23] org.apache.spark.deploy.SparkSubmit.submit
  [24] org.apache.spark.deploy.SparkSubmit.doSubmit
  [25] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [26] org.apache.spark.deploy.SparkSubmit$.main
  [27] org.apache.spark.deploy.SparkSubmit.main
  [28] [DestroyJavaVM tid=16065]

--- 1551158138862902 us
  [ 0] sun.misc.URLClassPath.getNextLoader
  [ 1] sun.misc.URLClassPath.getResource
  [ 2] java.net.URLClassLoader$1.run
  [ 3] java.net.URLClassLoader$1.run
  [ 4] java.security.AccessController.doPrivileged
  [ 5] java.net.URLClassLoader.findClass
  [ 6] java.lang.ClassLoader.loadClass
  [ 7] sun.misc.Launcher$AppClassLoader.loadClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] org.apache.ivy.plugins.resolver.BasicResolver.<init>
  [10] org.apache.ivy.plugins.resolver.AbstractPatternsBasedResolver.<init>
  [11] org.apache.ivy.plugins.resolver.RepositoryResolver.<init>
  [12] org.apache.ivy.plugins.resolver.URLResolver.<init>
  [13] org.apache.ivy.plugins.resolver.IBiblioResolver.<init>
  [14] org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers
  [15] org.apache.spark.deploy.SparkSubmitUtils$.buildIvySettings
  [16] org.apache.spark.deploy.DependencyUtils$.resolveMavenDependencies
  [17] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [18] org.apache.spark.deploy.SparkSubmit.submit
  [19] org.apache.spark.deploy.SparkSubmit.doSubmit
  [20] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [21] org.apache.spark.deploy.SparkSubmit$.main
  [22] org.apache.spark.deploy.SparkSubmit.main
  [23] [DestroyJavaVM tid=16065]

--- 1551158138869224 us
  [ 0] jvmti_GetClassMethods
  [ 1] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 2] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 3] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 4] InstanceKlass::initialize(Thread*)
  [ 5] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 6] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [ 7] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 8] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [ 9] java.net.InetAddress$Cache.getPolicy
  [10] java.net.InetAddress$Cache.put
  [11] java.net.InetAddress.cacheInitIfNeeded
  [12] java.net.InetAddress.cacheAddresses
  [13] java.net.InetAddress.getAddressesFromNameService
  [14] java.net.InetAddress.getLocalHost
  [15] org.apache.ivy.util.HostUtil.getLocalHostName
  [16] org.apache.ivy.plugins.resolver.BasicResolver.<init>
  [17] org.apache.ivy.plugins.resolver.AbstractPatternsBasedResolver.<init>
  [18] org.apache.ivy.plugins.resolver.RepositoryResolver.<init>
  [19] org.apache.ivy.plugins.resolver.URLResolver.<init>
  [20] org.apache.ivy.plugins.resolver.IBiblioResolver.<init>
  [21] org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers
  [22] org.apache.spark.deploy.SparkSubmitUtils$.buildIvySettings
  [23] org.apache.spark.deploy.DependencyUtils$.resolveMavenDependencies
  [24] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [25] org.apache.spark.deploy.SparkSubmit.submit
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16065]

--- 1551158138963361 us
  [ 0] [DestroyJavaVM tid=16065]

--- 1551158138970266 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [21] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [22] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [23] scala.Option.getOrElse
  [24] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [25] org.apache.spark.deploy.SparkSubmit.submit
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16065]

--- 1551158139064160 us
  [ 0] java.util.zip.ZipFile.getEntry
  [ 1] java.util.jar.JarFile.getEntry
  [ 2] java.util.jar.JarFile.getJarEntry
  [ 3] sun.misc.URLClassPath$JarLoader.getResource
  [ 4] sun.misc.URLClassPath.getResource
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] org.apache.xerces.parsers.XML11Configuration.<init>
  [13] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [14] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] java.lang.Class.newInstance
  [20] org.apache.xerces.parsers.ObjectFactory.newInstance
  [21] org.apache.xerces.parsers.ObjectFactory.createObject
  [22] org.apache.xerces.parsers.ObjectFactory.createObject
  [23] org.apache.xerces.parsers.DOMParser.<init>
  [24] org.apache.xerces.parsers.DOMParser.<init>
  [25] org.apache.xerces.jaxp.DocumentBuilderImpl.<init>
  [26] org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder
  [27] org.apache.hadoop.conf.Configuration.loadResource
  [28] org.apache.hadoop.conf.Configuration.loadResources
  [29] org.apache.hadoop.conf.Configuration.getProps
  [30] org.apache.hadoop.conf.Configuration.set
  [31] org.apache.hadoop.conf.Configuration.set
  [32] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [33] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [34] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [35] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [36] scala.Option.getOrElse
  [37] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [38] org.apache.spark.deploy.SparkSubmit.submit
  [39] org.apache.spark.deploy.SparkSubmit.doSubmit
  [40] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [41] org.apache.spark.deploy.SparkSubmit$.main
  [42] org.apache.spark.deploy.SparkSubmit.main
  [43] [DestroyJavaVM tid=16065]

--- 1551158139070072 us
  [ 0] java.lang.String.equals
  [ 1] java.util.ArrayList.indexOf
  [ 2] java.util.ArrayList.contains
  [ 3] org.apache.xerces.util.ParserConfigurationSettings.addRecognizedFeatures
  [ 4] org.apache.xerces.parsers.XML11Configuration.addRecognizedParamsAndSetDefaults
  [ 5] org.apache.xerces.parsers.XML11Configuration.addComponent
  [ 6] org.apache.xerces.parsers.XML11Configuration.<init>
  [ 7] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 8] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 9] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [10] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [11] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [12] java.lang.reflect.Constructor.newInstance
  [13] java.lang.Class.newInstance
  [14] org.apache.xerces.parsers.ObjectFactory.newInstance
  [15] org.apache.xerces.parsers.ObjectFactory.createObject
  [16] org.apache.xerces.parsers.ObjectFactory.createObject
  [17] org.apache.xerces.parsers.DOMParser.<init>
  [18] org.apache.xerces.parsers.DOMParser.<init>
  [19] org.apache.xerces.jaxp.DocumentBuilderImpl.<init>
  [20] org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder
  [21] org.apache.hadoop.conf.Configuration.loadResource
  [22] org.apache.hadoop.conf.Configuration.loadResources
  [23] org.apache.hadoop.conf.Configuration.getProps
  [24] org.apache.hadoop.conf.Configuration.set
  [25] org.apache.hadoop.conf.Configuration.set
  [26] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [27] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [28] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [29] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [30] scala.Option.getOrElse
  [31] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [32] org.apache.spark.deploy.SparkSubmit.submit
  [33] org.apache.spark.deploy.SparkSubmit.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [35] org.apache.spark.deploy.SparkSubmit$.main
  [36] org.apache.spark.deploy.SparkSubmit.main
  [37] [DestroyJavaVM tid=16065]

--- 1551158139172854 us
  [ 0] java.util.zip.ZipFile.getEntry
  [ 1] java.util.zip.ZipFile.getEntry
  [ 2] java.util.jar.JarFile.getEntry
  [ 3] java.util.jar.JarFile.getJarEntry
  [ 4] sun.misc.URLClassPath$JarLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] com.google.common.collect.MapMaker.getTicker
  [14] com.google.common.collect.MapMakerInternalMap.<init>
  [15] com.google.common.collect.MapMaker.makeCustomMap
  [16] com.google.common.collect.Interners$WeakInterner.<init>
  [17] com.google.common.collect.Interners$WeakInterner.<init>
  [18] com.google.common.collect.Interners.newWeakInterner
  [19] org.apache.hadoop.util.StringInterner.<clinit>
  [20] org.apache.hadoop.conf.Configuration.loadResource
  [21] org.apache.hadoop.conf.Configuration.loadResources
  [22] org.apache.hadoop.conf.Configuration.getProps
  [23] org.apache.hadoop.conf.Configuration.set
  [24] org.apache.hadoop.conf.Configuration.set
  [25] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [26] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [28] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [31] org.apache.spark.deploy.SparkSubmit.submit
  [32] org.apache.spark.deploy.SparkSubmit.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$.main
  [35] org.apache.spark.deploy.SparkSubmit.main
  [36] [DestroyJavaVM tid=16065]

--- 1551158139178762 us
  [ 0] org.apache.hadoop.conf.Configuration.loadResource
  [ 1] org.apache.hadoop.conf.Configuration.loadResources
  [ 2] org.apache.hadoop.conf.Configuration.getProps
  [ 3] org.apache.hadoop.conf.Configuration.set
  [ 4] org.apache.hadoop.conf.Configuration.set
  [ 5] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [ 6] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [ 7] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [ 8] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [ 9] scala.Option.getOrElse
  [10] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [11] org.apache.spark.deploy.SparkSubmit.submit
  [12] org.apache.spark.deploy.SparkSubmit.doSubmit
  [13] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [14] org.apache.spark.deploy.SparkSubmit$.main
  [15] org.apache.spark.deploy.SparkSubmit.main
  [16] [DestroyJavaVM tid=16065]

--- 1551158139291355 us
  [ 0] java.util.HashMap.get
  [ 1] sun.misc.ProxyGenerator$ConstantPool.getValue
  [ 2] sun.misc.ProxyGenerator$ConstantPool.getUtf8
  [ 3] sun.misc.ProxyGenerator$ConstantPool.getClass
  [ 4] sun.misc.ProxyGenerator$ProxyMethod.generateMethod
  [ 5] sun.misc.ProxyGenerator$ProxyMethod.access$100
  [ 6] sun.misc.ProxyGenerator.generateClassFile
  [ 7] sun.misc.ProxyGenerator.generateProxyClass
  [ 8] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [ 9] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [10] java.lang.reflect.WeakCache$Factory.get
  [11] java.lang.reflect.WeakCache.get
  [12] java.lang.reflect.Proxy.getProxyClass0
  [13] java.lang.reflect.Proxy.newProxyInstance
  [14] sun.reflect.annotation.AnnotationParser$1.run
  [15] sun.reflect.annotation.AnnotationParser$1.run
  [16] java.security.AccessController.doPrivileged
  [17] sun.reflect.annotation.AnnotationParser.annotationForMap
  [18] sun.reflect.annotation.AnnotationParser.parseAnnotation2
  [19] sun.reflect.annotation.AnnotationParser.parseAnnotations2
  [20] sun.reflect.annotation.AnnotationParser.parseSelectAnnotations
  [21] sun.reflect.annotation.AnnotationType.<init>
  [22] sun.reflect.annotation.AnnotationType.getInstance
  [23] sun.reflect.annotation.AnnotationParser.parseAnnotation2
  [24] sun.reflect.annotation.AnnotationParser.parseAnnotations2
  [25] sun.reflect.annotation.AnnotationParser.parseAnnotations
  [26] java.lang.Class.createAnnotationData
  [27] java.lang.Class.annotationData
  [28] java.lang.Class.getAnnotations
  [29] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.initRegistry
  [30] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.<init>
  [31] org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder
  [32] org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register
  [33] org.apache.hadoop.metrics2.MetricsSystem.register
  [34] org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create
  [35] org.apache.hadoop.security.UserGroupInformation.<clinit>
  [36] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [37] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [38] scala.Option.getOrElse
  [39] org.apache.spark.util.Utils$.getCurrentUserName
  [40] org.apache.spark.SecurityManager.<init>
  [41] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [42] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [43] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [44] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [45] scala.Option.map
  [46] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [47] org.apache.spark.deploy.SparkSubmit.submit
  [48] org.apache.spark.deploy.SparkSubmit.doSubmit
  [49] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [50] org.apache.spark.deploy.SparkSubmit$.main
  [51] org.apache.spark.deploy.SparkSubmit.main
  [52] [DestroyJavaVM tid=16065]

--- 1551158139297396 us
  [ 0] __strncpy_sse2_unaligned
  [ 1] ClassLoader::load_classfile(Symbol*, Thread*)
  [ 2] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 3] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 4] JVM_FindClassFromBootLoader
  [ 5] Java_java_lang_ClassLoader_findBootstrapClass
  [ 6] java.lang.ClassLoader.findBootstrapClass
  [ 7] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] org.apache.hadoop.metrics2.lib.MetricsRegistry.setContext
  [13] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.initRegistry
  [14] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.<init>
  [15] org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder
  [16] org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register
  [17] org.apache.hadoop.metrics2.MetricsSystem.register
  [18] org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create
  [19] org.apache.hadoop.security.UserGroupInformation.<clinit>
  [20] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [21] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.util.Utils$.getCurrentUserName
  [24] org.apache.spark.SecurityManager.<init>
  [25] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [26] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [28] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [29] scala.Option.map
  [30] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [31] org.apache.spark.deploy.SparkSubmit.submit
  [32] org.apache.spark.deploy.SparkSubmit.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$.main
  [35] org.apache.spark.deploy.SparkSubmit.main
  [36] [DestroyJavaVM tid=16065]

--- 1551158139391413 us
  [ 0] java.lang.String.contains
  [ 1] java.lang.invoke.InvokerBytecodeGenerator.<init>
  [ 2] java.lang.invoke.InvokerBytecodeGenerator.<init>
  [ 3] java.lang.invoke.InvokerBytecodeGenerator.generateCustomizedCode
  [ 4] java.lang.invoke.LambdaForm.compileToBytecode
  [ 5] java.lang.invoke.LambdaForm.prepare
  [ 6] java.lang.invoke.MethodHandle.<init>
  [ 7] java.lang.invoke.BoundMethodHandle.<init>
  [ 8] java.lang.invoke.SimpleMethodHandle.<init>
  [ 9] java.lang.invoke.SimpleMethodHandle.make
  [10] java.lang.invoke.LambdaForm.createIdentityForms
  [11] java.lang.invoke.LambdaForm.<clinit>
  [12] java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm
  [13] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [14] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [15] java.lang.invoke.DirectMethodHandle.make
  [16] java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon
  [17] java.lang.invoke.MethodHandles$Lookup.getDirectMethodNoSecurityManager
  [18] java.lang.invoke.MethodHandles$Lookup.getDirectMethodForConstant
  [19] java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant
  [20] java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant
  [21] java.lang.UNIXProcess$Platform.get
  [22] java.lang.UNIXProcess.<clinit>
  [23] java.lang.ProcessImpl.start
  [24] java.lang.ProcessBuilder.start
  [25] org.apache.hadoop.util.Shell.runCommand
  [26] org.apache.hadoop.util.Shell.run
  [27] org.apache.hadoop.util.Shell$ShellCommandExecutor.execute
  [28] org.apache.hadoop.util.Shell.isSetsidSupported
  [29] org.apache.hadoop.util.Shell.<clinit>
  [30] org.apache.hadoop.util.StringUtils.<clinit>
  [31] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [32] org.apache.hadoop.security.UserGroupInformation.initialize
  [33] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [34] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [35] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [36] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [37] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [38] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [39] scala.Option.getOrElse
  [40] org.apache.spark.util.Utils$.getCurrentUserName
  [41] org.apache.spark.SecurityManager.<init>
  [42] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [43] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [44] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [45] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [46] scala.Option.map
  [47] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [48] org.apache.spark.deploy.SparkSubmit.submit
  [49] org.apache.spark.deploy.SparkSubmit.doSubmit
  [50] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [51] org.apache.spark.deploy.SparkSubmit$.main
  [52] org.apache.spark.deploy.SparkSubmit.main
  [53] [DestroyJavaVM tid=16065]

--- 1551158139397314 us
  [ 0] klassVtable::update_inherited_vtable(InstanceKlass*, methodHandle, int, int, bool, Thread*)
  [ 1] klassVtable::initialize_vtable(bool, Thread*)
  [ 2] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 3] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 4] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 5] InstanceKlass::initialize(Thread*)
  [ 6] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 7] java.lang.invoke.MethodHandleImpl.makeVarargsCollector
  [ 8] java.lang.invoke.MethodHandle.setVarargs
  [ 9] java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon
  [10] java.lang.invoke.MethodHandles$Lookup.getDirectMethod
  [11] java.lang.invoke.MethodHandles$Lookup.findStatic
  [12] java.lang.invoke.CallSite.<clinit>
  [13] java.lang.invoke.MethodHandleNatives.linkCallSiteImpl
  [14] java.lang.invoke.MethodHandleNatives.linkCallSite
  [15] java.lang.UNIXProcess$Platform.get
  [16] java.lang.UNIXProcess.<clinit>
  [17] java.lang.ProcessImpl.start
  [18] java.lang.ProcessBuilder.start
  [19] org.apache.hadoop.util.Shell.runCommand
  [20] org.apache.hadoop.util.Shell.run
  [21] org.apache.hadoop.util.Shell$ShellCommandExecutor.execute
  [22] org.apache.hadoop.util.Shell.isSetsidSupported
  [23] org.apache.hadoop.util.Shell.<clinit>
  [24] org.apache.hadoop.util.StringUtils.<clinit>
  [25] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [26] org.apache.hadoop.security.UserGroupInformation.initialize
  [27] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [28] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [29] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [30] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [31] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [32] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [33] scala.Option.getOrElse
  [34] org.apache.spark.util.Utils$.getCurrentUserName
  [35] org.apache.spark.SecurityManager.<init>
  [36] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [37] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [38] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [39] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [40] scala.Option.map
  [41] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [42] org.apache.spark.deploy.SparkSubmit.submit
  [43] org.apache.spark.deploy.SparkSubmit.doSubmit
  [44] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [45] org.apache.spark.deploy.SparkSubmit$.main
  [46] org.apache.spark.deploy.SparkSubmit.main
  [47] [DestroyJavaVM tid=16065]

--- 1551158139494604 us
  [ 0] sun.misc.Unsafe.compareAndSwapObject
  [ 1] java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl.compareAndSet
  [ 2] java.io.BufferedInputStream.close
  [ 3] org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.close
  [ 4] org.apache.xerces.impl.io.UTF8Reader.close
  [ 5] org.apache.xerces.impl.XMLEntityManager.endEntity
  [ 6] org.apache.xerces.impl.XMLEntityScanner.load
  [ 7] org.apache.xerces.impl.XMLEntityScanner.skipSpaces
  [ 8] org.apache.xerces.impl.XMLDocumentScannerImpl$TrailingMiscDispatcher.dispatch
  [ 9] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument
  [10] org.apache.xerces.parsers.XML11Configuration.parse
  [11] org.apache.xerces.parsers.XML11Configuration.parse
  [12] org.apache.xerces.parsers.XMLParser.parse
  [13] org.apache.xerces.parsers.DOMParser.parse
  [14] org.apache.xerces.jaxp.DocumentBuilderImpl.parse
  [15] javax.xml.parsers.DocumentBuilder.parse
  [16] org.apache.hadoop.conf.Configuration.parse
  [17] org.apache.hadoop.conf.Configuration.parse
  [18] org.apache.hadoop.conf.Configuration.loadResource
  [19] org.apache.hadoop.conf.Configuration.loadResources
  [20] org.apache.hadoop.conf.Configuration.getProps
  [21] org.apache.hadoop.conf.Configuration.get
  [22] org.apache.hadoop.conf.Configuration.getTrimmed
  [23] org.apache.hadoop.conf.Configuration.getLong
  [24] org.apache.hadoop.security.Groups.<init>
  [25] org.apache.hadoop.security.Groups.<init>
  [26] org.apache.hadoop.security.Groups.getUserToGroupsMappingService
  [27] org.apache.hadoop.security.UserGroupInformation.initialize
  [28] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [29] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [30] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [31] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [32] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [33] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [34] scala.Option.getOrElse
  [35] org.apache.spark.util.Utils$.getCurrentUserName
  [36] org.apache.spark.SecurityManager.<init>
  [37] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [38] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [39] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [40] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [41] scala.Option.map
  [42] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [43] org.apache.spark.deploy.SparkSubmit.submit
  [44] org.apache.spark.deploy.SparkSubmit.doSubmit
  [45] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [46] org.apache.spark.deploy.SparkSubmit$.main
  [47] org.apache.spark.deploy.SparkSubmit.main
  [48] [DestroyJavaVM tid=16065]

--- 1551158139501092 us
  [ 0] com.google.common.collect.MapMakerInternalMap$Segment.drainKeyReferenceQueue
  [ 1] com.google.common.collect.MapMakerInternalMap$Segment.drainReferenceQueues
  [ 2] com.google.common.collect.MapMakerInternalMap$Segment.runLockedCleanup
  [ 3] com.google.common.collect.MapMakerInternalMap$Segment.preWriteCleanup
  [ 4] com.google.common.collect.MapMakerInternalMap$Segment.put
  [ 5] com.google.common.collect.MapMakerInternalMap.putIfAbsent
  [ 6] com.google.common.collect.Interners$WeakInterner.intern
  [ 7] org.apache.hadoop.util.StringInterner.weakIntern
  [ 8] org.apache.hadoop.conf.Configuration.loadResource
  [ 9] org.apache.hadoop.conf.Configuration.loadResources
  [10] org.apache.hadoop.conf.Configuration.getProps
  [11] org.apache.hadoop.conf.Configuration.get
  [12] org.apache.hadoop.conf.Configuration.getTrimmed
  [13] org.apache.hadoop.conf.Configuration.getLong
  [14] org.apache.hadoop.security.Groups.<init>
  [15] org.apache.hadoop.security.Groups.<init>
  [16] org.apache.hadoop.security.Groups.getUserToGroupsMappingService
  [17] org.apache.hadoop.security.UserGroupInformation.initialize
  [18] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [19] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [20] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [21] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [22] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [23] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.util.Utils$.getCurrentUserName
  [26] org.apache.spark.SecurityManager.<init>
  [27] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [28] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [29] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [30] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [31] scala.Option.map
  [32] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [33] org.apache.spark.deploy.SparkSubmit.submit
  [34] org.apache.spark.deploy.SparkSubmit.doSubmit
  [35] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$.main
  [37] org.apache.spark.deploy.SparkSubmit.main
  [38] [DestroyJavaVM tid=16065]

--- 1551158139595974 us
  [ 0] org.apache.xerces.util.XMLChar.isName
  [ 1] org.apache.xerces.impl.XMLEntityScanner.scanQName
  [ 2] org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement
  [ 3] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch
  [ 4] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument
  [ 5] org.apache.xerces.parsers.XML11Configuration.parse
  [ 6] org.apache.xerces.parsers.XML11Configuration.parse
  [ 7] org.apache.xerces.parsers.XMLParser.parse
  [ 8] org.apache.xerces.parsers.DOMParser.parse
  [ 9] org.apache.xerces.jaxp.DocumentBuilderImpl.parse
  [10] javax.xml.parsers.DocumentBuilder.parse
  [11] org.apache.hadoop.conf.Configuration.parse
  [12] org.apache.hadoop.conf.Configuration.parse
  [13] org.apache.hadoop.conf.Configuration.loadResource
  [14] org.apache.hadoop.conf.Configuration.loadResources
  [15] org.apache.hadoop.conf.Configuration.getProps
  [16] org.apache.hadoop.conf.Configuration.get
  [17] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [18] org.apache.hadoop.security.UserGroupInformation.initialize
  [19] org.apache.hadoop.security.UserGroupInformation.setConfiguration
  [20] org.apache.spark.deploy.SparkHadoopUtil.<init>
  [21] org.apache.spark.deploy.SparkHadoopUtil$.instance$lzycompute
  [22] org.apache.spark.deploy.SparkHadoopUtil$.instance
  [23] org.apache.spark.deploy.SparkHadoopUtil$.get
  [24] org.apache.spark.SecurityManager.<init>
  [25] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [26] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [28] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [29] scala.Option.map
  [30] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [31] org.apache.spark.deploy.SparkSubmit.submit
  [32] org.apache.spark.deploy.SparkSubmit.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$.main
  [35] org.apache.spark.deploy.SparkSubmit.main
  [36] [DestroyJavaVM tid=16065]

--- 1551158139601841 us
  [ 0] org.apache.xerces.parsers.XML11Configuration.setProperty
  [ 1] org.apache.xerces.parsers.XML11Configuration.configurePipeline
  [ 2] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.configurePipeline
  [ 3] org.apache.xerces.parsers.XML11Configuration.parse
  [ 4] org.apache.xerces.parsers.XML11Configuration.parse
  [ 5] org.apache.xerces.parsers.XMLParser.parse
  [ 6] org.apache.xerces.parsers.DOMParser.parse
  [ 7] org.apache.xerces.jaxp.DocumentBuilderImpl.parse
  [ 8] javax.xml.parsers.DocumentBuilder.parse
  [ 9] org.apache.hadoop.conf.Configuration.parse
  [10] org.apache.hadoop.conf.Configuration.parse
  [11] org.apache.hadoop.conf.Configuration.loadResource
  [12] org.apache.hadoop.conf.Configuration.loadResources
  [13] org.apache.hadoop.conf.Configuration.getProps
  [14] org.apache.hadoop.conf.Configuration.get
  [15] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [16] org.apache.hadoop.security.UserGroupInformation.initialize
  [17] org.apache.hadoop.security.UserGroupInformation.setConfiguration
  [18] org.apache.spark.deploy.SparkHadoopUtil.<init>
  [19] org.apache.spark.deploy.SparkHadoopUtil$.instance$lzycompute
  [20] org.apache.spark.deploy.SparkHadoopUtil$.instance
  [21] org.apache.spark.deploy.SparkHadoopUtil$.get
  [22] org.apache.spark.SecurityManager.<init>
  [23] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [24] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [25] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [26] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [27] scala.Option.map
  [28] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [29] org.apache.spark.deploy.SparkSubmit.submit
  [30] org.apache.spark.deploy.SparkSubmit.doSubmit
  [31] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [32] org.apache.spark.deploy.SparkSubmit$.main
  [33] org.apache.spark.deploy.SparkSubmit.main
  [34] [DestroyJavaVM tid=16065]

--- 1551158139696541 us
  [ 0] java.util.zip.ZipFile.getEntry
  [ 1] java.util.zip.ZipFile.getEntry
  [ 2] java.util.jar.JarFile.getEntry
  [ 3] java.util.jar.JarFile.getJarEntry
  [ 4] sun.misc.URLClassPath$JarLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] org.apache.spark.deploy.PythonRunner$.formatPath
  [14] org.apache.spark.deploy.PythonRunner$.main
  [15] org.apache.spark.deploy.PythonRunner.main
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] org.apache.spark.deploy.JavaMainApplication.start
  [21] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain
  [22] org.apache.spark.deploy.SparkSubmit.doRunMain$1
  [23] org.apache.spark.deploy.SparkSubmit.submit
  [24] org.apache.spark.deploy.SparkSubmit.doSubmit
  [25] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [26] org.apache.spark.deploy.SparkSubmit$.main
  [27] org.apache.spark.deploy.SparkSubmit.main
  [28] [DestroyJavaVM tid=16065]

--- 1551158139702445 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] py4j.GatewayServer$GatewayServerBuilder.<init>
  [16] py4j.GatewayServer$GatewayServerBuilder.<init>
  [17] org.apache.spark.deploy.PythonRunner$.main
  [18] org.apache.spark.deploy.PythonRunner.main
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] org.apache.spark.deploy.JavaMainApplication.start
  [24] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain
  [25] org.apache.spark.deploy.SparkSubmit.doRunMain$1
  [26] org.apache.spark.deploy.SparkSubmit.submit
  [27] org.apache.spark.deploy.SparkSubmit.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [29] org.apache.spark.deploy.SparkSubmit$.main
  [30] org.apache.spark.deploy.SparkSubmit.main
  [31] [DestroyJavaVM tid=16065]

--- 1551158140072459 us
  [ 0] ClassFileParser::parse_annotations(unsigned char*, int, ClassFileParser::AnnotationCollector*, Thread*) [clone .part.113]
  [ 1] ClassFileParser::parse_classfile_attributes(ClassFileParser::ClassAnnotationCollector*, Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 4] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 5] JVM_DefineClassWithSource
  [ 6] Java_java_lang_ClassLoader_defineClass1
  [ 7] java.lang.ClassLoader.defineClass1
  [ 8] java.lang.ClassLoader.defineClass
  [ 9] java.security.SecureClassLoader.defineClass
  [10] java.net.URLClassLoader.defineClass
  [11] java.net.URLClassLoader.access$100
  [12] java.net.URLClassLoader$1.run
  [13] java.net.URLClassLoader$1.run
  [14] java.security.AccessController.doPrivileged
  [15] java.net.URLClassLoader.findClass
  [16] java.lang.ClassLoader.loadClass
  [17] sun.misc.Launcher$AppClassLoader.loadClass
  [18] java.lang.ClassLoader.loadClass
  [19] org.apache.hadoop.fs.permission.FsPermission.<clinit>
  [20] org.apache.spark.scheduler.EventLoggingListener$.<init>
  [21] org.apache.spark.scheduler.EventLoggingListener$.<clinit>
  [22] org.apache.spark.SparkContext.<init>
  [23] org.apache.spark.api.java.JavaSparkContext.<init>
  [24] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [26] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [27] java.lang.reflect.Constructor.newInstance
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.ConstructorCommand.invokeConstructor
  [32] py4j.commands.ConstructorCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158140192862 us
  [ 0] java.util.ArrayList.indexOf
  [ 1] java.util.ArrayList.contains
  [ 2] org.apache.xerces.util.ParserConfigurationSettings.addRecognizedFeatures
  [ 3] org.apache.xerces.parsers.XML11Configuration.<init>
  [ 4] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 5] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 6] sun.reflect.GeneratedConstructorAccessor2.newInstance
  [ 7] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [ 8] java.lang.reflect.Constructor.newInstance
  [ 9] java.lang.Class.newInstance
  [10] org.apache.xerces.parsers.ObjectFactory.newInstance
  [11] org.apache.xerces.parsers.ObjectFactory.createObject
  [12] org.apache.xerces.parsers.ObjectFactory.createObject
  [13] org.apache.xerces.parsers.DOMParser.<init>
  [14] org.apache.xerces.parsers.DOMParser.<init>
  [15] org.apache.xerces.jaxp.DocumentBuilderImpl.<init>
  [16] org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder
  [17] org.apache.hadoop.conf.Configuration.loadResource
  [18] org.apache.hadoop.conf.Configuration.loadResources
  [19] org.apache.hadoop.conf.Configuration.getProps
  [20] org.apache.hadoop.conf.Configuration.get
  [21] org.apache.hadoop.conf.Configuration.getStringCollection
  [22] org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders
  [23] org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders
  [24] org.apache.hadoop.conf.Configuration.getPassword
  [25] org.apache.spark.SSLOptions$$anonfun$8.apply
  [26] org.apache.spark.SSLOptions$$anonfun$8.apply
  [27] scala.Option.orElse
  [28] org.apache.spark.SSLOptions$.parse
  [29] org.apache.spark.SecurityManager.<init>
  [30] org.apache.spark.SparkEnv$.create
  [31] org.apache.spark.SparkEnv$.createDriverEnv
  [32] org.apache.spark.SparkContext.createSparkEnv
  [33] org.apache.spark.SparkContext.<init>
  [34] org.apache.spark.api.java.JavaSparkContext.<init>
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [37] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [38] java.lang.reflect.Constructor.newInstance
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.ConstructorCommand.invokeConstructor
  [43] py4j.commands.ConstructorCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158140294511 us
  [ 0] method_comparator(Method*, Method*)
  [ 1] void QuickSort::inner_sort<Method*, int (*)(Method*, Method*), false>(Method**, int, int (*)(Method*, Method*))
  [ 2] Method::sort_methods(Array<Method*>*, bool, bool)
  [ 3] ClassFileParser::sort_methods(Array<Method*>*)
  [ 4] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 5] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 6] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 7] JVM_DefineClassWithSource
  [ 8] Java_java_lang_ClassLoader_defineClass1
  [ 9] java.lang.ClassLoader.defineClass1
  [10] java.lang.ClassLoader.defineClass
  [11] java.security.SecureClassLoader.defineClass
  [12] java.net.URLClassLoader.defineClass
  [13] java.net.URLClassLoader.access$100
  [14] java.net.URLClassLoader$1.run
  [15] java.net.URLClassLoader$1.run
  [16] java.security.AccessController.doPrivileged
  [17] java.net.URLClassLoader.findClass
  [18] java.lang.ClassLoader.loadClass
  [19] sun.misc.Launcher$AppClassLoader.loadClass
  [20] java.lang.ClassLoader.loadClass
  [21] io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>
  [22] io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>
  [23] io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>
  [24] io.netty.channel.MultithreadEventLoopGroup.<init>
  [25] io.netty.channel.nio.NioEventLoopGroup.<init>
  [26] io.netty.channel.nio.NioEventLoopGroup.<init>
  [27] io.netty.channel.nio.NioEventLoopGroup.<init>
  [28] org.apache.spark.network.util.NettyUtils.createEventLoop
  [29] org.apache.spark.network.client.TransportClientFactory.<init>
  [30] org.apache.spark.network.TransportContext.createClientFactory
  [31] org.apache.spark.rpc.netty.NettyRpcEnv.<init>
  [32] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [33] org.apache.spark.rpc.RpcEnv$.create
  [34] org.apache.spark.SparkEnv$.create
  [35] org.apache.spark.SparkEnv$.createDriverEnv
  [36] org.apache.spark.SparkContext.createSparkEnv
  [37] org.apache.spark.SparkContext.<init>
  [38] org.apache.spark.api.java.JavaSparkContext.<init>
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [40] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [41] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [42] java.lang.reflect.Constructor.newInstance
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.ConstructorCommand.invokeConstructor
  [47] py4j.commands.ConstructorCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158140394588 us
  [ 0] java.net.URLStreamHandler.parseURL
  [ 1] sun.net.www.protocol.file.Handler.parseURL
  [ 2] java.net.URL.<init>
  [ 3] java.net.URL.<init>
  [ 4] sun.misc.URLClassPath$FileLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] org.apache.spark.network.util.NettyUtils.createPooledByteBufAllocator
  [14] org.apache.spark.network.client.TransportClientFactory.<init>
  [15] org.apache.spark.network.TransportContext.createClientFactory
  [16] org.apache.spark.rpc.netty.NettyRpcEnv.<init>
  [17] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [18] org.apache.spark.rpc.RpcEnv$.create
  [19] org.apache.spark.SparkEnv$.create
  [20] org.apache.spark.SparkEnv$.createDriverEnv
  [21] org.apache.spark.SparkContext.createSparkEnv
  [22] org.apache.spark.SparkContext.<init>
  [23] org.apache.spark.api.java.JavaSparkContext.<init>
  [24] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [26] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [27] java.lang.reflect.Constructor.newInstance
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.ConstructorCommand.invokeConstructor
  [32] py4j.commands.ConstructorCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158140522664 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] io.netty.util.internal.SocketUtils.hardwareAddressFromNetworkInterface
  [10] io.netty.util.internal.MacAddressUtil.bestAvailableMac
  [11] io.netty.util.internal.MacAddressUtil.defaultMachineId
  [12] io.netty.channel.DefaultChannelId.<clinit>
  [13] io.netty.channel.AbstractChannel.newId
  [14] io.netty.channel.AbstractChannel.<init>
  [15] io.netty.channel.nio.AbstractNioChannel.<init>
  [16] io.netty.channel.nio.AbstractNioMessageChannel.<init>
  [17] io.netty.channel.socket.nio.NioServerSocketChannel.<init>
  [18] io.netty.channel.socket.nio.NioServerSocketChannel.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] io.netty.channel.ReflectiveChannelFactory.newChannel
  [24] io.netty.bootstrap.AbstractBootstrap.initAndRegister
  [25] io.netty.bootstrap.AbstractBootstrap.doBind
  [26] io.netty.bootstrap.AbstractBootstrap.bind
  [27] org.apache.spark.network.server.TransportServer.init
  [28] org.apache.spark.network.server.TransportServer.<init>
  [29] org.apache.spark.network.TransportContext.createServer
  [30] org.apache.spark.rpc.netty.NettyRpcEnv.startServer
  [31] org.apache.spark.rpc.netty.NettyRpcEnvFactory$$anonfun$4.apply
  [32] org.apache.spark.rpc.netty.NettyRpcEnvFactory$$anonfun$4.apply
  [33] org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp
  [34] scala.collection.immutable.Range.foreach$mVc$sp
  [35] org.apache.spark.util.Utils$.startServiceOnPort
  [36] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [37] org.apache.spark.rpc.RpcEnv$.create
  [38] org.apache.spark.SparkEnv$.create
  [39] org.apache.spark.SparkEnv$.createDriverEnv
  [40] org.apache.spark.SparkContext.createSparkEnv
  [41] org.apache.spark.SparkContext.<init>
  [42] org.apache.spark.api.java.JavaSparkContext.<init>
  [43] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [44] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [45] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [46] java.lang.reflect.Constructor.newInstance
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.ConstructorCommand.invokeConstructor
  [51] py4j.commands.ConstructorCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16145]

--- 1551158140632780 us
  [ 0] clear_page_erms_[k]
  [ 1] get_page_from_freelist_[k]
  [ 2] __alloc_pages_nodemask_[k]
  [ 3] alloc_pages_vma_[k]
  [ 4] handle_pte_fault_[k]
  [ 5] __handle_mm_fault_[k]
  [ 6] handle_mm_fault_[k]
  [ 7] __do_page_fault_[k]
  [ 8] page_fault_[k]
  [ 9] __pthread_create_2_1
  [10] os::create_thread(Thread*, os::ThreadType, unsigned long)
  [11] JVM_StartThread
  [12] java.lang.Thread.start0
  [13] java.lang.Thread.start
  [14] java.util.concurrent.ThreadPoolExecutor.addWorker
  [15] java.util.concurrent.ThreadPoolExecutor.execute
  [16] org.apache.spark.MapOutputTrackerMaster$$anonfun$2.apply$mcVI$sp
  [17] scala.collection.immutable.Range.foreach$mVc$sp
  [18] org.apache.spark.MapOutputTrackerMaster.<init>
  [19] org.apache.spark.SparkEnv$.create
  [20] org.apache.spark.SparkEnv$.createDriverEnv
  [21] org.apache.spark.SparkContext.createSparkEnv
  [22] org.apache.spark.SparkContext.<init>
  [23] org.apache.spark.api.java.JavaSparkContext.<init>
  [24] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [26] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [27] java.lang.reflect.Constructor.newInstance
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.ConstructorCommand.invokeConstructor
  [32] py4j.commands.ConstructorCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158140759939 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.math.BigInt.<init>
  [13] scala.math.BigInt$.apply
  [14] org.apache.spark.util.Utils$.bytesToString
  [15] org.apache.spark.storage.memory.MemoryStore$$anonfun$2.apply
  [16] org.apache.spark.storage.memory.MemoryStore$$anonfun$2.apply
  [17] org.apache.spark.internal.Logging$class.logInfo
  [18] org.apache.spark.storage.memory.MemoryStore.logInfo
  [19] org.apache.spark.storage.memory.MemoryStore.<init>
  [20] org.apache.spark.storage.BlockManager.<init>
  [21] org.apache.spark.SparkEnv$.create
  [22] org.apache.spark.SparkEnv$.createDriverEnv
  [23] org.apache.spark.SparkContext.createSparkEnv
  [24] org.apache.spark.SparkContext.<init>
  [25] org.apache.spark.api.java.JavaSparkContext.<init>
  [26] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [27] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [28] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [29] java.lang.reflect.Constructor.newInstance
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.ConstructorCommand.invokeConstructor
  [34] py4j.commands.ConstructorCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158140867591 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] scala.xml.Node.buildString
  [12] scala.xml.Node.toString
  [13] org.apache.spark.ui.jobs.AllJobsPage.<init>
  [14] org.apache.spark.ui.jobs.JobsTab.<init>
  [15] org.apache.spark.ui.SparkUI.initialize
  [16] org.apache.spark.ui.SparkUI.<init>
  [17] org.apache.spark.ui.SparkUI$.create
  [18] org.apache.spark.SparkContext.<init>
  [19] org.apache.spark.api.java.JavaSparkContext.<init>
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [22] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [23] java.lang.reflect.Constructor.newInstance
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.ConstructorCommand.invokeConstructor
  [28] py4j.commands.ConstructorCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16145]

--- 1551158140967899 us
  [ 0] pvclock_clocksource_read_[k]
  [ 1] xen_clocksource_get_cycles_[k]
  [ 2] ktime_get_ts64_[k]
  [ 3] posix_ktime_get_ts_[k]
  [ 4] sys_clock_gettime_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] __vdso_clock_gettime
  [ 8] __GI___clock_gettime
  [ 9] [unknown]
  [10] java.lang.ClassLoader.defineClass1
  [11] java.lang.ClassLoader.defineClass
  [12] java.security.SecureClassLoader.defineClass
  [13] java.net.URLClassLoader.defineClass
  [14] java.net.URLClassLoader.access$100
  [15] java.net.URLClassLoader$1.run
  [16] java.net.URLClassLoader$1.run
  [17] java.security.AccessController.doPrivileged
  [18] java.net.URLClassLoader.findClass
  [19] java.lang.ClassLoader.loadClass
  [20] sun.misc.Launcher$AppClassLoader.loadClass
  [21] java.lang.ClassLoader.loadClass
  [22] org.spark_project.jetty.server.handler.ContextHandler.<init>
  [23] org.spark_project.jetty.server.handler.ContextHandler.<init>
  [24] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [25] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [26] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [27] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [28] org.apache.spark.ui.JettyUtils$.createServletHandler
  [29] org.apache.spark.ui.JettyUtils$.createServletHandler
  [30] org.apache.spark.ui.WebUI.attachPage
  [31] org.apache.spark.ui.WebUI$$anonfun$attachTab$1.apply
  [32] org.apache.spark.ui.WebUI$$anonfun$attachTab$1.apply
  [33] scala.collection.mutable.ResizableArray$class.foreach
  [34] scala.collection.mutable.ArrayBuffer.foreach
  [35] org.apache.spark.ui.WebUI.attachTab
  [36] org.apache.spark.ui.SparkUI.initialize
  [37] org.apache.spark.ui.SparkUI.<init>
  [38] org.apache.spark.ui.SparkUI$.create
  [39] org.apache.spark.SparkContext.<init>
  [40] org.apache.spark.api.java.JavaSparkContext.<init>
  [41] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [42] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [43] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [44] java.lang.reflect.Constructor.newInstance
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.ConstructorCommand.invokeConstructor
  [49] py4j.commands.ConstructorCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158141070818 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.spark_project.jetty.http.HttpHeaderValue.<init>
  [13] org.spark_project.jetty.http.HttpHeaderValue.<clinit>
  [14] org.spark_project.jetty.http.HttpGenerator.<clinit>
  [15] org.spark_project.jetty.server.Server.doStart
  [16] org.spark_project.jetty.util.component.AbstractLifeCycle.start
  [17] org.apache.spark.ui.JettyUtils$.startJettyServer
  [18] org.apache.spark.ui.WebUI.bind
  [19] org.apache.spark.SparkContext$$anonfun$11.apply
  [20] org.apache.spark.SparkContext$$anonfun$11.apply
  [21] scala.Option.foreach
  [22] org.apache.spark.SparkContext.<init>
  [23] org.apache.spark.api.java.JavaSparkContext.<init>
  [24] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [26] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [27] java.lang.reflect.Constructor.newInstance
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.ConstructorCommand.invokeConstructor
  [32] py4j.commands.ConstructorCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158141171950 us
  [ 0] java.util.WeakHashMap.hash
  [ 1] java.util.WeakHashMap.getEntry
  [ 2] java.util.WeakHashMap.containsKey
  [ 3] java.util.Collections$SetFromMap.contains
  [ 4] java.lang.ClassLoader$ParallelLoaders.isRegistered
  [ 5] java.lang.ClassLoader.<init>
  [ 6] java.lang.ClassLoader.<init>
  [ 7] sun.reflect.DelegatingClassLoader.<init>
  [ 8] sun.reflect.ClassDefiner$1.run
  [ 9] sun.reflect.ClassDefiner$1.run
  [10] java.security.AccessController.doPrivileged
  [11] sun.reflect.ClassDefiner.defineClass
  [12] sun.reflect.MethodAccessorGenerator$1.run
  [13] sun.reflect.MethodAccessorGenerator$1.run
  [14] java.security.AccessController.doPrivileged
  [15] sun.reflect.MethodAccessorGenerator.generate
  [16] sun.reflect.MethodAccessorGenerator.generateConstructor
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [18] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [19] java.lang.reflect.Constructor.newInstance
  [20] java.lang.Class.newInstance
  [21] org.spark_project.jetty.util.IncludeExcludeSet.<init>
  [22] org.spark_project.jetty.util.IncludeExcludeSet.<init>
  [23] org.spark_project.jetty.util.IncludeExclude.<init>
  [24] org.spark_project.jetty.server.handler.gzip.GzipHandler.<init>
  [25] org.apache.spark.ui.JettyUtils$$anonfun$startJettyServer$1.apply
  [26] org.apache.spark.ui.JettyUtils$$anonfun$startJettyServer$1.apply
  [27] scala.collection.mutable.ResizableArray$class.foreach
  [28] scala.collection.mutable.ArrayBuffer.foreach
  [29] org.apache.spark.ui.JettyUtils$.startJettyServer
  [30] org.apache.spark.ui.WebUI.bind
  [31] org.apache.spark.SparkContext$$anonfun$11.apply
  [32] org.apache.spark.SparkContext$$anonfun$11.apply
  [33] scala.Option.foreach
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158141288535 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.Enumeration$ValueSet$.<init>
  [16] scala.Enumeration.ValueSet$lzycompute
  [17] scala.Enumeration.ValueSet
  [18] scala.Enumeration.values
  [19] scala.Enumeration.withName
  [20] org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree1$1
  [21] org.apache.spark.scheduler.TaskSchedulerImpl.<init>
  [22] org.apache.spark.scheduler.TaskSchedulerImpl.<init>
  [23] org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler
  [24] org.apache.spark.SparkContext.<init>
  [25] org.apache.spark.api.java.JavaSparkContext.<init>
  [26] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [27] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [28] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [29] java.lang.reflect.Constructor.newInstance
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.ConstructorCommand.invokeConstructor
  [34] py4j.commands.ConstructorCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158141420607 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.collection.mutable.ArraySeq$$anonfun$newBuilder$1.apply
  [10] scala.collection.mutable.ArraySeq$$anonfun$newBuilder$1.apply
  [11] scala.collection.mutable.Builder$$anon$1.result
  [12] scala.collection.TraversableLike$class.map
  [13] scala.collection.mutable.ArrayOps$ofRef.map
  [14] org.apache.spark.util.Utils$.sparkJavaOpts
  [15] org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start
  [16] org.apache.spark.scheduler.TaskSchedulerImpl.start
  [17] org.apache.spark.SparkContext.<init>
  [18] org.apache.spark.api.java.JavaSparkContext.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.ConstructorCommand.invokeConstructor
  [27] py4j.commands.ConstructorCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16145]

--- 1551158141664864 us
  [ 0] UTF8::strrchr(signed char const*, int, signed char)
  [ 1] InstanceKlass::is_same_class_package(oopDesc*, Symbol*, oopDesc*, Symbol*) [clone .part.135]
  [ 2] Reflection::verify_class_access(Klass*, Klass*, bool)
  [ 3] LinkResolver::check_klass_accessability(KlassHandle, KlassHandle, Thread*)
  [ 4] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [ 5] ConstantPool::klass_ref_at(int, Thread*)
  [ 6] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 7] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 8] io.netty.buffer.PooledByteBuf.init0
  [ 9] io.netty.buffer.PooledByteBuf.init
  [10] io.netty.buffer.PoolChunk.initBufWithSubpage
  [11] io.netty.buffer.PoolChunk.initBuf
  [12] io.netty.buffer.PoolArena.allocateNormal
  [13] io.netty.buffer.PoolArena.allocate
  [14] io.netty.buffer.PoolArena.allocate
  [15] io.netty.buffer.PooledByteBufAllocator.newHeapBuffer
  [16] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [17] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [18] org.apache.spark.network.protocol.MessageEncoder.encode
  [19] org.apache.spark.network.protocol.MessageEncoder.encode
  [20] io.netty.handler.codec.MessageToMessageEncoder.write
  [21] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [22] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [23] io.netty.channel.AbstractChannelHandlerContext.write
  [24] io.netty.channel.AbstractChannelHandlerContext.write
  [25] io.netty.handler.timeout.IdleStateHandler.write
  [26] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [27] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [28] io.netty.channel.AbstractChannelHandlerContext.access$1900
  [29] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write
  [30] io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write
  [31] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run
  [32] io.netty.util.concurrent.AbstractEventExecutor.safeExecute
  [33] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks
  [34] io.netty.channel.nio.NioEventLoop.run
  [35] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [36] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [37] java.lang.Thread.run
  [38] [rpc-client-1-1 tid=16231]

--- 1551158141905928 us
  [ 0] java.io.ObjectInputStream.readObject0
  [ 1] java.io.ObjectInputStream.defaultReadFields
  [ 2] java.io.ObjectInputStream.readSerialData
  [ 3] java.io.ObjectInputStream.readOrdinaryObject
  [ 4] java.io.ObjectInputStream.readObject0
  [ 5] java.io.ObjectInputStream.readObject
  [ 6] scala.collection.mutable.HashMap$$anonfun$readObject$1.apply
  [ 7] scala.collection.mutable.HashMap$$anonfun$readObject$1.apply
  [ 8] scala.collection.mutable.HashTable$class.init
  [ 9] scala.collection.mutable.HashMap.init
  [10] scala.collection.mutable.HashMap.readObject
  [11] sun.reflect.NativeMethodAccessorImpl.invoke0
  [12] sun.reflect.NativeMethodAccessorImpl.invoke
  [13] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [14] java.lang.reflect.Method.invoke
  [15] java.io.ObjectStreamClass.invokeReadObject
  [16] java.io.ObjectInputStream.readSerialData
  [17] java.io.ObjectInputStream.readOrdinaryObject
  [18] java.io.ObjectInputStream.readObject0
  [19] java.io.ObjectInputStream.defaultReadFields
  [20] java.io.ObjectInputStream.readSerialData
  [21] java.io.ObjectInputStream.readOrdinaryObject
  [22] java.io.ObjectInputStream.readObject0
  [23] java.io.ObjectInputStream.defaultReadFields
  [24] java.io.ObjectInputStream.readSerialData
  [25] java.io.ObjectInputStream.readOrdinaryObject
  [26] java.io.ObjectInputStream.readObject0
  [27] java.io.ObjectInputStream.defaultReadFields
  [28] java.io.ObjectInputStream.readSerialData
  [29] java.io.ObjectInputStream.readOrdinaryObject
  [30] java.io.ObjectInputStream.readObject0
  [31] java.io.ObjectInputStream.readObject
  [32] org.apache.spark.serializer.JavaDeserializationStream.readObject
  [33] org.apache.spark.serializer.JavaSerializerInstance.deserialize
  [34] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply
  [35] scala.util.DynamicVariable.withValue
  [36] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [37] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply
  [38] scala.util.DynamicVariable.withValue
  [39] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [40] org.apache.spark.rpc.netty.RequestMessage$.apply
  [41] org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive
  [42] org.apache.spark.rpc.netty.NettyRpcHandler.receive
  [43] org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage
  [44] org.apache.spark.network.server.TransportRequestHandler.handle
  [45] org.apache.spark.network.server.TransportChannelHandler.channelRead
  [46] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [47] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [48] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [49] io.netty.handler.timeout.IdleStateHandler.channelRead
  [50] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [51] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [52] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [53] io.netty.handler.codec.MessageToMessageDecoder.channelRead
  [54] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [55] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [56] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [57] org.apache.spark.network.util.TransportFrameDecoder.channelRead
  [58] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [59] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [60] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [61] io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead
  [62] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [63] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [64] io.netty.channel.DefaultChannelPipeline.fireChannelRead
  [65] io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read
  [66] io.netty.channel.nio.NioEventLoop.processSelectedKey
  [67] io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized
  [68] io.netty.channel.nio.NioEventLoop.processSelectedKeys
  [69] io.netty.channel.nio.NioEventLoop.run
  [70] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [71] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [72] java.lang.Thread.run
  [73] [rpc-client-1-1 tid=16231]

--- 1551158141924863 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.storage.BlockManagerId$.apply
  [10] org.apache.spark.storage.BlockManager.initialize
  [11] org.apache.spark.SparkContext.<init>
  [12] org.apache.spark.api.java.JavaSparkContext.<init>
  [13] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [15] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [16] java.lang.reflect.Constructor.newInstance
  [17] py4j.reflection.MethodInvoker.invoke
  [18] py4j.reflection.ReflectionEngine.invoke
  [19] py4j.Gateway.invoke
  [20] py4j.commands.ConstructorCommand.invokeConstructor
  [21] py4j.commands.ConstructorCommand.execute
  [22] py4j.GatewayConnection.run
  [23] java.lang.Thread.run
  [24] [tid=16145]

--- 1551158142038188 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] com.fasterxml.jackson.databind.ObjectMapper.<clinit>
  [11] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [13] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [14] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [15] java.lang.reflect.Constructor.newInstance
  [16] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [17] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [18] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [19] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [20] scala.collection.mutable.HashTable$class.foreachEntry
  [21] scala.collection.mutable.HashMap.foreachEntry
  [22] scala.collection.mutable.HashMap.foreach
  [23] org.apache.spark.metrics.MetricsSystem.registerSinks
  [24] org.apache.spark.metrics.MetricsSystem.start
  [25] org.apache.spark.SparkContext.<init>
  [26] org.apache.spark.api.java.JavaSparkContext.<init>
  [27] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [28] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [29] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [30] java.lang.reflect.Constructor.newInstance
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.ConstructorCommand.invokeConstructor
  [35] py4j.commands.ConstructorCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158142138336 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [12] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [13] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [19] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [20] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [21] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [22] scala.collection.mutable.HashTable$class.foreachEntry
  [23] scala.collection.mutable.HashMap.foreachEntry
  [24] scala.collection.mutable.HashMap.foreach
  [25] org.apache.spark.metrics.MetricsSystem.registerSinks
  [26] org.apache.spark.metrics.MetricsSystem.start
  [27] org.apache.spark.SparkContext.<init>
  [28] org.apache.spark.api.java.JavaSparkContext.<init>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.ConstructorCommand.invokeConstructor
  [37] py4j.commands.ConstructorCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16145]

--- 1551158142238074 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [13] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [14] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [20] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [21] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [22] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [23] scala.collection.mutable.HashTable$class.foreachEntry
  [24] scala.collection.mutable.HashMap.foreachEntry
  [25] scala.collection.mutable.HashMap.foreach
  [26] org.apache.spark.metrics.MetricsSystem.registerSinks
  [27] org.apache.spark.metrics.MetricsSystem.start
  [28] org.apache.spark.SparkContext.<init>
  [29] org.apache.spark.api.java.JavaSparkContext.<init>
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [32] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [33] java.lang.reflect.Constructor.newInstance
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.ConstructorCommand.invokeConstructor
  [38] py4j.commands.ConstructorCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16145]

--- 1551158142342864 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.hadoop.AbstractFileSystem.<init>
  [10] alluxio.hadoop.FileSystem.<init>
  [11] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [13] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [14] java.lang.reflect.Constructor.newInstance
  [15] java.lang.Class.newInstance
  [16] java.util.ServiceLoader$LazyIterator.nextService
  [17] java.util.ServiceLoader$LazyIterator.next
  [18] java.util.ServiceLoader$1.next
  [19] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [20] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [21] org.apache.hadoop.fs.FileSystem.createFileSystem
  [22] org.apache.hadoop.fs.FileSystem.access$200
  [23] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [24] org.apache.hadoop.fs.FileSystem$Cache.get
  [25] org.apache.hadoop.fs.FileSystem.get
  [26] org.apache.spark.util.Utils$.getHadoopFileSystem
  [27] org.apache.spark.scheduler.EventLoggingListener.<init>
  [28] org.apache.spark.SparkContext.<init>
  [29] org.apache.spark.api.java.JavaSparkContext.<init>
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [32] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [33] java.lang.reflect.Constructor.newInstance
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.ConstructorCommand.invokeConstructor
  [38] py4j.commands.ConstructorCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16145]

--- 1551158142473711 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [10] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [11] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [12] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [13] java.lang.reflect.Constructor.newInstance
  [14] java.lang.Class.newInstance
  [15] java.util.ServiceLoader$LazyIterator.nextService
  [16] java.util.ServiceLoader$LazyIterator.next
  [17] java.util.ServiceLoader$1.next
  [18] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [19] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [20] org.apache.hadoop.fs.FileSystem.createFileSystem
  [21] org.apache.hadoop.fs.FileSystem.access$200
  [22] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [23] org.apache.hadoop.fs.FileSystem$Cache.get
  [24] org.apache.hadoop.fs.FileSystem.get
  [25] org.apache.spark.util.Utils$.getHadoopFileSystem
  [26] org.apache.spark.scheduler.EventLoggingListener.<init>
  [27] org.apache.spark.SparkContext.<init>
  [28] org.apache.spark.api.java.JavaSparkContext.<init>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.ConstructorCommand.invokeConstructor
  [37] py4j.commands.ConstructorCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16145]

--- 1551158142578845 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.jackson.map.ObjectMapper.<init>
  [10] org.codehaus.jackson.map.ObjectMapper.<init>
  [11] org.codehaus.jackson.map.ObjectMapper.<init>
  [12] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [13] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [15] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [16] java.lang.reflect.Constructor.newInstance
  [17] java.lang.Class.newInstance
  [18] java.util.ServiceLoader$LazyIterator.nextService
  [19] java.util.ServiceLoader$LazyIterator.next
  [20] java.util.ServiceLoader$1.next
  [21] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [22] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [23] org.apache.hadoop.fs.FileSystem.createFileSystem
  [24] org.apache.hadoop.fs.FileSystem.access$200
  [25] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [26] org.apache.hadoop.fs.FileSystem$Cache.get
  [27] org.apache.hadoop.fs.FileSystem.get
  [28] org.apache.spark.util.Utils$.getHadoopFileSystem
  [29] org.apache.spark.scheduler.EventLoggingListener.<init>
  [30] org.apache.spark.SparkContext.<init>
  [31] org.apache.spark.api.java.JavaSparkContext.<init>
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [33] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [34] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [35] java.lang.reflect.Constructor.newInstance
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.ConstructorCommand.invokeConstructor
  [40] py4j.commands.ConstructorCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16145]

--- 1551158142681734 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.codehaus.jackson.map.ser.BasicSerializerFactory.<clinit>
  [12] org.codehaus.jackson.map.ObjectMapper.<init>
  [13] org.codehaus.jackson.map.ObjectMapper.<init>
  [14] org.codehaus.jackson.map.ObjectMapper.<init>
  [15] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [18] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [19] java.lang.reflect.Constructor.newInstance
  [20] java.lang.Class.newInstance
  [21] java.util.ServiceLoader$LazyIterator.nextService
  [22] java.util.ServiceLoader$LazyIterator.next
  [23] java.util.ServiceLoader$1.next
  [24] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [25] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [26] org.apache.hadoop.fs.FileSystem.createFileSystem
  [27] org.apache.hadoop.fs.FileSystem.access$200
  [28] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [29] org.apache.hadoop.fs.FileSystem$Cache.get
  [30] org.apache.hadoop.fs.FileSystem.get
  [31] org.apache.spark.util.Utils$.getHadoopFileSystem
  [32] org.apache.spark.scheduler.EventLoggingListener.<init>
  [33] org.apache.spark.SparkContext.<init>
  [34] org.apache.spark.api.java.JavaSparkContext.<init>
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [37] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [38] java.lang.reflect.Constructor.newInstance
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.ConstructorCommand.invokeConstructor
  [43] py4j.commands.ConstructorCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158142792650 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructor0
  [12] java.lang.Class.newInstance
  [13] org.apache.htrace.commons.logging.LogFactory.createFactory
  [14] org.apache.htrace.commons.logging.LogFactory$2.run
  [15] java.security.AccessController.doPrivileged
  [16] org.apache.htrace.commons.logging.LogFactory.newFactory
  [17] org.apache.htrace.commons.logging.LogFactory.getFactory
  [18] org.apache.htrace.commons.logging.LogFactory.getLog
  [19] org.apache.htrace.SamplerBuilder.<clinit>
  [20] org.apache.hadoop.hdfs.DFSClient.<init>
  [21] org.apache.hadoop.hdfs.DFSClient.<init>
  [22] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [23] org.apache.hadoop.fs.FileSystem.createFileSystem
  [24] org.apache.hadoop.fs.FileSystem.access$200
  [25] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [26] org.apache.hadoop.fs.FileSystem$Cache.get
  [27] org.apache.hadoop.fs.FileSystem.get
  [28] org.apache.spark.util.Utils$.getHadoopFileSystem
  [29] org.apache.spark.scheduler.EventLoggingListener.<init>
  [30] org.apache.spark.SparkContext.<init>
  [31] org.apache.spark.api.java.JavaSparkContext.<init>
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [33] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [34] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [35] java.lang.reflect.Constructor.newInstance
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.ConstructorCommand.invokeConstructor
  [40] py4j.commands.ConstructorCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16145]

--- 1551158142916600 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [13] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [14] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [15] org.apache.hadoop.hdfs.DFSClient.<init>
  [16] org.apache.hadoop.hdfs.DFSClient.<init>
  [17] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [18] org.apache.hadoop.fs.FileSystem.createFileSystem
  [19] org.apache.hadoop.fs.FileSystem.access$200
  [20] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [21] org.apache.hadoop.fs.FileSystem$Cache.get
  [22] org.apache.hadoop.fs.FileSystem.get
  [23] org.apache.spark.util.Utils$.getHadoopFileSystem
  [24] org.apache.spark.scheduler.EventLoggingListener.<init>
  [25] org.apache.spark.SparkContext.<init>
  [26] org.apache.spark.api.java.JavaSparkContext.<init>
  [27] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [28] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [29] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [30] java.lang.reflect.Constructor.newInstance
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.ConstructorCommand.invokeConstructor
  [35] py4j.commands.ConstructorCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158143097139 us
  [ 0] inflate_table
  [ 1] inflate
  [ 2] Java_java_util_zip_Inflater_inflateBytes
  [ 3] java.util.zip.Inflater.inflateBytes
  [ 4] java.util.zip.Inflater.inflate
  [ 5] java.util.zip.InflaterInputStream.read
  [ 6] sun.misc.Resource.getBytes
  [ 7] java.net.URLClassLoader.defineClass
  [ 8] java.net.URLClassLoader.access$100
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] java.lang.ClassLoader.defineClass1
  [17] java.lang.ClassLoader.defineClass
  [18] java.security.SecureClassLoader.defineClass
  [19] java.net.URLClassLoader.defineClass
  [20] java.net.URLClassLoader.access$100
  [21] java.net.URLClassLoader$1.run
  [22] java.net.URLClassLoader$1.run
  [23] java.security.AccessController.doPrivileged
  [24] java.net.URLClassLoader.findClass
  [25] java.lang.ClassLoader.loadClass
  [26] sun.misc.Launcher$AppClassLoader.loadClass
  [27] java.lang.ClassLoader.loadClass
  [28] java.lang.Class.getDeclaredMethods0
  [29] java.lang.Class.privateGetDeclaredMethods
  [30] java.lang.Class.privateGetPublicMethods
  [31] java.lang.Class.privateGetPublicMethods
  [32] java.lang.Class.getMethods
  [33] sun.misc.ProxyGenerator.generateClassFile
  [34] sun.misc.ProxyGenerator.generateProxyClass
  [35] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [36] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [37] java.lang.reflect.WeakCache$Factory.get
  [38] java.lang.reflect.WeakCache.get
  [39] java.lang.reflect.Proxy.getProxyClass0
  [40] java.lang.reflect.Proxy.newProxyInstance
  [41] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [42] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [43] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [44] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [45] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [46] org.apache.hadoop.hdfs.DFSClient.<init>
  [47] org.apache.hadoop.hdfs.DFSClient.<init>
  [48] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [49] org.apache.hadoop.fs.FileSystem.createFileSystem
  [50] org.apache.hadoop.fs.FileSystem.access$200
  [51] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [52] org.apache.hadoop.fs.FileSystem$Cache.get
  [53] org.apache.hadoop.fs.FileSystem.get
  [54] org.apache.spark.util.Utils$.getHadoopFileSystem
  [55] org.apache.spark.scheduler.EventLoggingListener.<init>
  [56] org.apache.spark.SparkContext.<init>
  [57] org.apache.spark.api.java.JavaSparkContext.<init>
  [58] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [59] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [60] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [61] java.lang.reflect.Constructor.newInstance
  [62] py4j.reflection.MethodInvoker.invoke
  [63] py4j.reflection.ReflectionEngine.invoke
  [64] py4j.Gateway.invoke
  [65] py4j.commands.ConstructorCommand.invokeConstructor
  [66] py4j.commands.ConstructorCommand.execute
  [67] py4j.GatewayConnection.run
  [68] java.lang.Thread.run
  [69] [tid=16145]

--- 1551158143216842 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [12] JVM_FindClassFromCaller
  [13] Java_java_lang_Class_forName0
  [14] java.lang.Class.forName0
  [15] java.lang.Class.forName
  [16] com.sun.proxy.$Proxy13.<clinit>
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [18] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [19] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [20] java.lang.reflect.Constructor.newInstance
  [21] java.lang.reflect.Proxy.newProxyInstance
  [22] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [23] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [25] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [26] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [27] org.apache.hadoop.hdfs.DFSClient.<init>
  [28] org.apache.hadoop.hdfs.DFSClient.<init>
  [29] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [30] org.apache.hadoop.fs.FileSystem.createFileSystem
  [31] org.apache.hadoop.fs.FileSystem.access$200
  [32] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [33] org.apache.hadoop.fs.FileSystem$Cache.get
  [34] org.apache.hadoop.fs.FileSystem.get
  [35] org.apache.spark.util.Utils$.getHadoopFileSystem
  [36] org.apache.spark.scheduler.EventLoggingListener.<init>
  [37] org.apache.spark.SparkContext.<init>
  [38] org.apache.spark.api.java.JavaSparkContext.<init>
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [40] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [41] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [42] java.lang.reflect.Constructor.newInstance
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.ConstructorCommand.invokeConstructor
  [47] py4j.commands.ConstructorCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158143316579 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy13.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [20] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [27] org.apache.hadoop.fs.FileSystem.createFileSystem
  [28] org.apache.hadoop.fs.FileSystem.access$200
  [29] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [30] org.apache.hadoop.fs.FileSystem$Cache.get
  [31] org.apache.hadoop.fs.FileSystem.get
  [32] org.apache.spark.util.Utils$.getHadoopFileSystem
  [33] org.apache.spark.scheduler.EventLoggingListener.<init>
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158143416319 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy13.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [20] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [27] org.apache.hadoop.fs.FileSystem.createFileSystem
  [28] org.apache.hadoop.fs.FileSystem.access$200
  [29] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [30] org.apache.hadoop.fs.FileSystem$Cache.get
  [31] org.apache.hadoop.fs.FileSystem.get
  [32] org.apache.spark.util.Utils$.getHadoopFileSystem
  [33] org.apache.spark.scheduler.EventLoggingListener.<init>
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158143516050 us
  [ 0] java.lang.String.startsWith
  [ 1] java.lang.String.startsWith
  [ 2] sun.misc.URLClassPath$FileLoader.getResource
  [ 3] sun.misc.URLClassPath.getResource
  [ 4] java.net.URLClassLoader$1.run
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.security.AccessController.doPrivileged
  [ 7] java.net.URLClassLoader.findClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] sun.misc.Launcher$AppClassLoader.loadClass
  [10] java.lang.ClassLoader.loadClass
  [11] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto.<clinit>
  [12] java.lang.Class.forName0
  [13] java.lang.Class.forName
  [14] com.sun.proxy.$Proxy13.<clinit>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] java.lang.reflect.Proxy.newProxyInstance
  [20] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [21] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DFSClient.<init>
  [27] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [28] org.apache.hadoop.fs.FileSystem.createFileSystem
  [29] org.apache.hadoop.fs.FileSystem.access$200
  [30] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [31] org.apache.hadoop.fs.FileSystem$Cache.get
  [32] org.apache.hadoop.fs.FileSystem.get
  [33] org.apache.spark.util.Utils$.getHadoopFileSystem
  [34] org.apache.spark.scheduler.EventLoggingListener.<init>
  [35] org.apache.spark.SparkContext.<init>
  [36] org.apache.spark.api.java.JavaSparkContext.<init>
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [39] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [40] java.lang.reflect.Constructor.newInstance
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.ConstructorCommand.invokeConstructor
  [45] py4j.commands.ConstructorCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16145]

--- 1551158143615783 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto.initFields
  [13] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto.<clinit>
  [14] java.lang.Class.forName0
  [15] java.lang.Class.forName
  [16] com.sun.proxy.$Proxy13.<clinit>
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [18] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [19] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [20] java.lang.reflect.Constructor.newInstance
  [21] java.lang.reflect.Proxy.newProxyInstance
  [22] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [23] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [25] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [26] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [27] org.apache.hadoop.hdfs.DFSClient.<init>
  [28] org.apache.hadoop.hdfs.DFSClient.<init>
  [29] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [30] org.apache.hadoop.fs.FileSystem.createFileSystem
  [31] org.apache.hadoop.fs.FileSystem.access$200
  [32] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [33] org.apache.hadoop.fs.FileSystem$Cache.get
  [34] org.apache.hadoop.fs.FileSystem.get
  [35] org.apache.spark.util.Utils$.getHadoopFileSystem
  [36] org.apache.spark.scheduler.EventLoggingListener.<init>
  [37] org.apache.spark.SparkContext.<init>
  [38] org.apache.spark.api.java.JavaSparkContext.<init>
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [40] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [41] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [42] java.lang.reflect.Constructor.newInstance
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.ConstructorCommand.invokeConstructor
  [47] py4j.commands.ConstructorCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158143715566 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy13.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [20] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [27] org.apache.hadoop.fs.FileSystem.createFileSystem
  [28] org.apache.hadoop.fs.FileSystem.access$200
  [29] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [30] org.apache.hadoop.fs.FileSystem$Cache.get
  [31] org.apache.hadoop.fs.FileSystem.get
  [32] org.apache.spark.util.Utils$.getHadoopFileSystem
  [33] org.apache.spark.scheduler.EventLoggingListener.<init>
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158143815259 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [11] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [12] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [13] org.apache.hadoop.hdfs.DFSClient.<init>
  [14] org.apache.hadoop.hdfs.DFSClient.<init>
  [15] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [16] org.apache.hadoop.fs.FileSystem.createFileSystem
  [17] org.apache.hadoop.fs.FileSystem.access$200
  [18] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [19] org.apache.hadoop.fs.FileSystem$Cache.get
  [20] org.apache.hadoop.fs.FileSystem.get
  [21] org.apache.spark.util.Utils$.getHadoopFileSystem
  [22] org.apache.spark.scheduler.EventLoggingListener.<init>
  [23] org.apache.spark.SparkContext.<init>
  [24] org.apache.spark.api.java.JavaSparkContext.<init>
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [26] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [27] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [28] java.lang.reflect.Constructor.newInstance
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.ConstructorCommand.invokeConstructor
  [33] py4j.commands.ConstructorCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158143921422 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy14.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.io.retry.RetryProxy.create
  [20] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [23] org.apache.hadoop.hdfs.DFSClient.<init>
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [26] org.apache.hadoop.fs.FileSystem.createFileSystem
  [27] org.apache.hadoop.fs.FileSystem.access$200
  [28] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [29] org.apache.hadoop.fs.FileSystem$Cache.get
  [30] org.apache.hadoop.fs.FileSystem.get
  [31] org.apache.spark.util.Utils$.getHadoopFileSystem
  [32] org.apache.spark.scheduler.EventLoggingListener.<init>
  [33] org.apache.spark.SparkContext.<init>
  [34] org.apache.spark.api.java.JavaSparkContext.<init>
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [37] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [38] java.lang.reflect.Constructor.newInstance
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.ConstructorCommand.invokeConstructor
  [43] py4j.commands.ConstructorCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158144045367 us
  [ 0] SignatureVerifier::is_valid_type(char const*, long)
  [ 1] SignatureVerifier::is_valid_method_signature(Symbol*)
  [ 2] ClassVerifier::verify_invoke_instructions(RawBytecodeStream*, unsigned int, StackMapFrame*, bool, bool*, VerificationType, constantPoolHandle, StackMapTable*, Thread*)
  [ 3] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 4] ClassVerifier::verify_class(Thread*)
  [ 5] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [10] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [11] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [13] org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto.newBuilder
  [14] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequestHeader
  [15] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [16] com.sun.proxy.$Proxy13.getFileInfo
  [17] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [23] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [24] com.sun.proxy.$Proxy14.getFileInfo
  [25] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [26] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [27] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [28] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [29] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [30] org.apache.spark.scheduler.EventLoggingListener.start
  [31] org.apache.spark.SparkContext.<init>
  [32] org.apache.spark.api.java.JavaSparkContext.<init>
  [33] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [34] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [35] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [36] java.lang.reflect.Constructor.newInstance
  [37] py4j.reflection.MethodInvoker.invoke
  [38] py4j.reflection.ReflectionEngine.invoke
  [39] py4j.Gateway.invoke
  [40] py4j.commands.ConstructorCommand.invokeConstructor
  [41] py4j.commands.ConstructorCommand.execute
  [42] py4j.GatewayConnection.run
  [43] java.lang.Thread.run
  [44] [tid=16145]

--- 1551158144151865 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.hadoop.ipc.Client$Connection.sendRpcRequest
  [16] org.apache.hadoop.ipc.Client.call
  [17] org.apache.hadoop.ipc.Client.call
  [18] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [19] com.sun.proxy.$Proxy13.getFileInfo
  [20] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [26] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [27] com.sun.proxy.$Proxy14.getFileInfo
  [28] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [29] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [30] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [31] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [32] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [33] org.apache.spark.scheduler.EventLoggingListener.start
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158144316897 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [13] sun.reflect.NativeMethodAccessorImpl.invoke0
  [14] sun.reflect.NativeMethodAccessorImpl.invoke
  [15] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [16] java.lang.reflect.Method.invoke
  [17] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [18] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [19] com.sun.proxy.$Proxy14.getFileInfo
  [20] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [21] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [22] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [23] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [24] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [25] org.apache.spark.scheduler.EventLoggingListener.start
  [26] org.apache.spark.SparkContext.<init>
  [27] org.apache.spark.api.java.JavaSparkContext.<init>
  [28] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [30] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [31] java.lang.reflect.Constructor.newInstance
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.ConstructorCommand.invokeConstructor
  [36] py4j.commands.ConstructorCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16145]

--- 1551158144420535 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.hadoop.hdfs.protocolPB.PBHelper$1.<clinit>
  [16] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [17] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [18] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [24] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [25] com.sun.proxy.$Proxy14.create
  [26] org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate
  [27] org.apache.hadoop.hdfs.DFSClient.create
  [28] org.apache.hadoop.hdfs.DFSClient.create
  [29] org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall
  [30] org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall
  [31] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [32] org.apache.hadoop.hdfs.DistributedFileSystem.create
  [33] org.apache.hadoop.hdfs.DistributedFileSystem.create
  [34] org.apache.hadoop.fs.FileSystem.create
  [35] org.apache.hadoop.fs.FileSystem.create
  [36] org.apache.hadoop.fs.FileSystem.create
  [37] org.apache.hadoop.fs.FileSystem.create
  [38] org.apache.spark.scheduler.EventLoggingListener.start
  [39] org.apache.spark.SparkContext.<init>
  [40] org.apache.spark.api.java.JavaSparkContext.<init>
  [41] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [42] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [43] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [44] java.lang.reflect.Constructor.newInstance
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.ConstructorCommand.invokeConstructor
  [49] py4j.commands.ConstructorCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158144546958 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [11] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [13] org.apache.spark.util.JsonProtocol$.<init>
  [14] org.apache.spark.util.JsonProtocol$.<clinit>
  [15] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [16] org.apache.spark.scheduler.EventLoggingListener.start
  [17] org.apache.spark.SparkContext.<init>
  [18] org.apache.spark.api.java.JavaSparkContext.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.ConstructorCommand.invokeConstructor
  [27] py4j.commands.ConstructorCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16145]

--- 1551158144646690 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] com.fasterxml.jackson.module.scala.ser.TupleSerializerModule$$anonfun$1.apply
  [21] com.fasterxml.jackson.module.scala.ser.TupleSerializerModule$$anonfun$1.apply
  [22] com.fasterxml.jackson.module.scala.JacksonModule$$anonfun$setupModule$1.apply
  [23] com.fasterxml.jackson.module.scala.JacksonModule$$anonfun$setupModule$1.apply
  [24] scala.collection.immutable.List.foreach
  [25] com.fasterxml.jackson.module.scala.JacksonModule$class.setupModule
  [26] com.fasterxml.jackson.module.scala.DefaultScalaModule.setupModule
  [27] com.fasterxml.jackson.databind.ObjectMapper.registerModule
  [28] org.apache.spark.util.JsonProtocol$.<init>
  [29] org.apache.spark.util.JsonProtocol$.<clinit>
  [30] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [31] org.apache.spark.scheduler.EventLoggingListener.start
  [32] org.apache.spark.SparkContext.<init>
  [33] org.apache.spark.api.java.JavaSparkContext.<init>
  [34] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [36] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [37] java.lang.reflect.Constructor.newInstance
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.ConstructorCommand.invokeConstructor
  [42] py4j.commands.ConstructorCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16145]

--- 1551158144750116 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] com.fasterxml.jackson.core.JsonFactory._createGenerator
  [13] com.fasterxml.jackson.core.JsonFactory.createGenerator
  [14] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [15] org.json4s.jackson.JsonMethods$class.compact
  [16] org.json4s.jackson.JsonMethods$.compact
  [17] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [18] org.apache.spark.scheduler.EventLoggingListener.start
  [19] org.apache.spark.SparkContext.<init>
  [20] org.apache.spark.api.java.JavaSparkContext.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.ConstructorCommand.invokeConstructor
  [29] py4j.commands.ConstructorCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16145]

--- 1551158144866626 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.SparkContext.<init>
  [10] org.apache.spark.api.java.JavaSparkContext.<init>
  [11] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [13] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [14] java.lang.reflect.Constructor.newInstance
  [15] py4j.reflection.MethodInvoker.invoke
  [16] py4j.reflection.ReflectionEngine.invoke
  [17] py4j.Gateway.invoke
  [18] py4j.commands.ConstructorCommand.invokeConstructor
  [19] py4j.commands.ConstructorCommand.execute
  [20] py4j.GatewayConnection.run
  [21] java.lang.Thread.run
  [22] [tid=16145]

--- 1551158145092968 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] py4j.reflection.CurrentThreadClassLoadingStrategy.classForName
  [14] py4j.reflection.ReflectionUtil.classForName
  [15] py4j.reflection.TypeUtil.forName
  [16] py4j.commands.ReflectionCommand.getUnknownMember
  [17] py4j.commands.ReflectionCommand.execute
  [18] py4j.GatewayConnection.run
  [19] java.lang.Thread.run
  [20] [tid=16145]

--- 1551158145145164 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] com.google.protobuf.UnknownFieldSet$Field.newBuilder
  [13] com.google.protobuf.UnknownFieldSet$Field.<clinit>
  [14] com.google.protobuf.UnknownFieldSet$Builder.getFieldBuilder
  [15] com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom
  [16] com.google.protobuf.GeneratedMessage.parseUnknownField
  [17] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto.<init>
  [18] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto.<init>
  [19] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$1.parsePartialFrom
  [20] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$1.parsePartialFrom
  [21] com.google.protobuf.CodedInputStream.readMessage
  [22] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto.<init>
  [23] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto.<init>
  [24] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$1.parsePartialFrom
  [25] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$1.parsePartialFrom
  [26] com.google.protobuf.CodedInputStream.readMessage
  [27] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto.<init>
  [28] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto.<init>
  [29] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$1.parsePartialFrom
  [30] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$1.parsePartialFrom
  [31] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder.mergeFrom
  [32] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder.mergeFrom
  [33] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [34] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [35] com.google.protobuf.AbstractMessageLite$Builder.mergeFrom
  [36] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [37] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [38] com.google.protobuf.AbstractMessageLite$Builder.mergeFrom
  [39] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [40] com.google.protobuf.AbstractMessage$Builder.mergeFrom
  [41] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [42] com.sun.proxy.$Proxy13.addBlock
  [43] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock
  [44] sun.reflect.NativeMethodAccessorImpl.invoke0
  [45] sun.reflect.NativeMethodAccessorImpl.invoke
  [46] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [47] java.lang.reflect.Method.invoke
  [48] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [49] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [50] com.sun.proxy.$Proxy14.addBlock
  [51] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock
  [52] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [53] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [54] [tid=16253]

--- 1551158145295852 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.buildClientHeader
  [13] org.apache.hadoop.hdfs.protocol.datatransfer.Sender.writeBlock
  [14] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream
  [15] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [16] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [17] [tid=16253]

--- 1551158145310939 us
  [ 0] java.util.zip.ZipFile.getEntry
  [ 1] java.util.zip.ZipFile.getEntry
  [ 2] java.util.jar.JarFile.getEntry
  [ 3] java.util.jar.JarFile.getJarEntry
  [ 4] sun.misc.URLClassPath$JarLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] java.lang.ClassLoader.defineClass1
  [14] java.lang.ClassLoader.defineClass
  [15] java.security.SecureClassLoader.defineClass
  [16] java.net.URLClassLoader.defineClass
  [17] java.net.URLClassLoader.access$100
  [18] java.net.URLClassLoader$1.run
  [19] java.net.URLClassLoader$1.run
  [20] java.security.AccessController.doPrivileged
  [21] java.net.URLClassLoader.findClass
  [22] java.lang.ClassLoader.loadClass
  [23] sun.misc.Launcher$AppClassLoader.loadClass
  [24] java.lang.ClassLoader.loadClass
  [25] java.lang.ClassLoader.defineClass1
  [26] java.lang.ClassLoader.defineClass
  [27] java.security.SecureClassLoader.defineClass
  [28] java.net.URLClassLoader.defineClass
  [29] java.net.URLClassLoader.access$100
  [30] java.net.URLClassLoader$1.run
  [31] java.net.URLClassLoader$1.run
  [32] java.security.AccessController.doPrivileged
  [33] java.net.URLClassLoader.findClass
  [34] java.lang.ClassLoader.loadClass
  [35] sun.misc.Launcher$AppClassLoader.loadClass
  [36] java.lang.ClassLoader.loadClass
  [37] org.apache.spark.sql.internal.SQLConf$.<init>
  [38] org.apache.spark.sql.internal.SQLConf$.<clinit>
  [39] org.apache.spark.sql.SparkSession.<init>
  [40] org.apache.spark.sql.SparkSession.<init>
  [41] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [42] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [43] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [44] java.lang.reflect.Constructor.newInstance
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.ConstructorCommand.invokeConstructor
  [49] py4j.commands.ConstructorCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158145410737 us
  [ 0] SpaceManager::allocate(unsigned long)
  [ 1] Metaspace::allocate(ClassLoaderData*, unsigned long, bool, MetaspaceObj::Type, Thread*)
  [ 2] Method::allocate(ClassLoaderData*, int, AccessFlags, InlineTableSizes*, ConstMethod::MethodType, Thread*)
  [ 3] ClassFileParser::parse_method(bool, AccessFlags*, Thread*)
  [ 4] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 5] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 6] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 7] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 8] JVM_DefineClassWithSource
  [ 9] Java_java_lang_ClassLoader_defineClass1
  [10] java.lang.ClassLoader.defineClass1
  [11] java.lang.ClassLoader.defineClass
  [12] java.security.SecureClassLoader.defineClass
  [13] java.net.URLClassLoader.defineClass
  [14] java.net.URLClassLoader.access$100
  [15] java.net.URLClassLoader$1.run
  [16] java.net.URLClassLoader$1.run
  [17] java.security.AccessController.doPrivileged
  [18] java.net.URLClassLoader.findClass
  [19] java.lang.ClassLoader.loadClass
  [20] sun.misc.Launcher$AppClassLoader.loadClass
  [21] java.lang.ClassLoader.loadClass
  [22] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto.newBuilder
  [23] org.apache.hadoop.hdfs.protocol.datatransfer.Sender.writeBlock
  [24] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream
  [25] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [26] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [27] [tid=16253]

--- 1551158145537226 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.internal.SharedState.<init>
  [10] org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply
  [11] org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply
  [12] scala.Option.getOrElse
  [13] org.apache.spark.sql.SparkSession.sharedState$lzycompute
  [14] org.apache.spark.sql.SparkSession.sharedState
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [16] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [17] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [18] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [19] scala.Option.getOrElse
  [20] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [21] org.apache.spark.sql.SparkSession.sessionState
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16145]

--- 1551158145661491 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158145758254 us
  [ 0] org.apache.xerces.dom.DeferredDocumentImpl.getPrevSibling
  [ 1] org.apache.xerces.dom.DeferredDocumentImpl.getPrevSibling
  [ 2] org.apache.xerces.dom.DeferredDocumentImpl.synchronizeChildren
  [ 3] org.apache.xerces.dom.DeferredElementNSImpl.synchronizeChildren
  [ 4] org.apache.xerces.dom.ParentNode.getChildNodes
  [ 5] org.apache.hadoop.conf.Configuration.loadResource
  [ 6] org.apache.hadoop.conf.Configuration.loadResources
  [ 7] org.apache.hadoop.conf.Configuration.getProps
  [ 8] org.apache.hadoop.conf.Configuration.get
  [ 9] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.<clinit>
  [10] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream
  [11] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [12] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [13] [tid=16253]

--- 1551158145811789 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158145960984 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158146089290 us
  [ 0] __audit_syscall_entry_[k]
  [ 1] syscall_trace_enter_[k]
  [ 2] do_syscall_64_[k]
  [ 3] entry_SYSCALL_64_after_hwframe_[k]
  [ 4] __vdso_clock_gettime
  [ 5] __GI___clock_gettime
  [ 6] [unknown]
  [ 7] java.lang.ClassLoader.findBootstrapClass
  [ 8] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 9] java.lang.ClassLoader.loadClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] java.lang.ClassLoader.defineClass1
  [14] java.lang.ClassLoader.defineClass
  [15] java.security.SecureClassLoader.defineClass
  [16] java.net.URLClassLoader.defineClass
  [17] java.net.URLClassLoader.access$100
  [18] java.net.URLClassLoader$1.run
  [19] java.net.URLClassLoader$1.run
  [20] java.security.AccessController.doPrivileged
  [21] java.net.URLClassLoader.findClass
  [22] java.lang.ClassLoader.loadClass
  [23] sun.misc.Launcher$AppClassLoader.loadClass
  [24] java.lang.ClassLoader.loadClass
  [25] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [26] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [27] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [28] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [31] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [32] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [33] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [34] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [35] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [36] scala.Option.getOrElse
  [37] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [38] org.apache.spark.sql.SparkSession.sessionState
  [39] sun.reflect.NativeMethodAccessorImpl.invoke0
  [40] sun.reflect.NativeMethodAccessorImpl.invoke
  [41] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [42] java.lang.reflect.Method.invoke
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.AbstractCommand.invokeMethod
  [47] py4j.commands.CallCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158146196635 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158146308328 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158146417214 us
  [ 0] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expressionInfo
  [ 1] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [ 2] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [ 3] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [ 4] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [ 5] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [ 6] scala.Option.getOrElse
  [ 7] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [ 8] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [ 9] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [10] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [11] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [12] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [13] scala.Option.getOrElse
  [14] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [15] org.apache.spark.sql.SparkSession.sessionState
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] py4j.reflection.MethodInvoker.invoke
  [21] py4j.reflection.ReflectionEngine.invoke
  [22] py4j.Gateway.invoke
  [23] py4j.commands.AbstractCommand.invokeMethod
  [24] py4j.commands.CallCommand.execute
  [25] py4j.GatewayConnection.run
  [26] java.lang.Thread.run
  [27] [tid=16145]

--- 1551158146519969 us
  [ 0] blk_flush_plug_list_[k]
  [ 1] blk_finish_plug_[k]
  [ 2] __do_page_cache_readahead_[k]
  [ 3] ondemand_readahead_[k]
  [ 4] generic_file_read_iter_[k]
  [ 5] xfs_file_buffered_aio_read	[xfs]_[k]
  [ 6] xfs_file_read_iter	[xfs]_[k]
  [ 7] __vfs_read_[k]
  [ 8] vfs_read_[k]
  [ 9] sys_read_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] read
  [13] java.util.zip.ZipFile.read
  [14] java.util.zip.ZipFile.access$1400
  [15] java.util.zip.ZipFile$ZipFileInputStream.read
  [16] java.util.zip.ZipFile$ZipFileInflaterInputStream.fill
  [17] java.util.zip.InflaterInputStream.read
  [18] sun.misc.Resource.getBytes
  [19] java.net.URLClassLoader.defineClass
  [20] java.net.URLClassLoader.access$100
  [21] java.net.URLClassLoader$1.run
  [22] java.net.URLClassLoader$1.run
  [23] java.security.AccessController.doPrivileged
  [24] java.net.URLClassLoader.findClass
  [25] java.lang.ClassLoader.loadClass
  [26] sun.misc.Launcher$AppClassLoader.loadClass
  [27] java.lang.ClassLoader.loadClass
  [28] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [29] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [30] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [31] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [32] scala.Option.getOrElse
  [33] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [34] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [35] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [36] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [37] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [38] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [39] scala.Option.getOrElse
  [40] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [41] org.apache.spark.sql.SparkSession.sessionState
  [42] sun.reflect.NativeMethodAccessorImpl.invoke0
  [43] sun.reflect.NativeMethodAccessorImpl.invoke
  [44] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [45] java.lang.reflect.Method.invoke
  [46] py4j.reflection.MethodInvoker.invoke
  [47] py4j.reflection.ReflectionEngine.invoke
  [48] py4j.Gateway.invoke
  [49] py4j.commands.AbstractCommand.invokeMethod
  [50] py4j.commands.CallCommand.execute
  [51] py4j.GatewayConnection.run
  [52] java.lang.Thread.run
  [53] [tid=16145]

--- 1551158146623687 us
  [ 0] LatestMethodCache::get_method()
  [ 1] SharedRuntime::register_finalizer(JavaThread*, oopDesc*)
  [ 2] [tid=16145]

--- 1551158146723415 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158146823195 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158146940991 us
  [ 0] sun.nio.cs.UTF_8$Encoder.encode
  [ 1] java.util.zip.ZipCoder.getBytes
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.jar.JarFile.getEntry
  [ 4] java.util.jar.JarFile.getJarEntry
  [ 5] sun.misc.URLClassPath$JarLoader.getResource
  [ 6] sun.misc.URLClassPath.getResource
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] java.lang.Class.getDeclaredConstructors0
  [15] java.lang.Class.privateGetDeclaredConstructors
  [16] java.lang.Class.getConstructors
  [17] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [18] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [19] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [24] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [25] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [26] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [27] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [28] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [31] org.apache.spark.sql.SparkSession.sessionState
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16145]

--- 1551158147040726 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [16] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [19] scala.Option.getOrElse
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [22] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [23] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [24] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [25] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [26] scala.Option.getOrElse
  [27] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [28] org.apache.spark.sql.SparkSession.sessionState
  [29] sun.reflect.NativeMethodAccessorImpl.invoke0
  [30] sun.reflect.NativeMethodAccessorImpl.invoke
  [31] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [32] java.lang.reflect.Method.invoke
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.AbstractCommand.invokeMethod
  [37] py4j.commands.CallCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16145]

--- 1551158147140460 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.types.DoubleType.<init>
  [12] org.apache.spark.sql.types.DoubleType$.<init>
  [13] org.apache.spark.sql.types.DoubleType$.<clinit>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [15] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [18] scala.Option.getOrElse
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [22] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [25] scala.Option.getOrElse
  [26] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [27] org.apache.spark.sql.SparkSession.sessionState
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16145]

--- 1551158147241772 us
  [ 0] ClassVerifier::verify_return_value(VerificationType, VerificationType, unsigned short, StackMapFrame*, Thread*)
  [ 1] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 2] ClassVerifier::verify_class(Thread*)
  [ 3] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 4] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser$lzycompute
  [10] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser
  [11] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [12] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [13] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [14] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [15] scala.Option.getOrElse
  [16] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [17] org.apache.spark.sql.SparkSession.sessionState
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] py4j.reflection.MethodInvoker.invoke
  [23] py4j.reflection.ReflectionEngine.invoke
  [24] py4j.Gateway.invoke
  [25] py4j.commands.AbstractCommand.invokeMethod
  [26] py4j.commands.CallCommand.execute
  [27] py4j.GatewayConnection.run
  [28] java.lang.Thread.run
  [29] [tid=16145]

--- 1551158147370595 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.sql.execution.SparkSqlParser.<init>
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser$lzycompute
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [19] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [20] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [21] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [24] org.apache.spark.sql.SparkSession.sessionState
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158147493227 us
  [ 0] java.util.zip.ZipCoder.getBytes
  [ 1] java.util.zip.ZipFile.getEntry
  [ 2] java.util.jar.JarFile.getEntry
  [ 3] java.util.jar.JarFile.getJarEntry
  [ 4] sun.misc.URLClassPath$JarLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef$.forDriver
  [14] org.apache.spark.sql.streaming.StreamingQueryManager.<init>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [17] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [18] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [19] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [22] org.apache.spark.sql.SparkSession.sessionState
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16145]

--- 1551158147598420 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2$$anonfun$apply$5.apply
  [10] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2$$anonfun$apply$5.apply
  [11] scala.collection.TraversableLike$$anonfun$map$1.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.IndexedSeqOptimized$class.foreach
  [14] scala.collection.mutable.ArrayOps$ofRef.foreach
  [15] scala.collection.TraversableLike$class.map
  [16] scala.collection.mutable.ArrayOps$ofRef.map
  [17] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2.apply
  [18] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2.apply
  [19] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [20] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [21] scala.collection.immutable.List.foreach
  [22] scala.collection.TraversableLike$class.flatMap
  [23] scala.collection.immutable.List.flatMap
  [24] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.apply
  [25] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$._descriptorFor
  [26] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.fieldName
  [27] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.findImplicitPropertyName
  [28] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.findImplicitPropertyName
  [29] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addFields
  [30] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [31] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [32] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [33] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [34] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [35] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [36] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [37] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [38] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [39] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [40] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [41] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [42] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [43] org.apache.spark.rdd.RDDOperationScope.toJson
  [44] org.apache.spark.rdd.RDDOperationScope$.withScope
  [45] org.apache.spark.rdd.RDDOperationScope$.withScope
  [46] org.apache.spark.SparkContext.withScope
  [47] org.apache.spark.SparkContext.textFile
  [48] org.apache.spark.api.java.JavaSparkContext.textFile
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16145]

--- 1551158147698688 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [10] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [11] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [12] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [13] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [14] InstanceKlass::initialize(Thread*)
  [15] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [16] scala.collection.SeqLike$class.view
  [17] scala.collection.AbstractSeq.view
  [18] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.hasCreatorAnnotation
  [19] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.hasCreatorAnnotation
  [20] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreatorParam
  [21] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreators
  [22] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [23] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [24] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [25] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [26] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [27] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [28] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [29] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [30] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [31] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [32] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [33] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [34] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [35] org.apache.spark.rdd.RDDOperationScope.toJson
  [36] org.apache.spark.rdd.RDDOperationScope$.withScope
  [37] org.apache.spark.rdd.RDDOperationScope$.withScope
  [38] org.apache.spark.SparkContext.withScope
  [39] org.apache.spark.SparkContext.textFile
  [40] org.apache.spark.api.java.JavaSparkContext.textFile
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158147798421 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.collection.SeqViewLike$class.newFlatMapped
  [11] scala.collection.SeqLike$$anon$2.newFlatMapped
  [12] scala.collection.SeqLike$$anon$2.newFlatMapped
  [13] scala.collection.TraversableViewLike$class.flatMap
  [14] scala.collection.SeqLike$$anon$2.flatMap
  [15] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.hasCreatorAnnotation
  [16] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.hasCreatorAnnotation
  [17] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreatorParam
  [18] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreators
  [19] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [20] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [21] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [22] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [23] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [24] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [25] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [26] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [27] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [28] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [29] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [30] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [31] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [32] org.apache.spark.rdd.RDDOperationScope.toJson
  [33] org.apache.spark.rdd.RDDOperationScope$.withScope
  [34] org.apache.spark.rdd.RDDOperationScope$.withScope
  [35] org.apache.spark.SparkContext.withScope
  [36] org.apache.spark.SparkContext.textFile
  [37] org.apache.spark.api.java.JavaSparkContext.textFile
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16145]

--- 1551158147898179 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] com.fasterxml.jackson.core.io.SerializedString.asQuotedChars
  [13] com.fasterxml.jackson.core.json.WriterBasedJsonGenerator._writeFieldName
  [14] com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.writeFieldName
  [15] com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField
  [16] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields
  [17] com.fasterxml.jackson.databind.ser.BeanSerializer.serialize
  [18] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [19] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [20] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [21] org.apache.spark.rdd.RDDOperationScope.toJson
  [22] org.apache.spark.rdd.RDDOperationScope$.withScope
  [23] org.apache.spark.rdd.RDDOperationScope$.withScope
  [24] org.apache.spark.SparkContext.withScope
  [25] org.apache.spark.SparkContext.textFile
  [26] org.apache.spark.api.java.JavaSparkContext.textFile
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158147998115 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] com.fasterxml.jackson.databind.deser.BeanDeserializerBuilder.build
  [11] com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.buildBeanDeserializer
  [12] com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer
  [13] com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2
  [14] com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer
  [15] com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2
  [16] com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer
  [17] com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer
  [18] com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer
  [19] com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer
  [20] com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose
  [21] com.fasterxml.jackson.databind.ObjectMapper.readValue
  [22] org.apache.spark.rdd.RDDOperationScope$.fromJson
  [23] org.apache.spark.rdd.RDDOperationScope$$anonfun$5.apply
  [24] org.apache.spark.rdd.RDDOperationScope$$anonfun$5.apply
  [25] scala.Option.map
  [26] org.apache.spark.rdd.RDDOperationScope$.withScope
  [27] org.apache.spark.rdd.RDDOperationScope$.withScope
  [28] org.apache.spark.SparkContext.withScope
  [29] org.apache.spark.SparkContext.hadoopFile
  [30] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [31] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [32] org.apache.spark.rdd.RDDOperationScope$.withScope
  [33] org.apache.spark.rdd.RDDOperationScope$.withScope
  [34] org.apache.spark.SparkContext.withScope
  [35] org.apache.spark.SparkContext.textFile
  [36] org.apache.spark.api.java.JavaSparkContext.textFile
  [37] sun.reflect.NativeMethodAccessorImpl.invoke0
  [38] sun.reflect.NativeMethodAccessorImpl.invoke
  [39] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [40] java.lang.reflect.Method.invoke
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.AbstractCommand.invokeMethod
  [45] py4j.commands.CallCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16145]

--- 1551158148097863 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.util.collection.SizeTracker$class.$init$
  [11] org.apache.spark.util.collection.SizeTrackingVector.<init>
  [12] org.apache.spark.storage.memory.DeserializedValuesHolder.<init>
  [13] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [14] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [15] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [16] org.apache.spark.storage.BlockManager.doPut
  [17] org.apache.spark.storage.BlockManager.doPutIterator
  [18] org.apache.spark.storage.BlockManager.putIterator
  [19] org.apache.spark.storage.BlockManager.putSingle
  [20] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [21] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [22] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [23] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [24] org.apache.spark.SparkContext.broadcast
  [25] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [26] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [27] org.apache.spark.rdd.RDDOperationScope$.withScope
  [28] org.apache.spark.rdd.RDDOperationScope$.withScope
  [29] org.apache.spark.SparkContext.withScope
  [30] org.apache.spark.SparkContext.hadoopFile
  [31] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [32] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [33] org.apache.spark.rdd.RDDOperationScope$.withScope
  [34] org.apache.spark.rdd.RDDOperationScope$.withScope
  [35] org.apache.spark.SparkContext.withScope
  [36] org.apache.spark.SparkContext.textFile
  [37] org.apache.spark.api.java.JavaSparkContext.textFile
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16145]

--- 1551158148197653 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] sun.management.ManagementFactoryHelper.getMemoryMXBean
  [11] java.lang.management.PlatformComponent$3.getMXBeans
  [12] java.lang.management.PlatformComponent.getMXBeans
  [13] java.lang.management.ManagementFactory.getPlatformMBeanServer
  [14] org.apache.spark.util.SizeEstimator$.getIsCompressedOops
  [15] org.apache.spark.util.SizeEstimator$.initialize
  [16] org.apache.spark.util.SizeEstimator$.<init>
  [17] org.apache.spark.util.SizeEstimator$.<clinit>
  [18] org.apache.spark.util.collection.SizeTracker$class.takeSample
  [19] org.apache.spark.util.collection.SizeTracker$class.resetSamples
  [20] org.apache.spark.util.collection.SizeTrackingVector.resetSamples
  [21] org.apache.spark.util.collection.SizeTracker$class.$init$
  [22] org.apache.spark.util.collection.SizeTrackingVector.<init>
  [23] org.apache.spark.storage.memory.DeserializedValuesHolder.<init>
  [24] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [25] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [26] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [27] org.apache.spark.storage.BlockManager.doPut
  [28] org.apache.spark.storage.BlockManager.doPutIterator
  [29] org.apache.spark.storage.BlockManager.putIterator
  [30] org.apache.spark.storage.BlockManager.putSingle
  [31] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [32] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [33] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [34] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [35] org.apache.spark.SparkContext.broadcast
  [36] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [37] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [38] org.apache.spark.rdd.RDDOperationScope$.withScope
  [39] org.apache.spark.rdd.RDDOperationScope$.withScope
  [40] org.apache.spark.SparkContext.withScope
  [41] org.apache.spark.SparkContext.hadoopFile
  [42] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [43] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [44] org.apache.spark.rdd.RDDOperationScope$.withScope
  [45] org.apache.spark.rdd.RDDOperationScope$.withScope
  [46] org.apache.spark.SparkContext.withScope
  [47] org.apache.spark.SparkContext.textFile
  [48] org.apache.spark.api.java.JavaSparkContext.textFile
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16145]

--- 1551158148298302 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.util.collection.OpenHashSet$mcI$sp.<init>
  [12] org.apache.spark.util.collection.OpenHashSet$mcI$sp.<init>
  [13] org.apache.spark.util.SizeEstimator$.visitArray
  [14] org.apache.spark.util.SizeEstimator$.visitSingleObject
  [15] org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate
  [16] org.apache.spark.util.SizeEstimator$.estimate
  [17] org.apache.spark.util.collection.SizeTracker$class.takeSample
  [18] org.apache.spark.util.collection.SizeTracker$class.afterUpdate
  [19] org.apache.spark.util.collection.SizeTrackingVector.$plus$eq
  [20] org.apache.spark.storage.memory.DeserializedValuesHolder.storeValue
  [21] org.apache.spark.storage.memory.MemoryStore.putIterator
  [22] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [23] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [24] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [25] org.apache.spark.storage.BlockManager.doPut
  [26] org.apache.spark.storage.BlockManager.doPutIterator
  [27] org.apache.spark.storage.BlockManager.putIterator
  [28] org.apache.spark.storage.BlockManager.putSingle
  [29] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [30] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [31] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [32] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [33] org.apache.spark.SparkContext.broadcast
  [34] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [35] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [36] org.apache.spark.rdd.RDDOperationScope$.withScope
  [37] org.apache.spark.rdd.RDDOperationScope$.withScope
  [38] org.apache.spark.SparkContext.withScope
  [39] org.apache.spark.SparkContext.hadoopFile
  [40] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [41] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [42] org.apache.spark.rdd.RDDOperationScope$.withScope
  [43] org.apache.spark.rdd.RDDOperationScope$.withScope
  [44] org.apache.spark.SparkContext.withScope
  [45] org.apache.spark.SparkContext.textFile
  [46] org.apache.spark.api.java.JavaSparkContext.textFile
  [47] sun.reflect.NativeMethodAccessorImpl.invoke0
  [48] sun.reflect.NativeMethodAccessorImpl.invoke
  [49] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [50] java.lang.reflect.Method.invoke
  [51] py4j.reflection.MethodInvoker.invoke
  [52] py4j.reflection.ReflectionEngine.invoke
  [53] py4j.Gateway.invoke
  [54] py4j.commands.AbstractCommand.invokeMethod
  [55] py4j.commands.CallCommand.execute
  [56] py4j.GatewayConnection.run
  [57] java.lang.Thread.run
  [58] [tid=16145]

--- 1551158148399793 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] net.jpountz.lz4.LZ4JNICompressor.compress
  [13] net.jpountz.lz4.LZ4Factory.<init>
  [14] net.jpountz.lz4.LZ4Factory.instance
  [15] net.jpountz.lz4.LZ4Factory.nativeInstance
  [16] net.jpountz.lz4.LZ4Factory.fastestInstance
  [17] net.jpountz.lz4.LZ4BlockOutputStream.<init>
  [18] org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream
  [19] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$4.apply
  [20] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$4.apply
  [21] scala.Option.map
  [22] org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject
  [23] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [24] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [25] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [26] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [27] org.apache.spark.SparkContext.broadcast
  [28] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [29] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [30] org.apache.spark.rdd.RDDOperationScope$.withScope
  [31] org.apache.spark.rdd.RDDOperationScope$.withScope
  [32] org.apache.spark.SparkContext.withScope
  [33] org.apache.spark.SparkContext.hadoopFile
  [34] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [35] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [36] org.apache.spark.rdd.RDDOperationScope$.withScope
  [37] org.apache.spark.rdd.RDDOperationScope$.withScope
  [38] org.apache.spark.SparkContext.withScope
  [39] org.apache.spark.SparkContext.textFile
  [40] org.apache.spark.api.java.JavaSparkContext.textFile
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158148499582 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.Predef$.longArrayOps
  [10] org.apache.spark.util.io.ChunkedByteBuffer.<init>
  [11] org.apache.spark.util.io.ChunkedByteBufferOutputStream.toChunkedByteBuffer
  [12] org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject
  [13] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [14] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [15] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [16] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [17] org.apache.spark.SparkContext.broadcast
  [18] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [19] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [20] org.apache.spark.rdd.RDDOperationScope$.withScope
  [21] org.apache.spark.rdd.RDDOperationScope$.withScope
  [22] org.apache.spark.SparkContext.withScope
  [23] org.apache.spark.SparkContext.hadoopFile
  [24] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [25] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [26] org.apache.spark.rdd.RDDOperationScope$.withScope
  [27] org.apache.spark.rdd.RDDOperationScope$.withScope
  [28] org.apache.spark.SparkContext.withScope
  [29] org.apache.spark.SparkContext.textFile
  [30] org.apache.spark.api.java.JavaSparkContext.textFile
  [31] sun.reflect.NativeMethodAccessorImpl.invoke0
  [32] sun.reflect.NativeMethodAccessorImpl.invoke
  [33] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [34] java.lang.reflect.Method.invoke
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.AbstractCommand.invokeMethod
  [39] py4j.commands.CallCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16145]

--- 1551158148536562 us
  [ 0] LinkResolver::resolve_method(methodHandle&, KlassHandle, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 1] LinkResolver::linktime_resolve_virtual_method(methodHandle&, KlassHandle, Symbol*, Symbol*, KlassHandle, bool, Thread*)
  [ 2] LinkResolver::resolve_virtual_call(CallInfo&, Handle, KlassHandle, KlassHandle, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 3] JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)
  [ 4] JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Handle, Thread*)
  [ 5] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 6] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 7] SystemDictionary::resolve_or_fail(Symbol*, Handle, Handle, bool, Thread*)
  [ 8] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [ 9] InterpreterRuntime::quicken_io_cc(JavaThread*)
  [10] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [11] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [12] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [13] org.apache.spark.util.ListenerBus$class.postToAll
  [14] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [15] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [16] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [17] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [18] scala.util.DynamicVariable.withValue
  [19] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [20] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [21] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [22] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [23] [tid=16265]

--- 1551158148609384 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.util.ClosureCleaner$.getClassReader
  [10] org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses
  [11] org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean
  [12] org.apache.spark.util.ClosureCleaner$.clean
  [13] org.apache.spark.SparkContext.clean
  [14] org.apache.spark.rdd.HadoopRDD.<init>
  [15] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [16] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [17] org.apache.spark.rdd.RDDOperationScope$.withScope
  [18] org.apache.spark.rdd.RDDOperationScope$.withScope
  [19] org.apache.spark.SparkContext.withScope
  [20] org.apache.spark.SparkContext.hadoopFile
  [21] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [22] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [23] org.apache.spark.rdd.RDDOperationScope$.withScope
  [24] org.apache.spark.rdd.RDDOperationScope$.withScope
  [25] org.apache.spark.SparkContext.withScope
  [26] org.apache.spark.SparkContext.textFile
  [27] org.apache.spark.api.java.JavaSparkContext.textFile
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16145]

--- 1551158148710789 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.api.java.JavaRDD$.fromRDD
  [11] org.apache.spark.api.java.JavaSparkContext.textFile
  [12] sun.reflect.NativeMethodAccessorImpl.invoke0
  [13] sun.reflect.NativeMethodAccessorImpl.invoke
  [14] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [15] java.lang.reflect.Method.invoke
  [16] py4j.reflection.MethodInvoker.invoke
  [17] py4j.reflection.ReflectionEngine.invoke
  [18] py4j.Gateway.invoke
  [19] py4j.commands.AbstractCommand.invokeMethod
  [20] py4j.commands.CallCommand.execute
  [21] py4j.GatewayConnection.run
  [22] java.lang.Thread.run
  [23] [tid=16145]

--- 1551158148818618 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.hadoop.mapred.TextInputFormat.configure
  [10] sun.reflect.NativeMethodAccessorImpl.invoke0
  [11] sun.reflect.NativeMethodAccessorImpl.invoke
  [12] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [13] java.lang.reflect.Method.invoke
  [14] org.apache.hadoop.util.ReflectionUtils.setJobConf
  [15] org.apache.hadoop.util.ReflectionUtils.setConf
  [16] org.apache.hadoop.util.ReflectionUtils.newInstance
  [17] org.apache.spark.rdd.HadoopRDD.getInputFormat
  [18] org.apache.spark.rdd.HadoopRDD.getPartitions
  [19] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [20] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [21] scala.Option.getOrElse
  [22] org.apache.spark.rdd.RDD.partitions
  [23] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [24] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [25] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [26] scala.Option.getOrElse
  [27] org.apache.spark.rdd.RDD.partitions
  [28] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [29] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [30] sun.reflect.NativeMethodAccessorImpl.invoke0
  [31] sun.reflect.NativeMethodAccessorImpl.invoke
  [32] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [33] java.lang.reflect.Method.invoke
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.AbstractCommand.invokeMethod
  [38] py4j.commands.CallCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16145]

--- 1551158148930573 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] alluxio.hadoop.AbstractFileSystem.initialize
  [13] alluxio.hadoop.FileSystem.initialize
  [14] org.apache.hadoop.fs.FileSystem.createFileSystem
  [15] org.apache.hadoop.fs.FileSystem.access$200
  [16] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [17] org.apache.hadoop.fs.FileSystem$Cache.get
  [18] org.apache.hadoop.fs.FileSystem.get
  [19] org.apache.hadoop.fs.Path.getFileSystem
  [20] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [21] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [22] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [23] org.apache.spark.rdd.HadoopRDD.getPartitions
  [24] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [25] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [26] scala.Option.getOrElse
  [27] org.apache.spark.rdd.RDD.partitions
  [28] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [29] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [30] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [31] scala.Option.getOrElse
  [32] org.apache.spark.rdd.RDD.partitions
  [33] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [34] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158149054214 us
  [ 0] java.util.regex.Matcher.<init>
  [ 1] java.util.regex.Pattern.matcher
  [ 2] alluxio.PropertyKey$Template.matches
  [ 3] alluxio.PropertyKey.isValid
  [ 4] alluxio.hadoop.HadoopConfigurationUtils.mergeHadoopConfiguration
  [ 5] alluxio.hadoop.AbstractFileSystem.initializeInternal
  [ 6] alluxio.hadoop.AbstractFileSystem.initialize
  [ 7] alluxio.hadoop.FileSystem.initialize
  [ 8] org.apache.hadoop.fs.FileSystem.createFileSystem
  [ 9] org.apache.hadoop.fs.FileSystem.access$200
  [10] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [11] org.apache.hadoop.fs.FileSystem$Cache.get
  [12] org.apache.hadoop.fs.FileSystem.get
  [13] org.apache.hadoop.fs.Path.getFileSystem
  [14] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [15] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [16] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [17] org.apache.spark.rdd.HadoopRDD.getPartitions
  [18] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [19] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.rdd.RDD.partitions
  [22] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [23] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [24] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [25] scala.Option.getOrElse
  [26] org.apache.spark.rdd.RDD.partitions
  [27] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [28] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [29] sun.reflect.NativeMethodAccessorImpl.invoke0
  [30] sun.reflect.NativeMethodAccessorImpl.invoke
  [31] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [32] java.lang.reflect.Method.invoke
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.AbstractCommand.invokeMethod
  [37] py4j.commands.CallCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16145]

--- 1551158149185710 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] alluxio.client.file.FileSystemContext.init
  [12] alluxio.client.file.FileSystemContext.create
  [13] alluxio.client.file.FileSystemContext$$Lambda$30.1831444969.apply
  [14] java.util.HashMap.computeIfAbsent
  [15] alluxio.client.file.FileSystemContext.get
  [16] alluxio.client.file.FileSystemContext.get
  [17] alluxio.hadoop.AbstractFileSystem.initializeInternal
  [18] alluxio.hadoop.AbstractFileSystem.initialize
  [19] alluxio.hadoop.FileSystem.initialize
  [20] org.apache.hadoop.fs.FileSystem.createFileSystem
  [21] org.apache.hadoop.fs.FileSystem.access$200
  [22] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [23] org.apache.hadoop.fs.FileSystem$Cache.get
  [24] org.apache.hadoop.fs.FileSystem.get
  [25] org.apache.hadoop.fs.Path.getFileSystem
  [26] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [27] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [28] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [29] org.apache.spark.rdd.HadoopRDD.getPartitions
  [30] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [31] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [32] scala.Option.getOrElse
  [33] org.apache.spark.rdd.RDD.partitions
  [34] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [35] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [36] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [37] scala.Option.getOrElse
  [38] org.apache.spark.rdd.RDD.partitions
  [39] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [40] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158149324982 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.Configuration.loadClusterDefault
  [10] alluxio.AbstractClient.beforeConnect
  [11] alluxio.AbstractClient.connect
  [12] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [13] alluxio.client.metrics.ClientMasterSync.heartbeat
  [14] alluxio.heartbeat.HeartbeatThread.run
  [15] java.util.concurrent.Executors$RunnableAdapter.call
  [16] java.util.concurrent.FutureTask.run
  [17] java.util.concurrent.ThreadPoolExecutor.runWorker
  [18] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [19] java.lang.Thread.run
  [20] [tid=16277]

--- 1551158149432909 us
  [ 0] ClassFileParser::parse_annotations(unsigned char*, int, ClassFileParser::AnnotationCollector*, Thread*) [clone .part.113]
  [ 1] ClassFileParser::parse_method(bool, AccessFlags*, Thread*)
  [ 2] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] ClassLoader::load_classfile(Symbol*, Thread*)
  [ 5] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 6] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 7] SystemDictionary::resolve_or_fail(Symbol*, Handle, Handle, bool, Thread*)
  [ 8] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [ 9] ConstantPool::klass_ref_at(int, Thread*)
  [10] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [11] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [13] sun.security.pkcs.ContentInfo.<clinit>
  [14] sun.security.pkcs.PKCS7.parse
  [15] sun.security.pkcs.PKCS7.parse
  [16] sun.security.pkcs.PKCS7.<init>
  [17] sun.security.util.SignatureFileVerifier.<init>
  [18] java.util.jar.JarVerifier.processEntry
  [19] java.util.jar.JarVerifier.update
  [20] java.util.jar.JarFile.initializeVerifier
  [21] java.util.jar.JarFile.ensureInitialization
  [22] java.util.jar.JavaUtilJarAccessImpl.ensureInitialization
  [23] sun.misc.URLClassPath$JarLoader$2.getManifest
  [24] java.net.URLClassLoader.defineClass
  [25] java.net.URLClassLoader.access$100
  [26] java.net.URLClassLoader$1.run
  [27] java.net.URLClassLoader$1.run
  [28] java.security.AccessController.doPrivileged
  [29] java.net.URLClassLoader.findClass
  [30] java.lang.ClassLoader.loadClass
  [31] java.lang.ClassLoader.loadClass
  [32] sun.misc.Launcher$AppClassLoader.loadClass
  [33] java.lang.ClassLoader.loadClass
  [34] sun.security.jca.ProviderConfig$2.run
  [35] sun.security.jca.ProviderConfig$2.run
  [36] java.security.AccessController.doPrivileged
  [37] sun.security.jca.ProviderConfig.doLoadProvider
  [38] sun.security.jca.ProviderConfig.getProvider
  [39] sun.security.jca.ProviderList.loadAll
  [40] sun.security.jca.ProviderList.removeInvalid
  [41] sun.security.jca.Providers.getFullProviderList
  [42] java.security.Security.insertProviderAt
  [43] java.security.Security.addProvider
  [44] alluxio.security.authentication.PlainSaslTransportProvider.<clinit>
  [45] alluxio.security.authentication.TransportProvider$Factory.create
  [46] alluxio.AbstractClient.connect
  [47] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [48] alluxio.client.metrics.ClientMasterSync.heartbeat
  [49] alluxio.heartbeat.HeartbeatThread.run
  [50] java.util.concurrent.Executors$RunnableAdapter.call
  [51] java.util.concurrent.FutureTask.run
  [52] java.util.concurrent.ThreadPoolExecutor.runWorker
  [53] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [54] java.lang.Thread.run
  [55] [tid=16277]

--- 1551158149534103 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] alluxio.security.authentication.PlainSaslTransportProvider.getClientTransport
  [11] alluxio.security.authentication.PlainSaslTransportProvider.getClientTransport
  [12] alluxio.AbstractClient.connect
  [13] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [14] alluxio.client.metrics.ClientMasterSync.heartbeat
  [15] alluxio.heartbeat.HeartbeatThread.run
  [16] java.util.concurrent.Executors$RunnableAdapter.call
  [17] java.util.concurrent.FutureTask.run
  [18] java.util.concurrent.ThreadPoolExecutor.runWorker
  [19] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [20] java.lang.Thread.run
  [21] [tid=16277]

--- 1551158149638268 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredMethods
  [ 9] java.lang.Class.getDeclaredMethods0
  [10] java.lang.Class.privateGetDeclaredMethods
  [11] java.lang.Class.privateGetMethodRecursive
  [12] java.lang.Class.getMethod0
  [13] java.lang.Class.getMethod
  [14] java.lang.Class.getEnumConstantsShared
  [15] java.lang.System$2.getEnumConstantsShared
  [16] java.util.EnumMap.getKeyUniverse
  [17] java.util.EnumMap.<init>
  [18] alluxio.thrift.MetricsMasterClientService$metricsHeartbeat_result.<clinit>
  [19] alluxio.thrift.MetricsMasterClientService$Client.recv_metricsHeartbeat
  [20] alluxio.thrift.MetricsMasterClientService$Client.metricsHeartbeat
  [21] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [22] alluxio.client.metrics.ClientMasterSync.heartbeat
  [23] alluxio.heartbeat.HeartbeatThread.run
  [24] java.util.concurrent.Executors$RunnableAdapter.call
  [25] java.util.concurrent.FutureTask.run
  [26] java.util.concurrent.ThreadPoolExecutor.runWorker
  [27] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [28] java.lang.Thread.run
  [29] [tid=16277]

--- 1551158149801340 us
  [ 0] jshort_disjoint_arraycopy
  [ 1] java.lang.String.<init>
  [ 2] java.lang.String.substring
  [ 3] sun.net.www.protocol.jar.Handler.parseURL
  [ 4] java.net.URL.<init>
  [ 5] java.net.URL.<init>
  [ 6] sun.misc.URLClassPath$JarLoader.checkResource
  [ 7] sun.misc.URLClassPath$JarLoader.getResource
  [ 8] sun.misc.URLClassPath.getResource
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] java.lang.Class.getDeclaredMethods0
  [17] java.lang.Class.privateGetDeclaredMethods
  [18] java.lang.Class.privateGetMethodRecursive
  [19] java.lang.Class.getMethod0
  [20] java.lang.Class.getMethod
  [21] java.lang.Class.getEnumConstantsShared
  [22] java.lang.Class.enumConstantDirectory
  [23] java.lang.Enum.valueOf
  [24] alluxio.conf.InstancedConfiguration.getEnum
  [25] alluxio.Configuration.getEnum
  [26] alluxio.client.file.options.GetStatusOptions.<init>
  [27] alluxio.client.file.options.GetStatusOptions.defaults
  [28] alluxio.client.file.BaseFileSystem.getStatus
  [29] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [30] alluxio.hadoop.FileSystem.getFileStatus
  [31] org.apache.hadoop.fs.Globber.getFileStatus
  [32] org.apache.hadoop.fs.Globber.glob
  [33] org.apache.hadoop.fs.FileSystem.globStatus
  [34] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [35] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [36] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [37] org.apache.spark.rdd.HadoopRDD.getPartitions
  [38] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [39] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [40] scala.Option.getOrElse
  [41] org.apache.spark.rdd.RDD.partitions
  [42] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [43] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [44] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [45] scala.Option.getOrElse
  [46] org.apache.spark.rdd.RDD.partitions
  [47] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [48] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16145]

--- 1551158149857644 us
  [ 0] frame::sender(RegisterMap*) const
  [ 1] JavaThread::nmethods_do(CodeBlobClosure*) [clone .part.89]
  [ 2] Threads::nmethods_do(CodeBlobClosure*)
  [ 3] NMethodSweeper::mark_active_nmethods()
  [ 4] SafepointSynchronize::do_cleanup_tasks()
  [ 5] SafepointSynchronize::begin()
  [ 6] VMThread::loop()
  [ 7] VMThread::run()
  [ 8] java_start(Thread*)
  [ 9] start_thread
  [10] [tid=16092]

--- 1551158149961836 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16075]

--- 1551158149968734 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16089]

--- 1551158149973203 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16069]

--- 1551158149974259 us
  [ 0] ParMarkBitMap::live_words_in_range(HeapWord*, oopDesc*) const
  [ 1] ParallelCompactData::calc_new_pointer(HeapWord*)
  [ 2] InstanceRefKlass::oop_update_pointers(ParCompactionManager*, oopDesc*)
  [ 3] MoveAndUpdateClosure::do_addr(HeapWord*, unsigned long)
  [ 4] ParMarkBitMap::iterate(ParMarkBitMapClosure*, unsigned long, unsigned long) const
  [ 5] PSParallelCompact::fill_region(ParCompactionManager*, unsigned long)
  [ 6] ParCompactionManager::drain_region_stacks()
  [ 7] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 8] GCTaskThread::run()
  [ 9] java_start(Thread*)
  [10] start_thread
  [11] [tid=16087]

--- 1551158150034584 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.thrift.FileSystemMasterCommonTOptions.<clinit>
  [10] alluxio.wire.CommonOptions.toThrift
  [11] alluxio.client.file.options.GetStatusOptions.toThrift
  [12] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [13] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.362253015.call
  [14] alluxio.AbstractClient.retryRPCInternal
  [15] alluxio.AbstractClient.retryRPC
  [16] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [17] alluxio.client.file.BaseFileSystem.getStatus
  [18] alluxio.client.file.BaseFileSystem.getStatus
  [19] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [20] alluxio.hadoop.FileSystem.getFileStatus
  [21] org.apache.hadoop.fs.Globber.getFileStatus
  [22] org.apache.hadoop.fs.Globber.glob
  [23] org.apache.hadoop.fs.FileSystem.globStatus
  [24] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [25] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [26] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [27] org.apache.spark.rdd.HadoopRDD.getPartitions
  [28] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [29] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [30] scala.Option.getOrElse
  [31] org.apache.spark.rdd.RDD.partitions
  [32] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [33] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [34] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [35] scala.Option.getOrElse
  [36] org.apache.spark.rdd.RDD.partitions
  [37] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [38] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [39] sun.reflect.NativeMethodAccessorImpl.invoke0
  [40] sun.reflect.NativeMethodAccessorImpl.invoke
  [41] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [42] java.lang.reflect.Method.invoke
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.AbstractCommand.invokeMethod
  [47] py4j.commands.CallCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158150134355 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.thrift.FileInfo$FileInfoStandardScheme.read
  [10] alluxio.thrift.FileInfo$FileInfoStandardScheme.read
  [11] alluxio.thrift.FileInfo.read
  [12] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [13] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [14] alluxio.thrift.GetStatusTResponse.read
  [15] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [16] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [17] alluxio.thrift.FileSystemMasterClientService$getStatus_result.read
  [18] alluxio.core.client.runtime.org.apache.thrift.TServiceClient.receiveBase
  [19] alluxio.thrift.FileSystemMasterClientService$Client.recv_getStatus
  [20] alluxio.thrift.FileSystemMasterClientService$Client.getStatus
  [21] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [22] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.362253015.call
  [23] alluxio.AbstractClient.retryRPCInternal
  [24] alluxio.AbstractClient.retryRPC
  [25] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [26] alluxio.client.file.BaseFileSystem.getStatus
  [27] alluxio.client.file.BaseFileSystem.getStatus
  [28] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [29] alluxio.hadoop.FileSystem.getFileStatus
  [30] org.apache.hadoop.fs.Globber.getFileStatus
  [31] org.apache.hadoop.fs.Globber.glob
  [32] org.apache.hadoop.fs.FileSystem.globStatus
  [33] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [34] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [35] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [36] org.apache.spark.rdd.HadoopRDD.getPartitions
  [37] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [38] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [39] scala.Option.getOrElse
  [40] org.apache.spark.rdd.RDD.partitions
  [41] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [42] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [43] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [44] scala.Option.getOrElse
  [45] org.apache.spark.rdd.RDD.partitions
  [46] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [47] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16145]

--- 1551158150234450 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [13] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.362253015.call
  [14] alluxio.AbstractClient.retryRPCInternal
  [15] alluxio.AbstractClient.retryRPC
  [16] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [17] alluxio.client.file.BaseFileSystem.getStatus
  [18] alluxio.client.file.BaseFileSystem.getStatus
  [19] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [20] alluxio.hadoop.FileSystem.getFileStatus
  [21] org.apache.hadoop.fs.Globber.getFileStatus
  [22] org.apache.hadoop.fs.Globber.glob
  [23] org.apache.hadoop.fs.FileSystem.globStatus
  [24] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [25] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [26] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [27] org.apache.spark.rdd.HadoopRDD.getPartitions
  [28] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [29] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [30] scala.Option.getOrElse
  [31] org.apache.spark.rdd.RDD.partitions
  [32] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [33] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [34] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [35] scala.Option.getOrElse
  [36] org.apache.spark.rdd.RDD.partitions
  [37] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [38] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [39] sun.reflect.NativeMethodAccessorImpl.invoke0
  [40] sun.reflect.NativeMethodAccessorImpl.invoke
  [41] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [42] java.lang.reflect.Method.invoke
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.AbstractCommand.invokeMethod
  [47] py4j.commands.CallCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16145]

--- 1551158150342270 us
  [ 0] java.lang.String.toCharArray
  [ 1] java.util.zip.ZipCoder.getBytes
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.jar.JarFile.getEntry
  [ 4] java.util.jar.JarFile.getJarEntry
  [ 5] sun.misc.URLClassPath$JarLoader.getResource
  [ 6] sun.misc.URLClassPath.getResource
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] java.lang.Class.forName0
  [16] java.lang.Class.forName
  [17] py4j.reflection.CurrentThreadClassLoadingStrategy.classForName
  [18] py4j.reflection.ReflectionUtil.classForName
  [19] py4j.reflection.TypeUtil.getClass
  [20] py4j.reflection.TypeUtil.forName
  [21] py4j.commands.ReflectionCommand.getUnknownMember
  [22] py4j.commands.ReflectionCommand.execute
  [23] py4j.GatewayConnection.run
  [24] java.lang.Thread.run
  [25] [tid=16145]

--- 1551158150550443 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1$$anonfun$apply$5.apply
  [10] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1$$anonfun$apply$5.apply
  [11] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [12] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [13] scala.collection.IndexedSeqOptimized$class.foreach
  [14] scala.collection.mutable.ArrayOps$ofRef.foreach
  [15] scala.collection.TraversableLike$class.flatMap
  [16] scala.collection.mutable.ArrayOps$ofRef.flatMap
  [17] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1.apply
  [18] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1.apply
  [19] scala.Option.map
  [20] org.apache.spark.rdd.HadoopRDD$.convertSplitLocationInfo
  [21] org.apache.spark.rdd.HadoopRDD.getPreferredLocations
  [22] org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply
  [23] org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.rdd.RDD.preferredLocations
  [26] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [27] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply$mcVI$sp
  [28] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [29] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [30] scala.collection.immutable.List.foreach
  [31] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [32] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [33] scala.collection.immutable.List.foreach
  [34] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [35] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply$mcVI$sp
  [36] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [37] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [38] scala.collection.immutable.List.foreach
  [39] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [40] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [41] scala.collection.immutable.List.foreach
  [42] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [43] org.apache.spark.scheduler.DAGScheduler.getPreferredLocs
  [44] org.apache.spark.scheduler.DAGScheduler$$anonfun$17.apply
  [45] org.apache.spark.scheduler.DAGScheduler$$anonfun$17.apply
  [46] scala.collection.TraversableLike$$anonfun$map$1.apply
  [47] scala.collection.TraversableLike$$anonfun$map$1.apply
  [48] scala.collection.Iterator$class.foreach
  [49] scala.collection.AbstractIterator.foreach
  [50] scala.collection.IterableLike$class.foreach
  [51] scala.collection.AbstractIterable.foreach
  [52] scala.collection.TraversableLike$class.map
  [53] scala.collection.AbstractTraversable.map
  [54] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [55] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [56] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [57] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [58] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [59] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [60] org.apache.spark.util.EventLoop$$anon$1.run
  [61] [tid=16221]

--- 1551158150554408 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.util.JsonProtocol$.stageInfoToJson
  [10] org.apache.spark.util.JsonProtocol$$anonfun$jobStartToJson$4.apply
  [11] org.apache.spark.util.JsonProtocol$$anonfun$jobStartToJson$4.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.IndexedSeqOptimized$class.foreach
  [15] scala.collection.mutable.WrappedArray.foreach
  [16] scala.collection.TraversableLike$class.map
  [17] scala.collection.AbstractTraversable.map
  [18] org.apache.spark.util.JsonProtocol$.jobStartToJson
  [19] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [20] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [21] org.apache.spark.scheduler.EventLoggingListener.onJobStart
  [22] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [23] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [24] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [25] org.apache.spark.util.ListenerBus$class.postToAll
  [26] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [27] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [28] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [29] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [30] scala.util.DynamicVariable.withValue
  [31] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [32] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [33] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [34] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [35] [tid=16267]

--- 1551158150710196 us
  [ 0] SpaceManager::allocate(unsigned long)
  [ 1] Metaspace::allocate(ClassLoaderData*, unsigned long, bool, MetaspaceObj::Type, Thread*)
  [ 2] MethodCounters::allocate(ClassLoaderData*, Thread*)
  [ 3] Method::build_method_counters(Method*, Thread*)
  [ 4] InterpreterRuntime::build_method_counters(JavaThread*, Method*)
  [ 5] [tid=16265]

--- 1551158150756856 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [10] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [11] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [13] com.google.protobuf.DescriptorProtos$FileDescriptorProto.initFields
  [14] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<clinit>
  [15] com.google.protobuf.Descriptors$FileDescriptor.internalBuildGeneratedFileFrom
  [16] org.apache.hadoop.security.proto.SecurityProtos.<clinit>
  [17] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [18] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [19] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [20] com.google.protobuf.GeneratedMessage.getAllFields
  [21] com.google.protobuf.TextFormat$Printer.print
  [22] com.google.protobuf.TextFormat$Printer.access$400
  [23] com.google.protobuf.TextFormat.shortDebugString
  [24] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [25] java.lang.String.valueOf
  [26] java.lang.StringBuilder.append
  [27] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [28] [tid=16272]

--- 1551158150780994 us
  [ 0] new_type_array Runtime1 stub
  [ 1] java.nio.ByteBuffer.allocate
  [ 2] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply
  [ 3] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply
  [ 4] org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded
  [ 5] org.apache.spark.util.io.ChunkedByteBufferOutputStream.write
  [ 6] net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData
  [ 7] net.jpountz.lz4.LZ4BlockOutputStream.finish
  [ 8] net.jpountz.lz4.LZ4BlockOutputStream.close
  [ 9] java.io.ObjectOutputStream$BlockDataOutputStream.close
  [10] java.io.ObjectOutputStream.close
  [11] org.apache.spark.serializer.JavaSerializationStream.close
  [12] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$1.apply$mcV$sp
  [13] org.apache.spark.util.Utils$.tryWithSafeFinally
  [14] org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject
  [15] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [16] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [17] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [18] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [19] org.apache.spark.SparkContext.broadcast
  [20] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [21] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [22] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [23] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [24] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [25] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [26] org.apache.spark.util.EventLoop$$anon$1.run
  [27] [tid=16221]

--- 1551158150906396 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [12] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [13] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [14] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [15] com.google.protobuf.CodedInputStream.readMessage
  [16] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [17] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [18] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [19] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [20] com.google.protobuf.AbstractParser.parsePartialFrom
  [21] com.google.protobuf.AbstractParser.parseFrom
  [22] com.google.protobuf.AbstractParser.parseFrom
  [23] com.google.protobuf.AbstractParser.parseFrom
  [24] com.google.protobuf.AbstractParser.parseFrom
  [25] com.google.protobuf.DescriptorProtos$FileDescriptorProto.parseFrom
  [26] com.google.protobuf.Descriptors$FileDescriptor.internalBuildGeneratedFileFrom
  [27] org.apache.hadoop.security.proto.SecurityProtos.<clinit>
  [28] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [29] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [30] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [31] com.google.protobuf.GeneratedMessage.getAllFields
  [32] com.google.protobuf.TextFormat$Printer.print
  [33] com.google.protobuf.TextFormat$Printer.access$400
  [34] com.google.protobuf.TextFormat.shortDebugString
  [35] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [36] java.lang.String.valueOf
  [37] java.lang.StringBuilder.append
  [38] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [39] [tid=16272]

--- 1551158150978872 us
  [ 0] binary_search(Array<Method*>*, Symbol*)
  [ 1] InstanceKlass::find_method_index(Array<Method*>*, Symbol*, Symbol*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.290]
  [ 2] InstanceKlass::uncached_lookup_method(Symbol*, Symbol*, Klass::OverpassLookupMode) const
  [ 3] LinkResolver::lookup_method_in_klasses(methodHandle&, KlassHandle, Symbol*, Symbol*, bool, bool, Thread*)
  [ 4] LinkResolver::resolve_method(methodHandle&, KlassHandle, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 5] LinkResolver::linktime_resolve_virtual_method(methodHandle&, KlassHandle, Symbol*, Symbol*, KlassHandle, bool, Thread*)
  [ 6] LinkResolver::resolve_invokevirtual(CallInfo&, Handle, constantPoolHandle, int, Thread*)
  [ 7] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 8] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [ 9] org.apache.spark.scheduler.TaskSetManager.<init>
  [10] org.apache.spark.scheduler.TaskSchedulerImpl.createTaskSetManager
  [11] org.apache.spark.scheduler.TaskSchedulerImpl.submitTasks
  [12] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [13] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [14] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [15] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [16] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [17] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [18] org.apache.spark.util.EventLoop$$anon$1.run
  [19] [tid=16221]

--- 1551158151051418 us
  [ 0] SignatureVerifier::is_valid_method_signature(Symbol*)
  [ 1] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 2] ClassVerifier::verify_class(Thread*)
  [ 3] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 4] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 5] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 6] InstanceKlass::initialize(Thread*)
  [ 7] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 8] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.<clinit>
  [ 9] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [10] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [11] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [12] com.google.protobuf.GeneratedMessage.getAllFields
  [13] com.google.protobuf.TextFormat$Printer.print
  [14] com.google.protobuf.TextFormat$Printer.access$400
  [15] com.google.protobuf.TextFormat.shortDebugString
  [16] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [17] java.lang.String.valueOf
  [18] java.lang.StringBuilder.append
  [19] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [20] [tid=16272]

--- 1551158156573297 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.api.python.SerDeUtil$.initialize
  [13] org.apache.spark.api.python.SerDeUtil$.<init>
  [14] org.apache.spark.api.python.SerDeUtil$.<clinit>
  [15] org.apache.spark.api.python.SerDeUtil.pythonToJava
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] py4j.reflection.MethodInvoker.invoke
  [21] py4j.reflection.ReflectionEngine.invoke
  [22] py4j.Gateway.invoke
  [23] py4j.commands.AbstractCommand.invokeMethod
  [24] py4j.commands.CallCommand.execute
  [25] py4j.GatewayConnection.run
  [26] java.lang.Thread.run
  [27] [tid=16145]

--- 1551158156674344 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.types.StructType$.apply
  [10] org.apache.spark.sql.types.DataType$.parseDataType
  [11] org.apache.spark.sql.types.DataType$.fromJson
  [12] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [13] sun.reflect.NativeMethodAccessorImpl.invoke0
  [14] sun.reflect.NativeMethodAccessorImpl.invoke
  [15] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [16] java.lang.reflect.Method.invoke
  [17] py4j.reflection.MethodInvoker.invoke
  [18] py4j.reflection.ReflectionEngine.invoke
  [19] py4j.Gateway.invoke
  [20] py4j.commands.AbstractCommand.invokeMethod
  [21] py4j.commands.CallCommand.execute
  [22] py4j.GatewayConnection.run
  [23] java.lang.Thread.run
  [24] [tid=16145]

--- 1551158156774281 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [13] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [14] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [15] sun.reflect.NativeMethodAccessorImpl.invoke0
  [16] sun.reflect.NativeMethodAccessorImpl.invoke
  [17] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [18] java.lang.reflect.Method.invoke
  [19] py4j.reflection.MethodInvoker.invoke
  [20] py4j.reflection.ReflectionEngine.invoke
  [21] py4j.Gateway.invoke
  [22] py4j.commands.AbstractCommand.invokeMethod
  [23] py4j.commands.CallCommand.execute
  [24] py4j.GatewayConnection.run
  [25] java.lang.Thread.run
  [26] [tid=16145]

--- 1551158156874010 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1.<init>
  [11] org.apache.spark.sql.internal.BaseSessionStateBuilder.analyzer
  [12] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply
  [14] org.apache.spark.sql.internal.SessionState.analyzer$lzycompute
  [15] org.apache.spark.sql.internal.SessionState.analyzer
  [16] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [17] org.apache.spark.sql.execution.QueryExecution.analyzed
  [18] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [19] org.apache.spark.sql.Dataset$.ofRows
  [20] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16145]

--- 1551158156973741 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveGroupingAnalytics$lzycompute
  [10] org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveGroupingAnalytics
  [11] org.apache.spark.sql.catalyst.analysis.Analyzer.batches$lzycompute
  [12] org.apache.spark.sql.catalyst.analysis.Analyzer.batches
  [13] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [14] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [15] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [16] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [17] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [19] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [20] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [21] org.apache.spark.sql.execution.QueryExecution.analyzed
  [22] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158157073480 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.TypeCoercion$.typeCoercionRules
  [10] org.apache.spark.sql.catalyst.analysis.Analyzer.batches$lzycompute
  [11] org.apache.spark.sql.catalyst.analysis.Analyzer.batches
  [12] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [13] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [14] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [15] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [16] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [18] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [19] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [20] org.apache.spark.sql.execution.QueryExecution.analyzed
  [21] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158157173225 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.AttributeSet$.apply
  [10] org.apache.spark.sql.catalyst.expressions.AttributeSet$.<init>
  [11] org.apache.spark.sql.catalyst.expressions.AttributeSet$.<clinit>
  [12] org.apache.spark.sql.catalyst.plans.QueryPlan.references
  [13] org.apache.spark.sql.catalyst.plans.QueryPlan.missingInput
  [14] org.apache.spark.sql.catalyst.plans.QueryPlan.statePrefix
  [15] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.statePrefix
  [16] org.apache.spark.sql.catalyst.plans.QueryPlan.simpleString
  [17] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$34.apply
  [18] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$34.apply
  [19] org.apache.spark.internal.Logging$class.logTrace
  [20] org.apache.spark.sql.catalyst.rules.Rule.logTrace
  [21] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse
  [22] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse
  [23] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [24] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [25] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [26] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [27] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [29] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [30] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [31] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply
  [32] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply
  [33] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [34] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [35] scala.collection.LinearSeqOptimized$class.foldLeft
  [36] scala.collection.immutable.List.foldLeft
  [37] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [38] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [39] scala.collection.immutable.List.foreach
  [40] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [41] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [42] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [43] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [44] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [45] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [46] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [47] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [48] org.apache.spark.sql.execution.QueryExecution.analyzed
  [49] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [50] org.apache.spark.sql.Dataset$.ofRows
  [51] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [52] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [53] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [54] sun.reflect.NativeMethodAccessorImpl.invoke0
  [55] sun.reflect.NativeMethodAccessorImpl.invoke
  [56] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [57] java.lang.reflect.Method.invoke
  [58] py4j.reflection.MethodInvoker.invoke
  [59] py4j.reflection.ReflectionEngine.invoke
  [60] py4j.Gateway.invoke
  [61] py4j.commands.AbstractCommand.invokeMethod
  [62] py4j.commands.CallCommand.execute
  [63] py4j.GatewayConnection.run
  [64] java.lang.Thread.run
  [65] [tid=16145]

--- 1551158157273020 us
  [ 0] Arena::grow(unsigned long, AllocFailStrategy::AllocFailEnum)
  [ 1] ClassFileParser::verify_legal_method_signature(Symbol*, Symbol*, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion$.coerceTypes
  [21] org.apache.spark.sql.catalyst.analysis.TypeCoercionRule$class.apply
  [22] org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion$.apply
  [23] org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion$.apply
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [25] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [26] scala.collection.LinearSeqOptimized$class.foldLeft
  [27] scala.collection.immutable.List.foldLeft
  [28] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [29] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [30] scala.collection.immutable.List.foreach
  [31] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [32] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [33] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [34] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [35] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [36] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [37] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [38] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [39] org.apache.spark.sql.execution.QueryExecution.analyzed
  [40] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [41] org.apache.spark.sql.Dataset$.ofRows
  [42] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [43] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] sun.reflect.NativeMethodAccessorImpl.invoke0
  [46] sun.reflect.NativeMethodAccessorImpl.invoke
  [47] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [48] java.lang.reflect.Method.invoke
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.AbstractCommand.invokeMethod
  [53] py4j.commands.CallCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16145]

--- 1551158157372871 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply
  [10] org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply
  [11] org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp
  [12] org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis
  [13] org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis
  [14] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [15] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [16] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [17] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [18] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [19] org.apache.spark.sql.execution.QueryExecution.analyzed
  [20] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158157472618 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.catalyst.expressions.GenericInternalRow.<init>
  [13] org.apache.spark.sql.catalyst.InternalRow$.apply
  [14] org.apache.spark.sql.catalyst.InternalRow$.<init>
  [15] org.apache.spark.sql.catalyst.InternalRow$.<clinit>
  [16] org.apache.spark.sql.catalyst.expressions.BoundReference.<init>
  [17] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [18] org.apache.spark.sql.Dataset$.ofRows
  [19] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16145]

--- 1551158157613264 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [10] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [11] InstanceKlass::initialize(Thread*)
  [12] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [13] scala.reflect.runtime.package$.universe$lzycompute
  [14] scala.reflect.runtime.package$.universe
  [15] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [17] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [19] org.apache.spark.sql.Dataset$.ofRows
  [20] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16145]

--- 1551158157712994 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.package$.universe$lzycompute
  [12] scala.reflect.runtime.package$.universe
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [14] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [16] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [17] org.apache.spark.sql.Dataset$.ofRows
  [18] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.AbstractCommand.invokeMethod
  [29] py4j.commands.CallCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16145]

--- 1551158157831087 us
  [ 0] inflate_table
  [ 1] inflate
  [ 2] Java_java_util_zip_Inflater_inflateBytes
  [ 3] java.util.zip.Inflater.inflateBytes
  [ 4] java.util.zip.Inflater.inflate
  [ 5] java.util.zip.InflaterInputStream.read
  [ 6] sun.misc.Resource.getBytes
  [ 7] java.net.URLClassLoader.defineClass
  [ 8] java.net.URLClassLoader.access$100
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158157961643 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.package$.universe$lzycompute
  [11] scala.reflect.runtime.package$.universe
  [12] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [14] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16145]

--- 1551158158061376 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.package$.universe$lzycompute
  [11] scala.reflect.runtime.package$.universe
  [12] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [14] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16145]

--- 1551158158180678 us
  [ 0] vframeStreamCommon::security_get_caller_frame(int)
  [ 1] JVM_DoPrivileged
  [ 2] java.security.AccessController.doPrivileged
  [ 3] java.net.URLClassLoader.findClass
  [ 4] java.lang.ClassLoader.loadClass
  [ 5] sun.misc.Launcher$AppClassLoader.loadClass
  [ 6] java.lang.ClassLoader.loadClass
  [ 7] scala.reflect.runtime.package$.universe$lzycompute
  [ 8] scala.reflect.runtime.package$.universe
  [ 9] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [10] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [11] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [12] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [13] org.apache.spark.sql.Dataset$.ofRows
  [14] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [15] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [16] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16145]

--- 1551158158288094 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.api.Universe.<init>
  [13] scala.reflect.macros.Universe.<init>
  [14] scala.reflect.internal.SymbolTable.<init>
  [15] scala.reflect.runtime.JavaUniverse.<init>
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158158387833 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.Settings.<init>
  [12] scala.reflect.runtime.JavaUniverse.settings$lzycompute
  [13] scala.reflect.runtime.JavaUniverse.settings
  [14] scala.reflect.runtime.JavaUniverse.settings
  [15] scala.reflect.internal.tpe.GlbLubs$class.$init$
  [16] scala.reflect.internal.SymbolTable.<init>
  [17] scala.reflect.runtime.JavaUniverse.<init>
  [18] scala.reflect.runtime.package$.universe$lzycompute
  [19] scala.reflect.runtime.package$.universe
  [20] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [21] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [22] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [23] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [24] org.apache.spark.sql.Dataset$.ofRows
  [25] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16145]

--- 1551158158487580 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.runtime.SynchronizedOps$class.newBaseTypeSeq
  [10] scala.reflect.runtime.JavaUniverse.newBaseTypeSeq
  [11] scala.reflect.internal.BaseTypeSeqs$class.$init$
  [12] scala.reflect.internal.SymbolTable.<init>
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158158587305 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.nme$lzycompute
  [11] scala.reflect.internal.SymbolTable.nme
  [12] scala.reflect.internal.StdNames$class.$init$
  [13] scala.reflect.internal.SymbolTable.<init>
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158158687047 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.internal.SymbolTable.<init>
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158158787379 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.ReificationSupport$class.$init$
  [11] scala.reflect.internal.SymbolTable.<init>
  [12] scala.reflect.runtime.JavaUniverse.<init>
  [13] scala.reflect.runtime.package$.universe$lzycompute
  [14] scala.reflect.runtime.package$.universe
  [15] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [17] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [19] org.apache.spark.sql.Dataset$.ofRows
  [20] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16145]

--- 1551158158887740 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.internal.SymbolTable.<init>
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158158987592 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.definitions$lzycompute
  [11] scala.reflect.internal.SymbolTable.definitions
  [12] scala.reflect.runtime.JavaUniverse.init
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158159087275 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.definitions$lzycompute
  [11] scala.reflect.internal.SymbolTable.definitions
  [12] scala.reflect.runtime.JavaUniverse.init
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158159187849 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [12] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [13] scala.reflect.internal.Symbols$class.NoSymbol
  [14] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [15] scala.reflect.internal.SymbolTable.NoSymbol
  [16] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [17] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [18] scala.reflect.runtime.JavaUniverse.rootMirror
  [19] scala.reflect.runtime.JavaUniverse.rootMirror
  [20] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [21] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [22] scala.reflect.internal.Definitions$DefinitionsClass.init
  [23] scala.reflect.runtime.JavaUniverse.init
  [24] scala.reflect.runtime.JavaUniverse.<init>
  [25] scala.reflect.runtime.package$.universe$lzycompute
  [26] scala.reflect.runtime.package$.universe
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [28] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [30] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [31] org.apache.spark.sql.Dataset$.ofRows
  [32] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158159290331 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [11] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [12] scala.reflect.internal.Symbols$class.NoSymbol
  [13] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [14] scala.reflect.internal.SymbolTable.NoSymbol
  [15] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [16] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [17] scala.reflect.runtime.JavaUniverse.rootMirror
  [18] scala.reflect.runtime.JavaUniverse.rootMirror
  [19] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [20] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [21] scala.reflect.internal.Definitions$DefinitionsClass.init
  [22] scala.reflect.runtime.JavaUniverse.init
  [23] scala.reflect.runtime.JavaUniverse.<init>
  [24] scala.reflect.runtime.package$.universe$lzycompute
  [25] scala.reflect.runtime.package$.universe
  [26] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [28] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [30] org.apache.spark.sql.Dataset$.ofRows
  [31] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [32] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] sun.reflect.NativeMethodAccessorImpl.invoke0
  [35] sun.reflect.NativeMethodAccessorImpl.invoke
  [36] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [37] java.lang.reflect.Method.invoke
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.AbstractCommand.invokeMethod
  [42] py4j.commands.CallCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16145]

--- 1551158159390064 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.$init$
  [13] scala.reflect.runtime.SynchronizedSymbols$$anon$16.<init>
  [14] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [15] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [16] scala.reflect.internal.Symbols$class.NoSymbol
  [17] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [18] scala.reflect.internal.SymbolTable.NoSymbol
  [19] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [20] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [21] scala.reflect.runtime.JavaUniverse.rootMirror
  [22] scala.reflect.runtime.JavaUniverse.rootMirror
  [23] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [24] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [25] scala.reflect.internal.Definitions$DefinitionsClass.init
  [26] scala.reflect.runtime.JavaUniverse.init
  [27] scala.reflect.runtime.JavaUniverse.<init>
  [28] scala.reflect.runtime.package$.universe$lzycompute
  [29] scala.reflect.runtime.package$.universe
  [30] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [31] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [32] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [33] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [34] org.apache.spark.sql.Dataset$.ofRows
  [35] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [36] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [37] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16145]

--- 1551158159492887 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.ref.WeakReference.<init>
  [10] scala.ref.WeakReference.<init>
  [11] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [12] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [13] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [14] scala.reflect.runtime.JavaUniverse.rootMirror
  [15] scala.reflect.runtime.JavaUniverse.rootMirror
  [16] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [17] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [18] scala.reflect.internal.Definitions$DefinitionsClass.init
  [19] scala.reflect.runtime.JavaUniverse.init
  [20] scala.reflect.runtime.JavaUniverse.<init>
  [21] scala.reflect.runtime.package$.universe$lzycompute
  [22] scala.reflect.runtime.package$.universe
  [23] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [24] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [25] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [26] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [27] org.apache.spark.sql.Dataset$.ofRows
  [28] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [29] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [30] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [31] sun.reflect.NativeMethodAccessorImpl.invoke0
  [32] sun.reflect.NativeMethodAccessorImpl.invoke
  [33] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [34] java.lang.reflect.Method.invoke
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.AbstractCommand.invokeMethod
  [39] py4j.commands.CallCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16145]

--- 1551158159592617 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [10] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [11] InstanceKlass::initialize(Thread*)
  [12] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage$lzycompute
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage
  [15] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage
  [16] scala.reflect.internal.Mirrors$RootsBase.init
  [17] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [18] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [19] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [20] scala.reflect.runtime.JavaUniverse.rootMirror
  [21] scala.reflect.runtime.JavaUniverse.rootMirror
  [22] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [23] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [24] scala.reflect.internal.Definitions$DefinitionsClass.init
  [25] scala.reflect.runtime.JavaUniverse.init
  [26] scala.reflect.runtime.JavaUniverse.<init>
  [27] scala.reflect.runtime.package$.universe$lzycompute
  [28] scala.reflect.runtime.package$.universe
  [29] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [30] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [31] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [32] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [33] org.apache.spark.sql.Dataset$.ofRows
  [34] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [35] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [36] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [37] sun.reflect.NativeMethodAccessorImpl.invoke0
  [38] sun.reflect.NativeMethodAccessorImpl.invoke
  [39] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [40] java.lang.reflect.Method.invoke
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.AbstractCommand.invokeMethod
  [45] py4j.commands.CallCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16145]

--- 1551158159693820 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.Mirrors$Roots$RootPackage.<init>
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$3.<init>
  [11] scala.reflect.runtime.JavaMirrors$JavaMirror.RootPackage$lzycompute
  [12] scala.reflect.runtime.JavaMirrors$JavaMirror.RootPackage
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.RootPackage
  [14] scala.reflect.internal.Mirrors$RootsBase.init
  [15] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [16] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [17] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [18] scala.reflect.runtime.JavaUniverse.rootMirror
  [19] scala.reflect.runtime.JavaUniverse.rootMirror
  [20] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [21] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [22] scala.reflect.internal.Definitions$DefinitionsClass.init
  [23] scala.reflect.runtime.JavaUniverse.init
  [24] scala.reflect.runtime.JavaUniverse.<init>
  [25] scala.reflect.runtime.package$.universe$lzycompute
  [26] scala.reflect.runtime.package$.universe
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [28] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [30] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [31] org.apache.spark.sql.Dataset$.ofRows
  [32] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16145]

--- 1551158159794571 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createModuleSymbol
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.createModuleSymbol
  [11] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createPackageSymbol
  [12] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.createPackageSymbol
  [13] scala.reflect.internal.Symbols$Symbol.newTermSymbol
  [14] scala.reflect.internal.Symbols$Symbol.newModuleSymbol
  [15] scala.reflect.internal.Symbols$Symbol.newModule
  [16] scala.reflect.internal.Symbols$Symbol.newPackage
  [17] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$$makeScalaPackage$1.apply
  [18] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$$makeScalaPackage$1.apply
  [19] scala.reflect.runtime.Gil$class.gilSynchronized
  [20] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [21] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$$makeScalaPackage
  [22] scala.reflect.runtime.JavaMirrors$class.missingHook
  [23] scala.reflect.runtime.JavaUniverse.missingHook
  [24] scala.reflect.internal.Mirrors$RootsBase.universeMissingHook
  [25] scala.reflect.internal.Mirrors$RootsBase.missingHook
  [26] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [27] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [33] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [34] scala.reflect.internal.Definitions$DefinitionsClass.init
  [35] scala.reflect.runtime.JavaUniverse.init
  [36] scala.reflect.runtime.JavaUniverse.<init>
  [37] scala.reflect.runtime.package$.universe$lzycompute
  [38] scala.reflect.runtime.package$.universe
  [39] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [40] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [41] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [42] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [43] org.apache.spark.sql.Dataset$.ofRows
  [44] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [45] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [46] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [47] sun.reflect.NativeMethodAccessorImpl.invoke0
  [48] sun.reflect.NativeMethodAccessorImpl.invoke
  [49] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [50] java.lang.reflect.Method.invoke
  [51] py4j.reflection.MethodInvoker.invoke
  [52] py4j.reflection.ReflectionEngine.invoke
  [53] py4j.Gateway.invoke
  [54] py4j.commands.AbstractCommand.invokeMethod
  [55] py4j.commands.CallCommand.execute
  [56] py4j.GatewayConnection.run
  [57] java.lang.Thread.run
  [58] [tid=16145]

--- 1551158159894918 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.Predef$.byteArrayOps
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [11] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [12] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [13] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [14] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [15] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [16] scala.reflect.internal.Symbols$Symbol.info
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [19] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [20] scala.reflect.runtime.Gil$class.gilSynchronized
  [21] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [22] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [23] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [24] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [26] scala.reflect.internal.SymbolTable.openPackageModule
  [27] scala.reflect.internal.SymbolTable.openPackageModule
  [28] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [29] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [30] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [31] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [32] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [33] scala.reflect.internal.Symbols$Symbol.info
  [34] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [37] scala.reflect.runtime.Gil$class.gilSynchronized
  [38] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [39] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [40] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [41] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [42] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [43] scala.reflect.internal.Types$TypeRef.thisInfo
  [44] scala.reflect.internal.Types$TypeRef.baseClasses
  [45] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [46] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [47] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [48] scala.reflect.internal.Types$Type.findMember
  [49] scala.reflect.internal.Types$Type.memberBasedOnName
  [50] scala.reflect.internal.Types$Type.member
  [51] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [52] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [53] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [54] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [55] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [56] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [57] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [58] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [59] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [60] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [61] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [62] scala.reflect.internal.Mirrors$RootsBase.init
  [63] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [64] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [65] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [66] scala.reflect.runtime.Gil$class.gilSynchronized
  [67] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [68] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [69] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [70] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [71] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [72] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [73] scala.reflect.runtime.Gil$class.gilSynchronized
  [74] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [75] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [76] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [77] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [78] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [79] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [80] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [81] scala.reflect.internal.Types$Type.findMember
  [82] scala.reflect.internal.Types$Type.memberBasedOnName
  [83] scala.reflect.internal.Types$Type.member
  [84] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [85] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [86] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [87] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [88] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [89] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [90] scala.reflect.internal.Definitions$DefinitionsClass.init
  [91] scala.reflect.runtime.JavaUniverse.init
  [92] scala.reflect.runtime.JavaUniverse.<init>
  [93] scala.reflect.runtime.package$.universe$lzycompute
  [94] scala.reflect.runtime.package$.universe
  [95] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [96] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [97] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [98] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [99] org.apache.spark.sql.Dataset$.ofRows
  [100] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [101] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [102] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [103] sun.reflect.NativeMethodAccessorImpl.invoke0
  [104] sun.reflect.NativeMethodAccessorImpl.invoke
  [105] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [106] java.lang.reflect.Method.invoke
  [107] py4j.reflection.MethodInvoker.invoke
  [108] py4j.reflection.ReflectionEngine.invoke
  [109] py4j.Gateway.invoke
  [110] py4j.commands.AbstractCommand.invokeMethod
  [111] py4j.commands.CallCommand.execute
  [112] py4j.GatewayConnection.run
  [113] java.lang.Thread.run
  [114] [tid=16145]

--- 1551158159997385 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.io.Codec$.<init>
  [10] scala.io.Codec$.<clinit>
  [11] scala.reflect.internal.Names$class.newTermName
  [12] scala.reflect.internal.SymbolTable.newTermName
  [13] scala.reflect.internal.pickling.UnPickler$Scan.readName
  [14] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$2.apply
  [15] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$2.apply
  [16] scala.reflect.internal.pickling.UnPickler$Scan.at
  [17] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [18] scala.reflect.internal.pickling.UnPickler$Scan.run
  [19] scala.reflect.internal.pickling.UnPickler.unpickle
  [20] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [21] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [22] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [23] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [24] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [25] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [26] scala.reflect.internal.Symbols$Symbol.info
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [28] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [29] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [30] scala.reflect.runtime.Gil$class.gilSynchronized
  [31] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [32] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [33] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [34] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [36] scala.reflect.internal.SymbolTable.openPackageModule
  [37] scala.reflect.internal.SymbolTable.openPackageModule
  [38] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [39] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [40] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [41] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [42] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [43] scala.reflect.internal.Symbols$Symbol.info
  [44] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [45] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [46] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [47] scala.reflect.runtime.Gil$class.gilSynchronized
  [48] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [49] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [50] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [51] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [52] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [53] scala.reflect.internal.Types$TypeRef.thisInfo
  [54] scala.reflect.internal.Types$TypeRef.baseClasses
  [55] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [56] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [57] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [58] scala.reflect.internal.Types$Type.findMember
  [59] scala.reflect.internal.Types$Type.memberBasedOnName
  [60] scala.reflect.internal.Types$Type.member
  [61] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [62] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [63] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [64] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [65] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [66] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [67] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [68] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [69] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [70] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [71] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [72] scala.reflect.internal.Mirrors$RootsBase.init
  [73] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [74] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [75] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [76] scala.reflect.runtime.Gil$class.gilSynchronized
  [77] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [78] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [79] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [80] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [81] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [82] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [83] scala.reflect.runtime.Gil$class.gilSynchronized
  [84] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [85] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [86] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [87] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [88] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [89] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [90] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [91] scala.reflect.internal.Types$Type.findMember
  [92] scala.reflect.internal.Types$Type.memberBasedOnName
  [93] scala.reflect.internal.Types$Type.member
  [94] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [95] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [96] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [97] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [98] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [99] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [100] scala.reflect.internal.Definitions$DefinitionsClass.init
  [101] scala.reflect.runtime.JavaUniverse.init
  [102] scala.reflect.runtime.JavaUniverse.<init>
  [103] scala.reflect.runtime.package$.universe$lzycompute
  [104] scala.reflect.runtime.package$.universe
  [105] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [106] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [107] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [108] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [109] org.apache.spark.sql.Dataset$.ofRows
  [110] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [111] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [112] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [113] sun.reflect.NativeMethodAccessorImpl.invoke0
  [114] sun.reflect.NativeMethodAccessorImpl.invoke
  [115] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [116] java.lang.reflect.Method.invoke
  [117] py4j.reflection.MethodInvoker.invoke
  [118] py4j.reflection.ReflectionEngine.invoke
  [119] py4j.Gateway.invoke
  [120] py4j.commands.AbstractCommand.invokeMethod
  [121] py4j.commands.CallCommand.execute
  [122] py4j.GatewayConnection.run
  [123] java.lang.Thread.run
  [124] [tid=16145]

--- 1551158160095689 us
  [ 0] SpinPause
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16079]

--- 1551158160096767 us
  [ 0] ParallelTaskTerminator::offer_termination(TerminatorTerminator*)
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16077]

--- 1551158160096830 us
  [ 0] ParallelTaskTerminator::offer_termination(TerminatorTerminator*)
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16083]

--- 1551158160111836 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] scala.reflect.runtime.TwoWayCaches$TwoWayCache.scala$reflect$runtime$TwoWayCaches$TwoWayCache$$SomeRef$lzycompute
  [21] scala.reflect.runtime.TwoWayCaches$TwoWayCache.scala$reflect$runtime$TwoWayCaches$TwoWayCache$$SomeRef
  [22] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [23] scala.reflect.runtime.Gil$class.gilSynchronized
  [24] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [25] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [26] scala.reflect.runtime.JavaMirrors$JavaMirror.packageToScala
  [27] scala.reflect.runtime.JavaMirrors$JavaMirror.packageNameToScala
  [28] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [29] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [30] scala.reflect.runtime.Gil$class.gilSynchronized
  [31] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [32] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [33] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [34] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [35] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [36] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [37] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [38] scala.reflect.internal.Types$Type.findMember
  [39] scala.reflect.internal.Types$Type.memberBasedOnName
  [40] scala.reflect.internal.Types$Type.member
  [41] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [42] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [43] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [44] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [45] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [46] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [47] scala.reflect.internal.Definitions$DefinitionsClass.ObjectTpe$lzycompute
  [48] scala.reflect.internal.Definitions$DefinitionsClass.ObjectTpe
  [49] scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass$lzycompute
  [50] scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass
  [51] scala.reflect.runtime.JavaMirrors$class.missingHook
  [52] scala.reflect.runtime.JavaUniverse.missingHook
  [53] scala.reflect.internal.Mirrors$RootsBase.universeMissingHook
  [54] scala.reflect.internal.Mirrors$RootsBase.missingHook
  [55] scala.reflect.internal.pickling.UnPickler$Scan.readExtSymbol$1
  [56] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [57] scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef
  [58] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [59] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [60] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [61] scala.reflect.internal.pickling.UnPickler$Scan.at
  [62] scala.reflect.internal.pickling.UnPickler$Scan.readTypeRef
  [63] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypes$1$1.apply
  [64] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypes$1$1.apply
  [65] scala.reflect.internal.pickling.PickleBuffer.until
  [66] scala.reflect.internal.pickling.UnPickler$Scan.readTypes$1
  [67] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [68] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [69] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [70] scala.reflect.internal.pickling.UnPickler$Scan.at
  [71] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.completeInternal
  [72] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete
  [73] scala.reflect.internal.Symbols$Symbol.info
  [74] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [75] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [76] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [77] scala.reflect.runtime.Gil$class.gilSynchronized
  [78] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [79] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [80] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.gilSynchronizedIfNotThreadsafe
  [81] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [82] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.info
  [83] scala.reflect.internal.Types$TypeRef.decls
  [84] scala.reflect.internal.SymbolTable.openPackageModule
  [85] scala.reflect.internal.SymbolTable.openPackageModule
  [86] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [87] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [88] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [89] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [90] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [91] scala.reflect.internal.Symbols$Symbol.info
  [92] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [93] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [94] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [95] scala.reflect.runtime.Gil$class.gilSynchronized
  [96] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [97] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [98] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [99] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [100] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [101] scala.reflect.internal.Types$TypeRef.thisInfo
  [102] scala.reflect.internal.Types$TypeRef.baseClasses
  [103] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [104] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [105] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [106] scala.reflect.internal.Types$Type.findMember
  [107] scala.reflect.internal.Types$Type.memberBasedOnName
  [108] scala.reflect.internal.Types$Type.member
  [109] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [110] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [111] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [112] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [113] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [114] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [115] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [116] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [117] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [118] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [119] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [120] scala.reflect.internal.Mirrors$RootsBase.init
  [121] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [122] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [123] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [124] scala.reflect.runtime.Gil$class.gilSynchronized
  [125] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [126] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [127] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [128] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [129] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [130] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [131] scala.reflect.runtime.Gil$class.gilSynchronized
  [132] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [133] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [134] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [135] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [136] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [137] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [138] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [139] scala.reflect.internal.Types$Type.findMember
  [140] scala.reflect.internal.Types$Type.memberBasedOnName
  [141] scala.reflect.internal.Types$Type.member
  [142] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [143] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [144] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [145] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [146] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [147] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [148] scala.reflect.internal.Definitions$DefinitionsClass.init
  [149] scala.reflect.runtime.JavaUniverse.init
  [150] scala.reflect.runtime.JavaUniverse.<init>
  [151] scala.reflect.runtime.package$.universe$lzycompute
  [152] scala.reflect.runtime.package$.universe
  [153] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [154] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [155] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [156] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [157] org.apache.spark.sql.Dataset$.ofRows
  [158] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [159] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [160] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [161] sun.reflect.NativeMethodAccessorImpl.invoke0
  [162] sun.reflect.NativeMethodAccessorImpl.invoke
  [163] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [164] java.lang.reflect.Method.invoke
  [165] py4j.reflection.MethodInvoker.invoke
  [166] py4j.reflection.ReflectionEngine.invoke
  [167] py4j.Gateway.invoke
  [168] py4j.commands.AbstractCommand.invokeMethod
  [169] py4j.commands.CallCommand.execute
  [170] py4j.GatewayConnection.run
  [171] java.lang.Thread.run
  [172] [tid=16145]

--- 1551158160228631 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.internal.Types$AbstractNoArgsTypeRef.<init>
  [13] scala.reflect.internal.Types$TypeRef$.apply
  [14] scala.reflect.internal.Types$class.typeRef
  [15] scala.reflect.internal.SymbolTable.typeRef
  [16] scala.reflect.internal.Symbols$TypeSymbol.newTypeRef
  [17] scala.reflect.internal.Symbols$TypeSymbol.updateTypeCache
  [18] scala.reflect.internal.Symbols$TypeSymbol.maybeUpdateTypeCache
  [19] scala.reflect.internal.Symbols$TypeSymbol.tpe_$times
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$11.scala$reflect$runtime$SynchronizedSymbols$SynchronizedTypeSymbol$$super$tpe_$times
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$$anonfun$tpe_$times$1.apply
  [22] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$$anonfun$tpe_$times$1.apply
  [23] scala.reflect.runtime.Gil$class.gilSynchronized
  [24] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [26] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$11.gilSynchronizedIfNotThreadsafe
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$class.tpe_$times
  [28] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$11.tpe_$times
  [29] scala.reflect.internal.Symbols$Symbol.tpe
  [30] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$RepeatedParamClass$1.apply
  [31] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$RepeatedParamClass$1.apply
  [32] scala.reflect.internal.Definitions$DefinitionsClass.specialPolyClass
  [33] scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass$lzycompute
  [34] scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass
  [35] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [36] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [37] scala.reflect.internal.Mirrors$RootsBase.init
  [38] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [39] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [40] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [41] scala.reflect.runtime.Gil$class.gilSynchronized
  [42] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [43] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [44] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [45] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [46] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [47] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [48] scala.reflect.runtime.Gil$class.gilSynchronized
  [49] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [50] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [51] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [52] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [53] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [54] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [55] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [56] scala.reflect.internal.Types$Type.findMember
  [57] scala.reflect.internal.Types$Type.memberBasedOnName
  [58] scala.reflect.internal.Types$Type.member
  [59] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [60] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [61] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [62] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [63] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [64] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [65] scala.reflect.internal.Definitions$DefinitionsClass.init
  [66] scala.reflect.runtime.JavaUniverse.init
  [67] scala.reflect.runtime.JavaUniverse.<init>
  [68] scala.reflect.runtime.package$.universe$lzycompute
  [69] scala.reflect.runtime.package$.universe
  [70] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [71] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [72] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [73] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [74] org.apache.spark.sql.Dataset$.ofRows
  [75] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [76] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [77] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [78] sun.reflect.NativeMethodAccessorImpl.invoke0
  [79] sun.reflect.NativeMethodAccessorImpl.invoke
  [80] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [81] java.lang.reflect.Method.invoke
  [82] py4j.reflection.MethodInvoker.invoke
  [83] py4j.reflection.ReflectionEngine.invoke
  [84] py4j.Gateway.invoke
  [85] py4j.commands.AbstractCommand.invokeMethod
  [86] py4j.commands.CallCommand.execute
  [87] py4j.GatewayConnection.run
  [88] java.lang.Thread.run
  [89] [tid=16145]

--- 1551158160329841 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] scala.reflect.internal.AnnotationInfos$Annotatable$class.addThrowsAnnotation
  [13] scala.reflect.internal.Symbols$Symbol.addThrowsAnnotation
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$copyAnnotations
  [15] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [16] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [17] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [18] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [19] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [20] scala.reflect.runtime.Gil$class.gilSynchronized
  [21] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [22] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [23] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [24] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [25] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [26] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [27] scala.collection.IndexedSeqOptimized$class.foreach
  [28] scala.collection.mutable.ArrayOps$ofRef.foreach
  [29] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [30] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [31] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [32] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [33] scala.reflect.runtime.Gil$class.gilSynchronized
  [34] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [35] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.complete
  [37] scala.reflect.internal.Symbols$Symbol.info
  [38] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [39] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [40] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [41] scala.reflect.runtime.Gil$class.gilSynchronized
  [42] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [43] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [44] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [45] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [46] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [47] scala.reflect.internal.Symbols$Symbol.initialize
  [48] scala.reflect.internal.Definitions$DefinitionsClass.init
  [49] scala.reflect.runtime.JavaUniverse.init
  [50] scala.reflect.runtime.JavaUniverse.<init>
  [51] scala.reflect.runtime.package$.universe$lzycompute
  [52] scala.reflect.runtime.package$.universe
  [53] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [54] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [55] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [56] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [57] org.apache.spark.sql.Dataset$.ofRows
  [58] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [59] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [60] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [61] sun.reflect.NativeMethodAccessorImpl.invoke0
  [62] sun.reflect.NativeMethodAccessorImpl.invoke
  [63] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [64] java.lang.reflect.Method.invoke
  [65] py4j.reflection.MethodInvoker.invoke
  [66] py4j.reflection.ReflectionEngine.invoke
  [67] py4j.Gateway.invoke
  [68] py4j.commands.AbstractCommand.invokeMethod
  [69] py4j.commands.CallCommand.execute
  [70] py4j.GatewayConnection.run
  [71] java.lang.Thread.run
  [72] [tid=16145]

--- 1551158160431076 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.nonPrivateMember
  [28] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [29] scala.reflect.internal.Definitions$DefinitionsClass.getMember
  [30] scala.reflect.internal.Definitions$ValueClassDefinitions$class.valueClassSymbol
  [31] scala.reflect.internal.Definitions$ValueClassDefinitions$class.IntClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass.IntClass$lzycompute
  [33] scala.reflect.internal.Definitions$DefinitionsClass.IntClass
  [34] scala.reflect.internal.Definitions$DefinitionsClass.javaTypeToValueClass
  [35] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [37] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [38] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [39] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [40] scala.reflect.runtime.Gil$class.gilSynchronized
  [41] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [42] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [43] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [44] scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala
  [45] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [46] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [47] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [48] scala.collection.immutable.List.map
  [49] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [50] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [51] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [52] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [53] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [54] scala.reflect.runtime.Gil$class.gilSynchronized
  [55] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [56] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [57] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [58] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [59] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [60] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [61] scala.collection.IndexedSeqOptimized$class.foreach
  [62] scala.collection.mutable.ArrayOps$ofRef.foreach
  [63] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [64] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [65] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [66] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [67] scala.reflect.runtime.Gil$class.gilSynchronized
  [68] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [69] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [70] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.complete
  [71] scala.reflect.internal.Symbols$Symbol.info
  [72] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [73] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [74] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [75] scala.reflect.runtime.Gil$class.gilSynchronized
  [76] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [77] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [78] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [79] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [80] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [81] scala.reflect.internal.Symbols$Symbol.initialize
  [82] scala.reflect.internal.Definitions$DefinitionsClass.init
  [83] scala.reflect.runtime.JavaUniverse.init
  [84] scala.reflect.runtime.JavaUniverse.<init>
  [85] scala.reflect.runtime.package$.universe$lzycompute
  [86] scala.reflect.runtime.package$.universe
  [87] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [88] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [89] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [90] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [91] org.apache.spark.sql.Dataset$.ofRows
  [92] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [93] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [94] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [95] sun.reflect.NativeMethodAccessorImpl.invoke0
  [96] sun.reflect.NativeMethodAccessorImpl.invoke
  [97] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [98] java.lang.reflect.Method.invoke
  [99] py4j.reflection.MethodInvoker.invoke
  [100] py4j.reflection.ReflectionEngine.invoke
  [101] py4j.Gateway.invoke
  [102] py4j.commands.AbstractCommand.invokeMethod
  [103] py4j.commands.CallCommand.execute
  [104] py4j.GatewayConnection.run
  [105] java.lang.Thread.run
  [106] [tid=16145]

--- 1551158160531799 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.nonPrivateMember
  [28] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [29] scala.reflect.internal.Definitions$DefinitionsClass.getMember
  [30] scala.reflect.internal.Definitions$ValueClassDefinitions$class.valueClassSymbol
  [31] scala.reflect.internal.Definitions$ValueClassDefinitions$class.CharClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass.CharClass$lzycompute
  [33] scala.reflect.internal.Definitions$DefinitionsClass.CharClass
  [34] scala.reflect.internal.Definitions$DefinitionsClass.javaTypeToValueClass
  [35] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [37] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [38] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [39] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [40] scala.reflect.runtime.Gil$class.gilSynchronized
  [41] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [42] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [43] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [44] scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala
  [45] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [46] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [47] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala1
  [48] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala$1.apply
  [49] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala$1.apply
  [50] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [51] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [52] scala.reflect.runtime.Gil$class.gilSynchronized
  [53] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [54] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [55] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [56] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala
  [57] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$3.apply
  [58] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$3.apply
  [59] scala.collection.IndexedSeqOptimized$class.foreach
  [60] scala.collection.mutable.ArrayOps$ofRef.foreach
  [61] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [62] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [63] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [64] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [65] scala.reflect.runtime.Gil$class.gilSynchronized
  [66] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [67] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [68] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$LazyPolyType.complete
  [69] scala.reflect.internal.Symbols$Symbol.info
  [70] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [71] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [72] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [73] scala.reflect.runtime.Gil$class.gilSynchronized
  [74] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [75] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [76] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [77] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [78] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [79] scala.reflect.internal.Definitions$class.scala$reflect$internal$Definitions$$enterNewMethod
  [80] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus$lzycompute
  [81] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus
  [82] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods$lzycompute
  [83] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods
  [84] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode$lzycompute
  [85] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode
  [86] scala.reflect.internal.Definitions$DefinitionsClass.init
  [87] scala.reflect.runtime.JavaUniverse.init
  [88] scala.reflect.runtime.JavaUniverse.<init>
  [89] scala.reflect.runtime.package$.universe$lzycompute
  [90] scala.reflect.runtime.package$.universe
  [91] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [92] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [93] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [94] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [95] org.apache.spark.sql.Dataset$.ofRows
  [96] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [97] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [98] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [99] sun.reflect.NativeMethodAccessorImpl.invoke0
  [100] sun.reflect.NativeMethodAccessorImpl.invoke
  [101] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [102] java.lang.reflect.Method.invoke
  [103] py4j.reflection.MethodInvoker.invoke
  [104] py4j.reflection.ReflectionEngine.invoke
  [105] py4j.Gateway.invoke
  [106] py4j.commands.AbstractCommand.invokeMethod
  [107] py4j.commands.CallCommand.execute
  [108] py4j.GatewayConnection.run
  [109] java.lang.Thread.run
  [110] [tid=16145]

--- 1551158160632143 us
  [ 0] itable stub
  [ 1] scala.collection.generic.Growable$class.loop$1
  [ 2] scala.collection.generic.Growable$class.$plus$plus$eq
  [ 3] scala.collection.mutable.ArrayBuffer.$plus$plus$eq
  [ 4] scala.collection.mutable.ArrayBuffer.$plus$plus$eq
  [ 5] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [ 6] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [ 7] scala.collection.IndexedSeqOptimized$class.foreach
  [ 8] scala.collection.mutable.WrappedArray.foreach
  [ 9] scala.collection.TraversableLike$class.flatMap
  [10] scala.collection.AbstractTraversable.flatMap
  [11] scala.reflect.internal.Symbols$class.relevantSymbols
  [12] scala.reflect.internal.Symbols$class.markFlagsCompleted
  [13] scala.reflect.internal.SymbolTable.markFlagsCompleted
  [14] scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1
  [15] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [16] scala.reflect.internal.pickling.UnPickler$Scan.run
  [17] scala.reflect.internal.pickling.UnPickler.unpickle
  [18] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [19] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [20] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [21] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [22] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [23] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [24] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.load
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [26] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [27] scala.reflect.runtime.Gil$class.gilSynchronized
  [28] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [29] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [30] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [31] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.typeParams
  [32] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.typeParams
  [33] scala.reflect.internal.Types$class.isRawIfWithoutArgs
  [34] scala.reflect.internal.SymbolTable.isRawIfWithoutArgs
  [35] scala.reflect.internal.tpe.TypeMaps$$anon$1.apply
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [37] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [38] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [39] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [40] scala.collection.immutable.List.map
  [41] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [42] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [43] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [44] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [45] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [46] scala.reflect.runtime.Gil$class.gilSynchronized
  [47] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [48] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [49] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [50] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [51] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [52] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [53] scala.collection.IndexedSeqOptimized$class.foreach
  [54] scala.collection.mutable.ArrayOps$ofRef.foreach
  [55] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [56] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [57] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [58] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [59] scala.reflect.runtime.Gil$class.gilSynchronized
  [60] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [61] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [62] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$LazyPolyType.complete
  [63] scala.reflect.internal.Symbols$Symbol.info
  [64] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [65] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [66] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [67] scala.reflect.runtime.Gil$class.gilSynchronized
  [68] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [69] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [70] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [71] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [72] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [73] scala.reflect.internal.Definitions$class.scala$reflect$internal$Definitions$$enterNewMethod
  [74] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus$lzycompute
  [75] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus
  [76] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods$lzycompute
  [77] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods
  [78] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode$lzycompute
  [79] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode
  [80] scala.reflect.internal.Definitions$DefinitionsClass.init
  [81] scala.reflect.runtime.JavaUniverse.init
  [82] scala.reflect.runtime.JavaUniverse.<init>
  [83] scala.reflect.runtime.package$.universe$lzycompute
  [84] scala.reflect.runtime.package$.universe
  [85] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [86] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [87] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [88] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [89] org.apache.spark.sql.Dataset$.ofRows
  [90] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [91] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [92] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [93] sun.reflect.NativeMethodAccessorImpl.invoke0
  [94] sun.reflect.NativeMethodAccessorImpl.invoke
  [95] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [96] java.lang.reflect.Method.invoke
  [97] py4j.reflection.MethodInvoker.invoke
  [98] py4j.reflection.ReflectionEngine.invoke
  [99] py4j.Gateway.invoke
  [100] py4j.commands.AbstractCommand.invokeMethod
  [101] py4j.commands.CallCommand.execute
  [102] py4j.GatewayConnection.run
  [103] java.lang.Thread.run
  [104] [tid=16145]

--- 1551158160732790 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] scala.reflect.api.TypeTags$TypeTag$.<init>
  [21] scala.reflect.api.Universe.TypeTag$lzycompute
  [22] scala.reflect.api.Universe.TypeTag
  [23] scala.reflect.api.TypeTags$WeakTypeTag$.<init>
  [24] scala.reflect.api.Universe.WeakTypeTag$lzycompute
  [25] scala.reflect.api.Universe.WeakTypeTag
  [26] scala.reflect.runtime.JavaUniverseForce$class.force
  [27] scala.reflect.runtime.JavaUniverse.force
  [28] scala.reflect.runtime.JavaUniverse.init
  [29] scala.reflect.runtime.JavaUniverse.<init>
  [30] scala.reflect.runtime.package$.universe$lzycompute
  [31] scala.reflect.runtime.package$.universe
  [32] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [33] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [34] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [35] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [36] org.apache.spark.sql.Dataset$.ofRows
  [37] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [38] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [39] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [40] sun.reflect.NativeMethodAccessorImpl.invoke0
  [41] sun.reflect.NativeMethodAccessorImpl.invoke
  [42] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [43] java.lang.reflect.Method.invoke
  [44] py4j.reflection.MethodInvoker.invoke
  [45] py4j.reflection.ReflectionEngine.invoke
  [46] py4j.Gateway.invoke
  [47] py4j.commands.AbstractCommand.invokeMethod
  [48] py4j.commands.CallCommand.execute
  [49] py4j.GatewayConnection.run
  [50] java.lang.Thread.run
  [51] [tid=16145]

--- 1551158160832537 us
  [ 0] unroll_tree_refs_[k]
  [ 1] __audit_syscall_exit_[k]
  [ 2] syscall_slow_exit_work_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __vdso_clock_gettime
  [ 6] __GI___clock_gettime
  [ 7] [unknown]
  [ 8] scala.reflect.runtime.JavaUniverse$$anon$1.<init>
  [ 9] scala.reflect.runtime.JavaUniverse.internal$lzycompute
  [10] scala.reflect.runtime.JavaUniverse.internal
  [11] scala.reflect.runtime.JavaUniverseForce$class.force
  [12] scala.reflect.runtime.JavaUniverse.force
  [13] scala.reflect.runtime.JavaUniverse.init
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158160932284 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.Internals$class.treeBuild
  [10] scala.reflect.internal.SymbolTable.treeBuild$lzycompute
  [11] scala.reflect.internal.SymbolTable.treeBuild
  [12] scala.reflect.runtime.JavaUniverseForce$class.force
  [13] scala.reflect.runtime.JavaUniverse.force
  [14] scala.reflect.runtime.JavaUniverse.init
  [15] scala.reflect.runtime.JavaUniverse.<init>
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158161032026 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.SymbolTable.Throw$lzycompute
  [10] scala.reflect.internal.SymbolTable.Throw
  [11] scala.reflect.runtime.JavaUniverseForce$class.force
  [12] scala.reflect.runtime.JavaUniverse.force
  [13] scala.reflect.runtime.JavaUniverse.init
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158161131780 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.Trees$pendingSuperCall$.<init>
  [10] scala.reflect.internal.SymbolTable.pendingSuperCall$lzycompute
  [11] scala.reflect.internal.SymbolTable.pendingSuperCall
  [12] scala.reflect.runtime.JavaUniverseForce$class.force
  [13] scala.reflect.runtime.JavaUniverse.force
  [14] scala.reflect.runtime.JavaUniverse.init
  [15] scala.reflect.runtime.JavaUniverse.<init>
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158161233491 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.SymbolTable.RepeatedType$lzycompute
  [10] scala.reflect.internal.SymbolTable.RepeatedType
  [11] scala.reflect.runtime.JavaUniverseForce$class.force
  [12] scala.reflect.runtime.JavaUniverse.force
  [13] scala.reflect.runtime.JavaUniverse.init
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16145]

--- 1551158161337812 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.Types$ConstantType$.apply
  [11] scala.reflect.internal.Definitions$DefinitionsClass.ConstantTrue$lzycompute
  [12] scala.reflect.internal.Definitions$DefinitionsClass.ConstantTrue
  [13] scala.reflect.runtime.JavaUniverseForce$class.force
  [14] scala.reflect.runtime.JavaUniverse.force
  [15] scala.reflect.runtime.JavaUniverse.init
  [16] scala.reflect.runtime.JavaUniverse.<init>
  [17] scala.reflect.runtime.package$.universe$lzycompute
  [18] scala.reflect.runtime.package$.universe
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [20] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [22] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16145]

--- 1551158161439216 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.Types$Type.findDecl
  [22] scala.reflect.internal.Types$Type.decl
  [23] scala.reflect.internal.SymbolTable.openPackageModule
  [24] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [25] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [26] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [27] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [28] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [29] scala.reflect.internal.Symbols$Symbol.info
  [30] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [31] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [32] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [33] scala.reflect.runtime.Gil$class.gilSynchronized
  [34] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [37] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [38] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [39] scala.reflect.internal.Types$TypeRef.thisInfo
  [40] scala.reflect.internal.Types$TypeRef.baseClasses
  [41] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [42] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [43] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [44] scala.reflect.internal.Types$Type.findMember
  [45] scala.reflect.internal.Types$Type.memberBasedOnName
  [46] scala.reflect.internal.Types$Type.member
  [47] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [48] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [49] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [50] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [51] scala.reflect.internal.Mirrors$RootsBase.requiredClass
  [52] scala.reflect.internal.Definitions$DefinitionsClass.ScalaNumberClass$lzycompute
  [53] scala.reflect.internal.Definitions$DefinitionsClass.ScalaNumberClass
  [54] scala.reflect.runtime.JavaUniverseForce$class.force
  [55] scala.reflect.runtime.JavaUniverse.force
  [56] scala.reflect.runtime.JavaUniverse.init
  [57] scala.reflect.runtime.JavaUniverse.<init>
  [58] scala.reflect.runtime.package$.universe$lzycompute
  [59] scala.reflect.runtime.package$.universe
  [60] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [61] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [62] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [63] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [64] org.apache.spark.sql.Dataset$.ofRows
  [65] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [66] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [67] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [68] sun.reflect.NativeMethodAccessorImpl.invoke0
  [69] sun.reflect.NativeMethodAccessorImpl.invoke
  [70] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [71] java.lang.reflect.Method.invoke
  [72] py4j.reflection.MethodInvoker.invoke
  [73] py4j.reflection.ReflectionEngine.invoke
  [74] py4j.Gateway.invoke
  [75] py4j.commands.AbstractCommand.invokeMethod
  [76] py4j.commands.CallCommand.execute
  [77] py4j.GatewayConnection.run
  [78] java.lang.Thread.run
  [79] [tid=16145]

--- 1551158161543637 us
  [ 0] scala.reflect.internal.Types$ClassInfoType.<init>
  [ 1] scala.reflect.internal.pickling.UnPickler$Scan.CompoundType$1
  [ 2] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [ 3] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [ 4] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [ 5] scala.reflect.internal.pickling.UnPickler$Scan.at
  [ 6] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.completeInternal
  [ 7] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete
  [ 8] scala.reflect.internal.Symbols$Symbol.info
  [ 9] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [10] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [11] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [12] scala.reflect.runtime.Gil$class.gilSynchronized
  [13] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [14] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.gilSynchronizedIfNotThreadsafe
  [16] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.info
  [18] scala.reflect.internal.Types$TypeRef.decls
  [19] scala.reflect.internal.SymbolTable.openPackageModule
  [20] scala.reflect.internal.SymbolTable.openPackageModule
  [21] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [22] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [23] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [24] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [25] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [26] scala.reflect.internal.Symbols$Symbol.info
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [28] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [29] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [30] scala.reflect.runtime.Gil$class.gilSynchronized
  [31] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [32] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [33] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [34] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [36] scala.reflect.internal.Types$TypeRef.thisInfo
  [37] scala.reflect.internal.Types$TypeRef.baseClasses
  [38] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [39] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [40] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [41] scala.reflect.internal.Types$Type.findMember
  [42] scala.reflect.internal.Types$Type.memberBasedOnName
  [43] scala.reflect.internal.Types$Type.member
  [44] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [45] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [46] scala.reflect.internal.Mirrors$RootsBase.getModuleByName
  [47] scala.reflect.internal.Mirrors$RootsBase.getModuleIfDefined
  [48] scala.reflect.internal.Mirrors$RootsBase.getModuleIfDefined
  [49] scala.reflect.internal.Definitions$DefinitionsClass.ScalaXmlTopScope$lzycompute
  [50] scala.reflect.internal.Definitions$DefinitionsClass.ScalaXmlTopScope
  [51] scala.reflect.runtime.JavaUniverseForce$class.force
  [52] scala.reflect.runtime.JavaUniverse.force
  [53] scala.reflect.runtime.JavaUniverse.init
  [54] scala.reflect.runtime.JavaUniverse.<init>
  [55] scala.reflect.runtime.package$.universe$lzycompute
  [56] scala.reflect.runtime.package$.universe
  [57] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [58] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [59] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [60] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [61] org.apache.spark.sql.Dataset$.ofRows
  [62] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [63] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [64] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [65] sun.reflect.NativeMethodAccessorImpl.invoke0
  [66] sun.reflect.NativeMethodAccessorImpl.invoke
  [67] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [68] java.lang.reflect.Method.invoke
  [69] py4j.reflection.MethodInvoker.invoke
  [70] py4j.reflection.ReflectionEngine.invoke
  [71] py4j.Gateway.invoke
  [72] py4j.commands.AbstractCommand.invokeMethod
  [73] py4j.commands.CallCommand.execute
  [74] py4j.GatewayConnection.run
  [75] java.lang.Thread.run
  [76] [tid=16145]

--- 1551158161644475 us
  [ 0] scala.reflect.internal.Symbols$TypeSymbol.isType
  [ 1] scala.reflect.internal.Symbols$Symbol.isMonomorphicType
  [ 2] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$unsafeTypeParams$1.apply
  [ 3] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$unsafeTypeParams$1.apply
  [ 4] scala.reflect.runtime.Gil$class.gilSynchronized
  [ 5] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [ 6] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [ 7] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [ 8] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.unsafeTypeParams
  [ 9] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.unsafeTypeParams
  [10] scala.reflect.internal.Symbols$TypeSymbol.updateTypeCache
  [11] scala.reflect.internal.Symbols$TypeSymbol.maybeUpdateTypeCache
  [12] scala.reflect.internal.Symbols$TypeSymbol.tpe_$times
  [13] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedTypeSymbol$$super$tpe_$times
  [14] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$$anonfun$tpe_$times$1.apply
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$$anonfun$tpe_$times$1.apply
  [16] scala.reflect.runtime.Gil$class.gilSynchronized
  [17] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [19] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedTypeSymbol$class.tpe_$times
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.tpe_$times
  [22] scala.reflect.internal.BaseTypeSeqs$class.compoundBaseTypeSeq
  [23] scala.reflect.internal.SymbolTable.compoundBaseTypeSeq
  [24] scala.reflect.internal.Types$class.defineBaseTypeSeqOfCompoundType
  [25] scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$defineBaseTypeSeqOfCompoundType
  [26] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseTypeSeqOfCompoundType$1.apply
  [27] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseTypeSeqOfCompoundType$1.apply
  [28] scala.reflect.runtime.Gil$class.gilSynchronized
  [29] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [30] scala.reflect.runtime.SynchronizedTypes$class.defineBaseTypeSeqOfCompoundType
  [31] scala.reflect.runtime.JavaUniverse.defineBaseTypeSeqOfCompoundType
  [32] scala.reflect.internal.Types$CompoundType.baseTypeSeq
  [33] scala.reflect.internal.Symbols$Symbol.baseTypeSeqLength$1
  [34] scala.reflect.internal.Symbols$Symbol.isLess
  [35] scala.reflect.internal.Types$Type.baseTypeIndex
  [36] scala.reflect.internal.Types$class.isNew$1
  [37] scala.reflect.internal.Types$class.addMixinBaseClasses$1
  [38] scala.reflect.internal.Types$class.computeBaseClasses
  [39] scala.reflect.internal.SymbolTable.computeBaseClasses
  [40] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [41] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [42] scala.reflect.internal.Types$CompoundType.updateCache$1
  [43] scala.reflect.internal.Types$CompoundType.memo
  [44] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [45] scala.reflect.internal.Types$class.define$1
  [46] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [47] scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$defineBaseClassesOfCompoundType
  [48] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [49] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [50] scala.reflect.runtime.Gil$class.gilSynchronized
  [51] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [52] scala.reflect.runtime.SynchronizedTypes$class.defineBaseClassesOfCompoundType
  [53] scala.reflect.runtime.JavaUniverse.defineBaseClassesOfCompoundType
  [54] scala.reflect.internal.Types$CompoundType.baseClasses
  [55] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [56] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [57] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [58] scala.reflect.internal.Types$Type.findMember
  [59] scala.reflect.internal.Types$Type.memberBasedOnName
  [60] scala.reflect.internal.Types$Type.nonPrivateMember
  [61] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [62] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass$lzycompute
  [63] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass
  [64] scala.reflect.runtime.JavaUniverseForce$class.force
  [65] scala.reflect.runtime.JavaUniverse.force
  [66] scala.reflect.runtime.JavaUniverse.init
  [67] scala.reflect.runtime.JavaUniverse.<init>
  [68] scala.reflect.runtime.package$.universe$lzycompute
  [69] scala.reflect.runtime.package$.universe
  [70] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [71] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [72] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [73] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [74] org.apache.spark.sql.Dataset$.ofRows
  [75] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [76] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [77] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [78] sun.reflect.NativeMethodAccessorImpl.invoke0
  [79] sun.reflect.NativeMethodAccessorImpl.invoke
  [80] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [81] java.lang.reflect.Method.invoke
  [82] py4j.reflection.MethodInvoker.invoke
  [83] py4j.reflection.ReflectionEngine.invoke
  [84] py4j.Gateway.invoke
  [85] py4j.commands.AbstractCommand.invokeMethod
  [86] py4j.commands.CallCommand.execute
  [87] py4j.GatewayConnection.run
  [88] java.lang.Thread.run
  [89] [tid=16145]

--- 1551158161745351 us
  [ 0] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [ 1] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [ 2] scala.collection.IndexedSeqOptimized$class.foreach
  [ 3] scala.collection.mutable.WrappedArray.foreach
  [ 4] scala.collection.TraversableLike$class.flatMap
  [ 5] scala.collection.AbstractTraversable.flatMap
  [ 6] scala.reflect.internal.Symbols$class.relevantSymbols
  [ 7] scala.reflect.internal.Symbols$class.markFlagsCompleted
  [ 8] scala.reflect.internal.SymbolTable.markFlagsCompleted
  [ 9] scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1
  [10] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [11] scala.reflect.internal.pickling.UnPickler$Scan.run
  [12] scala.reflect.internal.pickling.UnPickler.unpickle
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [14] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [15] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [17] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [18] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [19] scala.reflect.internal.Symbols$Symbol.info
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [22] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [23] scala.reflect.runtime.Gil$class.gilSynchronized
  [24] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [26] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [28] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [29] scala.reflect.internal.Types$TypeRef.thisInfo
  [30] scala.reflect.internal.Types$TypeRef.baseClasses
  [31] scala.reflect.internal.Types$class.computeBaseClasses
  [32] scala.reflect.internal.SymbolTable.computeBaseClasses
  [33] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [34] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [35] scala.reflect.internal.Types$CompoundType.updateCache$1
  [36] scala.reflect.internal.Types$CompoundType.memo
  [37] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [38] scala.reflect.internal.Types$class.define$1
  [39] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [40] scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$defineBaseClassesOfCompoundType
  [41] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [42] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [43] scala.reflect.runtime.Gil$class.gilSynchronized
  [44] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [45] scala.reflect.runtime.SynchronizedTypes$class.defineBaseClassesOfCompoundType
  [46] scala.reflect.runtime.JavaUniverse.defineBaseClassesOfCompoundType
  [47] scala.reflect.internal.Types$CompoundType.baseClasses
  [48] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [49] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [50] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [51] scala.reflect.internal.Types$Type.findMember
  [52] scala.reflect.internal.Types$Type.memberBasedOnName
  [53] scala.reflect.internal.Types$Type.nonPrivateMember
  [54] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [55] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass$lzycompute
  [56] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass
  [57] scala.reflect.runtime.JavaUniverseForce$class.force
  [58] scala.reflect.runtime.JavaUniverse.force
  [59] scala.reflect.runtime.JavaUniverse.init
  [60] scala.reflect.runtime.JavaUniverse.<init>
  [61] scala.reflect.runtime.package$.universe$lzycompute
  [62] scala.reflect.runtime.package$.universe
  [63] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [64] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [65] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [66] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [67] org.apache.spark.sql.Dataset$.ofRows
  [68] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [69] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [70] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [71] sun.reflect.NativeMethodAccessorImpl.invoke0
  [72] sun.reflect.NativeMethodAccessorImpl.invoke
  [73] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [74] java.lang.reflect.Method.invoke
  [75] py4j.reflection.MethodInvoker.invoke
  [76] py4j.reflection.ReflectionEngine.invoke
  [77] py4j.Gateway.invoke
  [78] py4j.commands.AbstractCommand.invokeMethod
  [79] py4j.commands.CallCommand.execute
  [80] py4j.GatewayConnection.run
  [81] java.lang.Thread.run
  [82] [tid=16145]

--- 1551158161847392 us
  [ 0] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.<init>
  [ 1] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createModuleSymbol
  [ 2] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.createModuleSymbol
  [ 3] scala.reflect.internal.Symbols$Symbol.newTermSymbol
  [ 4] scala.reflect.internal.Symbols$Symbol.newModuleSymbol
  [ 5] scala.reflect.internal.Symbols$Symbol.newModule
  [ 6] scala.reflect.runtime.SymbolLoaders$class.initAndEnterClassAndModule
  [ 7] scala.reflect.runtime.JavaUniverse.initAndEnterClassAndModule
  [ 8] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [ 9] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [10] scala.reflect.runtime.Gil$class.gilSynchronized
  [11] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [12] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [13] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [14] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [15] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [16] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [17] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [18] scala.reflect.internal.Types$Type.findMember
  [19] scala.reflect.internal.Types$Type.memberBasedOnName
  [20] scala.reflect.internal.Types$Type.member
  [21] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [22] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [23] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [24] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [25] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [26] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [27] scala.collection.TraversableLike$$anonfun$map$1.apply
  [28] scala.collection.TraversableLike$$anonfun$map$1.apply
  [29] scala.collection.immutable.Range.foreach
  [30] scala.collection.TraversableLike$class.map
  [31] scala.collection.AbstractTraversable.map
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [33] scala.reflect.internal.Definitions$DefinitionsClass.ProductClass$lzycompute
  [34] scala.reflect.internal.Definitions$DefinitionsClass.ProductClass
  [35] scala.reflect.runtime.JavaUniverseForce$class.force
  [36] scala.reflect.runtime.JavaUniverse.force
  [37] scala.reflect.runtime.JavaUniverse.init
  [38] scala.reflect.runtime.JavaUniverse.<init>
  [39] scala.reflect.runtime.package$.universe$lzycompute
  [40] scala.reflect.runtime.package$.universe
  [41] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [42] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [43] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [44] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [45] org.apache.spark.sql.Dataset$.ofRows
  [46] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [47] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [48] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16145]

--- 1551158161947836 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.member
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [33] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [34] scala.collection.TraversableLike$$anonfun$map$1.apply
  [35] scala.collection.TraversableLike$$anonfun$map$1.apply
  [36] scala.collection.immutable.Range.foreach
  [37] scala.collection.TraversableLike$class.map
  [38] scala.collection.AbstractTraversable.map
  [39] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [40] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass$lzycompute
  [41] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass
  [42] scala.reflect.runtime.JavaUniverseForce$class.force
  [43] scala.reflect.runtime.JavaUniverse.force
  [44] scala.reflect.runtime.JavaUniverse.init
  [45] scala.reflect.runtime.JavaUniverse.<init>
  [46] scala.reflect.runtime.package$.universe$lzycompute
  [47] scala.reflect.runtime.package$.universe
  [48] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [49] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [50] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [51] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [52] org.apache.spark.sql.Dataset$.ofRows
  [53] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [54] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [55] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16145]

--- 1551158162047652 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.member
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [33] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [34] scala.collection.TraversableLike$$anonfun$map$1.apply
  [35] scala.collection.TraversableLike$$anonfun$map$1.apply
  [36] scala.collection.immutable.Range.foreach
  [37] scala.collection.TraversableLike$class.map
  [38] scala.collection.AbstractTraversable.map
  [39] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [40] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass$lzycompute
  [41] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass
  [42] scala.reflect.runtime.JavaUniverseForce$class.force
  [43] scala.reflect.runtime.JavaUniverse.force
  [44] scala.reflect.runtime.JavaUniverse.init
  [45] scala.reflect.runtime.JavaUniverse.<init>
  [46] scala.reflect.runtime.package$.universe$lzycompute
  [47] scala.reflect.runtime.package$.universe
  [48] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [49] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [50] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [51] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [52] org.apache.spark.sql.Dataset$.ofRows
  [53] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [54] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [55] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16145]

--- 1551158162148999 us
  [ 0] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [ 1] scala.reflect.internal.pickling.UnPickler$Scan.run
  [ 2] scala.reflect.internal.pickling.UnPickler.unpickle
  [ 3] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [ 4] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [ 5] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [ 6] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [ 7] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [ 8] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [ 9] scala.reflect.internal.Symbols$Symbol.info
  [10] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [11] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [12] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [13] scala.reflect.runtime.Gil$class.gilSynchronized
  [14] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [16] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [19] scala.reflect.internal.Symbols$TypeSymbol.isNonBottomSubClass
  [20] scala.reflect.internal.Symbols$Symbol.isSubClass
  [21] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$metaAnnotations$1.apply
  [22] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$metaAnnotations$1.apply
  [23] scala.collection.TraversableLike$$anonfun$filterImpl$1.apply
  [24] scala.collection.immutable.List.foreach
  [25] scala.collection.TraversableLike$class.filterImpl
  [26] scala.collection.TraversableLike$class.filter
  [27] scala.collection.AbstractTraversable.filter
  [28] scala.reflect.internal.Scopes$Scope.filter
  [29] scala.reflect.internal.Definitions$DefinitionsClass.metaAnnotations$lzycompute
  [30] scala.reflect.internal.Definitions$DefinitionsClass.metaAnnotations
  [31] scala.reflect.runtime.JavaUniverseForce$class.force
  [32] scala.reflect.runtime.JavaUniverse.force
  [33] scala.reflect.runtime.JavaUniverse.init
  [34] scala.reflect.runtime.JavaUniverse.<init>
  [35] scala.reflect.runtime.package$.universe$lzycompute
  [36] scala.reflect.runtime.package$.universe
  [37] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [38] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [39] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [40] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [41] org.apache.spark.sql.Dataset$.ofRows
  [42] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [43] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] sun.reflect.NativeMethodAccessorImpl.invoke0
  [46] sun.reflect.NativeMethodAccessorImpl.invoke
  [47] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [48] java.lang.reflect.Method.invoke
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.AbstractCommand.invokeMethod
  [53] py4j.commands.CallCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16145]

--- 1551158162249423 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [10] org.apache.spark.sql.catalyst.encoders.RowEncoder$$anonfun$2.apply
  [11] org.apache.spark.sql.catalyst.encoders.RowEncoder$$anonfun$2.apply
  [12] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [13] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [14] scala.collection.IndexedSeqOptimized$class.foreach
  [15] scala.collection.mutable.ArrayOps$ofRef.foreach
  [16] scala.collection.TraversableLike$class.flatMap
  [17] scala.collection.mutable.ArrayOps$ofRef.flatMap
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158162352416 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.Expression.references
  [10] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$$anonfun$9.apply
  [11] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$$anonfun$9.apply
  [12] scala.collection.LinearSeqOptimized$class.forall
  [13] scala.collection.immutable.List.forall
  [14] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.<init>
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16145]

--- 1551158162604315 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.execution.command.LocalTempView$.<init>
  [13] org.apache.spark.sql.execution.command.LocalTempView$.<clinit>
  [14] org.apache.spark.sql.Dataset.createTempViewCommand
  [15] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] py4j.reflection.MethodInvoker.invoke
  [21] py4j.reflection.ReflectionEngine.invoke
  [22] py4j.Gateway.invoke
  [23] py4j.commands.AbstractCommand.invokeMethod
  [24] py4j.commands.CallCommand.execute
  [25] py4j.GatewayConnection.run
  [26] java.lang.Thread.run
  [27] [tid=16145]

--- 1551158162718802 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] Runtime1::new_instance(JavaThread*, Klass*)
  [ 9] org.antlr.v4.runtime.atn.ATNDeserializer.edgeFactory
  [10] org.antlr.v4.runtime.atn.ATNDeserializer.deserialize
  [11] org.apache.spark.sql.catalyst.parser.SqlBaseLexer.<clinit>
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [13] org.apache.spark.sql.execution.SparkSqlParser.parse
  [14] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [15] org.apache.spark.sql.Dataset.createTempViewCommand
  [16] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16145]

--- 1551158162819949 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [10] org.apache.spark.sql.execution.SparkSqlParser.parse
  [11] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [12] org.apache.spark.sql.Dataset.createTempViewCommand
  [13] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [14] sun.reflect.NativeMethodAccessorImpl.invoke0
  [15] sun.reflect.NativeMethodAccessorImpl.invoke
  [16] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [17] java.lang.reflect.Method.invoke
  [18] py4j.reflection.MethodInvoker.invoke
  [19] py4j.reflection.ReflectionEngine.invoke
  [20] py4j.Gateway.invoke
  [21] py4j.commands.AbstractCommand.invokeMethod
  [22] py4j.commands.CallCommand.execute
  [23] py4j.GatewayConnection.run
  [24] java.lang.Thread.run
  [25] [tid=16145]

--- 1551158162925625 us
  [ 0] org.antlr.v4.runtime.atn.ATNDeserializer.deserialize
  [ 1] org.apache.spark.sql.catalyst.parser.SqlBaseParser.<clinit>
  [ 2] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [ 3] org.apache.spark.sql.execution.SparkSqlParser.parse
  [ 4] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [ 5] org.apache.spark.sql.Dataset.createTempViewCommand
  [ 6] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [ 7] sun.reflect.NativeMethodAccessorImpl.invoke0
  [ 8] sun.reflect.NativeMethodAccessorImpl.invoke
  [ 9] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [10] java.lang.reflect.Method.invoke
  [11] py4j.reflection.MethodInvoker.invoke
  [12] py4j.reflection.ReflectionEngine.invoke
  [13] py4j.Gateway.invoke
  [14] py4j.commands.AbstractCommand.invokeMethod
  [15] py4j.commands.CallCommand.execute
  [16] py4j.GatewayConnection.run
  [17] java.lang.Thread.run
  [18] [tid=16145]

--- 1551158163025910 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [11] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [13] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [14] org.apache.spark.sql.execution.SparkSqlParser.parse
  [15] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [16] org.apache.spark.sql.Dataset.createTempViewCommand
  [17] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] py4j.reflection.MethodInvoker.invoke
  [23] py4j.reflection.ReflectionEngine.invoke
  [24] py4j.Gateway.invoke
  [25] py4j.commands.AbstractCommand.invokeMethod
  [26] py4j.commands.CallCommand.execute
  [27] py4j.GatewayConnection.run
  [28] java.lang.Thread.run
  [29] [tid=16145]

--- 1551158163125632 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [13] org.apache.spark.sql.execution.SparkSqlParser.parse
  [14] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [15] org.apache.spark.sql.Dataset.createTempViewCommand
  [16] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16145]

--- 1551158163225916 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitTableIdentifier$1.apply
  [10] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitTableIdentifier$1.apply
  [11] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [12] org.apache.spark.sql.catalyst.parser.AstBuilder.visitTableIdentifier
  [13] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleTableIdentifier$1.apply
  [14] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleTableIdentifier$1.apply
  [15] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [16] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleTableIdentifier
  [17] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseTableIdentifier$1.apply
  [18] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseTableIdentifier$1.apply
  [19] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [20] org.apache.spark.sql.execution.SparkSqlParser.parse
  [21] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [22] org.apache.spark.sql.Dataset.createTempViewCommand
  [23] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158163331564 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.optimizer.Optimizer.defaultBatches
  [12] org.apache.spark.sql.execution.SparkOptimizer.defaultBatches
  [13] org.apache.spark.sql.catalyst.optimizer.Optimizer.batches
  [14] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [15] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [16] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [17] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [18] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [19] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [20] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [21] org.apache.spark.sql.Dataset.withAction
  [22] org.apache.spark.sql.Dataset.<init>
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [25] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16145]

--- 1551158163443570 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTime$.apply
  [10] org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTime$.apply
  [11] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [12] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [13] scala.collection.IndexedSeqOptimized$class.foldl
  [14] scala.collection.IndexedSeqOptimized$class.foldLeft
  [15] scala.collection.mutable.WrappedArray.foldLeft
  [16] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [17] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [18] scala.collection.immutable.List.foreach
  [19] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [20] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [21] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [22] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [23] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [24] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [25] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [26] org.apache.spark.sql.Dataset.withAction
  [27] org.apache.spark.sql.Dataset.<init>
  [28] org.apache.spark.sql.Dataset$.ofRows
  [29] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [30] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [31] sun.reflect.NativeMethodAccessorImpl.invoke0
  [32] sun.reflect.NativeMethodAccessorImpl.invoke
  [33] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [34] java.lang.reflect.Method.invoke
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.AbstractCommand.invokeMethod
  [39] py4j.commands.CallCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16145]

--- 1551158163546119 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.ExpressionSet$.apply
  [10] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.org$apache$spark$sql$catalyst$optimizer$ReorderAssociativeOperator$$collectGroupingExpressions
  [11] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$$anonfun$apply$3.applyOrElse
  [12] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$$anonfun$apply$3.applyOrElse
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [14] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [15] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [16] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [17] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [20] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [21] org.apache.spark.sql.catalyst.trees.TreeNode.transform
  [22] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.apply
  [23] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.apply
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [25] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [26] scala.collection.LinearSeqOptimized$class.foldLeft
  [27] scala.collection.immutable.List.foldLeft
  [28] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [29] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [30] scala.collection.immutable.List.foreach
  [31] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [32] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [33] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [34] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [35] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [36] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [37] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [38] org.apache.spark.sql.Dataset.withAction
  [39] org.apache.spark.sql.Dataset.<init>
  [40] org.apache.spark.sql.Dataset$.ofRows
  [41] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [42] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [43] sun.reflect.NativeMethodAccessorImpl.invoke0
  [44] sun.reflect.NativeMethodAccessorImpl.invoke
  [45] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [46] java.lang.reflect.Method.invoke
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.AbstractCommand.invokeMethod
  [51] py4j.commands.CallCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16145]

--- 1551158163660111 us
  [ 0] ClassFileParser::verify_legal_utf8(unsigned char const*, int, Thread*)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] org.apache.spark.sql.execution.SparkPlanner.strategies
  [21] org.apache.spark.sql.catalyst.planning.QueryPlanner.plan
  [22] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [23] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [24] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [25] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [26] org.apache.spark.sql.Dataset.withAction
  [27] org.apache.spark.sql.Dataset.<init>
  [28] org.apache.spark.sql.Dataset$.ofRows
  [29] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [30] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [31] sun.reflect.NativeMethodAccessorImpl.invoke0
  [32] sun.reflect.NativeMethodAccessorImpl.invoke
  [33] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [34] java.lang.reflect.Method.invoke
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.AbstractCommand.invokeMethod
  [39] py4j.commands.CallCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16145]

--- 1551158163800421 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [10] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [11] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [12] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [13] scala.collection.LinearSeqOptimized$class.foldLeft
  [14] scala.collection.immutable.List.foldLeft
  [15] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [16] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [17] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [18] org.apache.spark.sql.Dataset.withAction
  [19] org.apache.spark.sql.Dataset.<init>
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [22] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16145]

--- 1551158163886186 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onExecutionStart
  [12] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onOtherEvent
  [13] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [14] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [15] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [16] org.apache.spark.util.ListenerBus$class.postToAll
  [17] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [18] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [19] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [20] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [21] scala.util.DynamicVariable.withValue
  [22] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [23] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [24] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [25] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [26] [tid=16265]

--- 1551158163911356 us
  [ 0] com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.readUTF
  [ 1] com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.readUTF8
  [ 2] com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.readMethod
  [ 3] com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.accept
  [ 4] com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.access$200
  [ 5] com.thoughtworks.paranamer.BytecodeReadingParanamer.lookupParameterNames
  [ 6] com.thoughtworks.paranamer.CachingParanamer.lookupParameterNames
  [ 7] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.com$fasterxml$jackson$module$scala$introspect$BeanIntrospector$$getCtorParams
  [ 8] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$1.apply
  [ 9] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$1.apply
  [10] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [11] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [12] scala.collection.Iterator$class.foreach
  [13] scala.collection.AbstractIterator.foreach
  [14] scala.collection.IterableLike$class.foreach
  [15] scala.collection.AbstractIterable.foreach
  [16] scala.collection.TraversableLike$class.flatMap
  [17] scala.collection.AbstractTraversable.flatMap
  [18] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.com$fasterxml$jackson$module$scala$introspect$BeanIntrospector$$findConstructorParam$1
  [19] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2$$anonfun$apply$5.apply
  [20] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2$$anonfun$apply$5.apply
  [21] scala.collection.TraversableLike$$anonfun$map$1.apply
  [22] scala.collection.TraversableLike$$anonfun$map$1.apply
  [23] scala.collection.IndexedSeqOptimized$class.foreach
  [24] scala.collection.mutable.ArrayOps$ofRef.foreach
  [25] scala.collection.TraversableLike$class.map
  [26] scala.collection.mutable.ArrayOps$ofRef.map
  [27] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2.apply
  [28] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$$anonfun$2.apply
  [29] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [30] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [31] scala.collection.immutable.List.foreach
  [32] scala.collection.TraversableLike$class.flatMap
  [33] scala.collection.immutable.List.flatMap
  [34] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.apply
  [35] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$._descriptorFor
  [36] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.fieldName
  [37] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.findImplicitPropertyName
  [38] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.findImplicitPropertyName
  [39] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addFields
  [40] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [41] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [42] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [43] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [44] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [45] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [46] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [47] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [48] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [49] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [50] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [51] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [52] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [53] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [54] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [55] org.apache.spark.scheduler.EventLoggingListener.onOtherEvent
  [56] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [57] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [58] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [59] org.apache.spark.util.ListenerBus$class.postToAll
  [60] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [61] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [62] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [63] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [64] scala.util.DynamicVariable.withValue
  [65] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [66] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [67] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [68] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [69] [tid=16267]

--- 1551158163931585 us
  [ 0] java.lang.ProcessEnvironment$StringEntry.getKey
  [ 1] scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next
  [ 2] scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next
  [ 3] scala.collection.Iterator$class.foreach
  [ 4] scala.collection.AbstractIterator.foreach
  [ 5] scala.collection.IterableLike$class.foreach
  [ 6] scala.collection.AbstractIterable.foreach
  [ 7] scala.collection.generic.Growable$class.$plus$plus$eq
  [ 8] scala.collection.mutable.ArrayBuffer.$plus$plus$eq
  [ 9] scala.collection.mutable.ArrayBuffer.$plus$plus$eq
  [10] scala.collection.TraversableOnce$class.copyToBuffer
  [11] scala.collection.AbstractTraversable.copyToBuffer
  [12] scala.collection.MapLike$class.toBuffer
  [13] scala.collection.AbstractMap.toBuffer
  [14] scala.collection.MapLike$class.toSeq
  [15] scala.collection.AbstractMap.toSeq
  [16] scala.sys.package$.env
  [17] org.apache.spark.util.Utils$.isTesting
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.assertNotAnalysisRule
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.assertNotAnalysisRule
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [21] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [22] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [23] org.apache.spark.sql.catalyst.trees.TreeNode.transform
  [24] org.apache.spark.sql.catalyst.optimizer.RemoveRedundantProject$.apply
  [25] org.apache.spark.sql.catalyst.optimizer.RemoveRedundantProject$.apply
  [26] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [27] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [28] scala.collection.LinearSeqOptimized$class.foldLeft
  [29] scala.collection.immutable.List.foldLeft
  [30] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [31] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [32] scala.collection.immutable.List.foreach
  [33] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [34] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [35] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [36] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [37] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [38] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [39] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [40] org.apache.spark.sql.Dataset.withAction
  [41] org.apache.spark.sql.Dataset.<init>
  [42] org.apache.spark.sql.Dataset$.ofRows
  [43] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [44] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [45] sun.reflect.NativeMethodAccessorImpl.invoke0
  [46] sun.reflect.NativeMethodAccessorImpl.invoke
  [47] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [48] java.lang.reflect.Method.invoke
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.AbstractCommand.invokeMethod
  [53] py4j.commands.CallCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16145]

--- 1551158164058483 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.collection.convert.DecorateAsJava$class.asJavaCollectionConverter
  [10] scala.collection.JavaConverters$.asJavaCollectionConverter
  [11] com.fasterxml.jackson.module.scala.ser.IterableSerializer$class.serializeContents
  [12] com.fasterxml.jackson.module.scala.ser.ResolvedIterableSerializer.serializeContents
  [13] com.fasterxml.jackson.module.scala.ser.ResolvedIterableSerializer.serializeContents
  [14] com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize
  [15] com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField
  [16] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields
  [17] com.fasterxml.jackson.databind.ser.BeanSerializer.serialize
  [18] com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField
  [19] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields
  [20] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType
  [21] com.fasterxml.jackson.databind.ser.impl.TypeWrappedSerializer.serialize
  [22] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [23] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [24] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [25] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [26] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [27] org.apache.spark.scheduler.EventLoggingListener.onOtherEvent
  [28] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [29] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [30] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [31] org.apache.spark.util.ListenerBus$class.postToAll
  [32] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [33] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [34] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [36] scala.util.DynamicVariable.withValue
  [37] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [38] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [39] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [40] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [41] [tid=16267]

--- 1551158164096007 us
  [ 0] org.antlr.v4.runtime.misc.Array2DHashSet.getOrAddImpl
  [ 1] org.antlr.v4.runtime.misc.Array2DHashSet.getOrAdd
  [ 2] org.antlr.v4.runtime.atn.ATNConfigSet.add
  [ 3] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [ 4] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [ 5] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [ 6] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [ 7] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [ 8] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [ 9] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [10] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [11] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [12] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [13] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [14] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [15] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [16] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [17] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [18] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [19] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [20] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [21] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [22] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [23] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [24] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [25] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [26] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [27] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [28] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [29] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [30] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [31] org.antlr.v4.runtime.atn.ParserATNSimulator.closure_
  [32] org.antlr.v4.runtime.atn.ParserATNSimulator.closureCheckingStopState
  [33] org.antlr.v4.runtime.atn.ParserATNSimulator.closure
  [34] org.antlr.v4.runtime.atn.ParserATNSimulator.computeStartState
  [35] org.antlr.v4.runtime.atn.ParserATNSimulator.adaptivePredict
  [36] org.apache.spark.sql.catalyst.parser.SqlBaseParser.valueExpression
  [37] org.apache.spark.sql.catalyst.parser.SqlBaseParser.valueExpression
  [38] org.apache.spark.sql.catalyst.parser.SqlBaseParser.booleanExpression
  [39] org.apache.spark.sql.catalyst.parser.SqlBaseParser.querySpecification
  [40] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryPrimary
  [41] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryTerm
  [42] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryNoWith
  [43] org.apache.spark.sql.catalyst.parser.SqlBaseParser.query
  [44] org.apache.spark.sql.catalyst.parser.SqlBaseParser.statement
  [45] org.apache.spark.sql.catalyst.parser.SqlBaseParser.singleStatement
  [46] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [47] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [48] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [49] org.apache.spark.sql.execution.SparkSqlParser.parse
  [50] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan
  [51] org.apache.spark.sql.SparkSession.sql
  [52] sun.reflect.NativeMethodAccessorImpl.invoke0
  [53] sun.reflect.NativeMethodAccessorImpl.invoke
  [54] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [55] java.lang.reflect.Method.invoke
  [56] py4j.reflection.MethodInvoker.invoke
  [57] py4j.reflection.ReflectionEngine.invoke
  [58] py4j.Gateway.invoke
  [59] py4j.commands.AbstractCommand.invokeMethod
  [60] py4j.commands.CallCommand.execute
  [61] py4j.GatewayConnection.run
  [62] java.lang.Thread.run
  [63] [tid=16145]

--- 1551158164209898 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.createProject$1
  [10] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.apply
  [11] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.apply
  [12] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [13] org.apache.spark.sql.catalyst.parser.AstBuilder.org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification
  [14] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuerySpecification$1.apply
  [15] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuerySpecification$1.apply
  [16] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [17] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuerySpecification
  [18] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuerySpecification
  [19] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QuerySpecificationContext.accept
  [20] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [21] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault
  [22] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept
  [23] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [24] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault
  [25] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept
  [26] org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit
  [27] org.apache.spark.sql.catalyst.parser.AstBuilder.plan
  [28] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply
  [29] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply
  [30] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [31] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery
  [32] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery
  [33] org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleInsertQueryContext.accept
  [34] org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit
  [35] org.apache.spark.sql.catalyst.parser.AstBuilder.plan
  [36] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply
  [37] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply
  [38] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [39] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery
  [40] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery
  [41] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept
  [42] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [43] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault
  [44] org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept
  [45] org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit
  [46] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply
  [47] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply
  [48] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [49] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement
  [50] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [51] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [52] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [53] org.apache.spark.sql.execution.SparkSqlParser.parse
  [54] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan
  [55] org.apache.spark.sql.SparkSession.sql
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16145]

--- 1551158164314645 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute
  [10] org.apache.spark.sql.internal.SharedState.externalCatalog
  [11] org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute
  [12] org.apache.spark.sql.internal.SharedState.globalTempViewManager
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$4.apply
  [14] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$4.apply
  [15] org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute
  [16] org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager
  [17] org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation
  [18] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog
  [19] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation
  [20] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse
  [21] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse
  [22] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [23] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [24] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [25] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [26] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [27] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [29] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [32] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [33] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [34] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [35] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [36] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [37] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [38] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [39] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [41] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [42] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [43] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [44] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [45] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [46] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [47] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [48] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [49] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [50] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [51] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [52] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [53] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [54] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [55] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [56] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [57] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [58] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [59] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [60] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [61] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [62] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [63] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [64] scala.collection.LinearSeqOptimized$class.foldLeft
  [65] scala.collection.immutable.List.foldLeft
  [66] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [67] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [68] scala.collection.immutable.List.foreach
  [69] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [70] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [71] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [72] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [73] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [74] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [75] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [76] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [77] org.apache.spark.sql.execution.QueryExecution.analyzed
  [78] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [79] org.apache.spark.sql.Dataset$.ofRows
  [80] org.apache.spark.sql.SparkSession.sql
  [81] sun.reflect.NativeMethodAccessorImpl.invoke0
  [82] sun.reflect.NativeMethodAccessorImpl.invoke
  [83] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [84] java.lang.reflect.Method.invoke
  [85] py4j.reflection.MethodInvoker.invoke
  [86] py4j.reflection.ReflectionEngine.invoke
  [87] py4j.Gateway.invoke
  [88] py4j.commands.AbstractCommand.invokeMethod
  [89] py4j.commands.CallCommand.execute
  [90] py4j.GatewayConnection.run
  [91] java.lang.Thread.run
  [92] [tid=16145]

--- 1551158164416290 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.commons.lang3.ClassUtils.isAssignable
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$6.apply
  [14] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$6.apply
  [15] scala.collection.IndexedSeqOptimized$$anonfun$1.apply
  [16] scala.collection.IndexedSeqOptimized$$anonfun$1.apply
  [17] scala.collection.IndexedSeqOptimized$class.segmentLength
  [18] scala.collection.mutable.ArrayOps$ofRef.segmentLength
  [19] scala.collection.GenSeqLike$class.prefixLength
  [20] scala.collection.mutable.ArrayOps$ofRef.prefixLength
  [21] scala.collection.IndexedSeqOptimized$class.find
  [22] scala.collection.mutable.ArrayOps$ofRef.find
  [23] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply
  [24] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply
  [25] org.apache.spark.sql.catalyst.errors.package$.attachTree
  [26] org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy
  [27] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [29] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [32] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [33] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [34] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [35] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [36] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [37] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [38] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [39] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [41] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [42] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [43] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [44] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [45] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [46] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [47] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [48] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [49] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [50] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [51] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [52] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [53] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [54] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [55] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [56] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [57] scala.collection.LinearSeqOptimized$class.foldLeft
  [58] scala.collection.immutable.List.foldLeft
  [59] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [60] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [61] scala.collection.immutable.List.foreach
  [62] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [63] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [64] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [65] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [66] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [67] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [68] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [69] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [70] org.apache.spark.sql.execution.QueryExecution.analyzed
  [71] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [72] org.apache.spark.sql.Dataset$.ofRows
  [73] org.apache.spark.sql.SparkSession.sql
  [74] sun.reflect.NativeMethodAccessorImpl.invoke0
  [75] sun.reflect.NativeMethodAccessorImpl.invoke
  [76] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [77] java.lang.reflect.Method.invoke
  [78] py4j.reflection.MethodInvoker.invoke
  [79] py4j.reflection.ReflectionEngine.invoke
  [80] py4j.Gateway.invoke
  [81] py4j.commands.AbstractCommand.invokeMethod
  [82] py4j.commands.CallCommand.execute
  [83] py4j.GatewayConnection.run
  [84] java.lang.Thread.run
  [85] [tid=16145]

--- 1551158164517337 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$.hasGroupingFunction
  [10] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse
  [11] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse
  [12] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [13] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [14] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [15] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [16] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [21] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [22] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [23] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [24] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [25] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [26] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [27] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [29] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [30] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$.apply
  [31] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$.apply
  [32] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [33] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [34] scala.collection.LinearSeqOptimized$class.foldLeft
  [35] scala.collection.immutable.List.foldLeft
  [36] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [37] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [38] scala.collection.immutable.List.foreach
  [39] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [40] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [41] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [42] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [43] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [44] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [45] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [46] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [47] org.apache.spark.sql.execution.QueryExecution.analyzed
  [48] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [49] org.apache.spark.sql.Dataset$.ofRows
  [50] org.apache.spark.sql.SparkSession.sql
  [51] sun.reflect.NativeMethodAccessorImpl.invoke0
  [52] sun.reflect.NativeMethodAccessorImpl.invoke
  [53] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [54] java.lang.reflect.Method.invoke
  [55] py4j.reflection.MethodInvoker.invoke
  [56] py4j.reflection.ReflectionEngine.invoke
  [57] py4j.Gateway.invoke
  [58] py4j.commands.AbstractCommand.invokeMethod
  [59] py4j.commands.CallCommand.execute
  [60] py4j.GatewayConnection.run
  [61] java.lang.Thread.run
  [62] [tid=16145]

--- 1551158164620804 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.Column$.apply
  [10] org.apache.spark.sql.Dataset$$anonfun$9.apply
  [11] org.apache.spark.sql.Dataset$$anonfun$9.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.immutable.List.foreach
  [15] scala.collection.TraversableLike$class.map
  [16] scala.collection.immutable.List.map
  [17] org.apache.spark.sql.Dataset.getRows
  [18] org.apache.spark.sql.Dataset.showString
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.AbstractCommand.invokeMethod
  [27] py4j.commands.CallCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16145]

--- 1551158164751049 us
  [ 0] [tid=16145]

--- 1551158164775162 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.spark_project.jetty.server.HttpConnection.newHttpChannel
  [11] org.spark_project.jetty.server.HttpConnection.<init>
  [12] org.spark_project.jetty.server.HttpConnectionFactory.newConnection
  [13] org.spark_project.jetty.server.ServerConnector$ServerConnectorManager.newConnection
  [14] org.spark_project.jetty.io.ManagedSelector.createEndPoint
  [15] org.spark_project.jetty.io.ManagedSelector.access$1600
  [16] org.spark_project.jetty.io.ManagedSelector$CreateEndPoint.run
  [17] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [18] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [19] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.execute
  [20] org.spark_project.jetty.io.ManagedSelector.run
  [21] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [22] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [23] java.lang.Thread.run
  [24] [tid=16199]

--- 1551158164875993 us
  [ 0] restore_nameidata_[k]
  [ 1] filename_lookup_[k]
  [ 2] vfs_statx_[k]
  [ 3] SYSC_newstat_[k]
  [ 4] do_syscall_64_[k]
  [ 5] entry_SYSCALL_64_after_hwframe_[k]
  [ 6] __xstat
  [ 7] java.io.UnixFileSystem.getBooleanAttributes0
  [ 8] java.io.UnixFileSystem.getBooleanAttributes
  [ 9] java.io.File.exists
  [10] sun.misc.URLClassPath$FileLoader.getResource
  [11] sun.misc.URLClassPath.getResource
  [12] java.net.URLClassLoader$1.run
  [13] java.net.URLClassLoader$1.run
  [14] java.security.AccessController.doPrivileged
  [15] java.net.URLClassLoader.findClass
  [16] java.lang.ClassLoader.loadClass
  [17] sun.misc.Launcher$AppClassLoader.loadClass
  [18] java.lang.ClassLoader.loadClass
  [19] org.spark_project.jetty.server.HttpChannelOverHttp.newHttpInput
  [20] org.spark_project.jetty.server.HttpChannel.<init>
  [21] org.spark_project.jetty.server.HttpChannelOverHttp.<init>
  [22] org.spark_project.jetty.server.HttpConnection.newHttpChannel
  [23] org.spark_project.jetty.server.HttpConnection.<init>
  [24] org.spark_project.jetty.server.HttpConnectionFactory.newConnection
  [25] org.spark_project.jetty.server.ServerConnector$ServerConnectorManager.newConnection
  [26] org.spark_project.jetty.io.ManagedSelector.createEndPoint
  [27] org.spark_project.jetty.io.ManagedSelector.access$1600
  [28] org.spark_project.jetty.io.ManagedSelector$CreateEndPoint.run
  [29] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [30] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [31] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.execute
  [32] org.spark_project.jetty.io.ManagedSelector.run
  [33] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [34] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [35] java.lang.Thread.run
  [36] [tid=16199]

--- 1551158164877690 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.plans.logical.Limit$.apply
  [10] org.apache.spark.sql.Dataset.limit
  [11] org.apache.spark.sql.Dataset.head
  [12] org.apache.spark.sql.Dataset.take
  [13] org.apache.spark.sql.Dataset.getRows
  [14] org.apache.spark.sql.Dataset.showString
  [15] sun.reflect.NativeMethodAccessorImpl.invoke0
  [16] sun.reflect.NativeMethodAccessorImpl.invoke
  [17] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [18] java.lang.reflect.Method.invoke
  [19] py4j.reflection.MethodInvoker.invoke
  [20] py4j.reflection.ReflectionEngine.invoke
  [21] py4j.Gateway.invoke
  [22] py4j.commands.AbstractCommand.invokeMethod
  [23] py4j.commands.CallCommand.execute
  [24] py4j.GatewayConnection.run
  [25] java.lang.Thread.run
  [26] [tid=16145]

--- 1551158164961258 us
  [ 0] __memcpy_sse2_unaligned_erms
  [ 1] PSPromotionManager::drain_stacks_depth(bool)
  [ 2] CardTableExtension::scavenge_contents_parallel(ObjectStartArray*, MutableSpace*, HeapWord*, PSPromotionManager*, unsigned int, unsigned int)
  [ 3] OldToYoungRootsTask::do_it(GCTaskManager*, unsigned int)
  [ 4] GCTaskThread::run()
  [ 5] java_start(Thread*)
  [ 6] start_thread
  [ 7] [tid=16081]

--- 1551158164963094 us
  [ 0] __memcpy_sse2_unaligned_erms
  [ 1] PSPromotionManager::drain_stacks_depth(bool)
  [ 2] CardTableExtension::scavenge_contents_parallel(ObjectStartArray*, MutableSpace*, HeapWord*, PSPromotionManager*, unsigned int, unsigned int)
  [ 3] OldToYoungRootsTask::do_it(GCTaskManager*, unsigned int)
  [ 4] GCTaskThread::run()
  [ 5] java_start(Thread*)
  [ 6] start_thread
  [ 7] [tid=16073]

--- 1551158164965303 us
  [ 0] SpinPause
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16091]

--- 1551158165002222 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.spark_project.jetty.server.HttpChannelOverHttp.<init>
  [10] org.spark_project.jetty.server.HttpConnection.newHttpChannel
  [11] org.spark_project.jetty.server.HttpConnection.<init>
  [12] org.spark_project.jetty.server.HttpConnectionFactory.newConnection
  [13] org.spark_project.jetty.server.ServerConnector$ServerConnectorManager.newConnection
  [14] org.spark_project.jetty.io.ManagedSelector.createEndPoint
  [15] org.spark_project.jetty.io.ManagedSelector.access$1600
  [16] org.spark_project.jetty.io.ManagedSelector$CreateEndPoint.run
  [17] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [18] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [19] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.execute
  [20] org.spark_project.jetty.io.ManagedSelector.run
  [21] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [22] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [23] java.lang.Thread.run
  [24] [tid=16199]

--- 1551158165065250 us
  [ 0] scala.collection.AbstractTraversable.toSet
  [ 1] org.apache.spark.sql.catalyst.expressions.AttributeSet.<init>
  [ 2] org.apache.spark.sql.catalyst.expressions.AttributeSet$.apply
  [ 3] org.apache.spark.sql.catalyst.plans.QueryPlan.inputSet
  [ 4] org.apache.spark.sql.catalyst.plans.QueryPlan.missingInput
  [ 5] org.apache.spark.sql.catalyst.plans.QueryPlan.statePrefix
  [ 6] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.statePrefix
  [ 7] org.apache.spark.sql.catalyst.plans.QueryPlan.simpleString
  [ 8] org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString
  [ 9] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [10] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [11] org.apache.spark.sql.catalyst.trees.TreeNode.treeString
  [12] org.apache.spark.sql.catalyst.trees.TreeNode.treeString
  [13] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [14] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [15] org.apache.spark.internal.Logging$class.logTrace
  [16] org.apache.spark.sql.catalyst.rules.RuleExecutor.logTrace
  [17] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [18] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [19] scala.collection.LinearSeqOptimized$class.foldLeft
  [20] scala.collection.immutable.List.foldLeft
  [21] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [22] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [23] scala.collection.immutable.List.foreach
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [25] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [26] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [27] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [28] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [29] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [30] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [31] org.apache.spark.sql.Dataset.withAction
  [32] org.apache.spark.sql.Dataset.head
  [33] org.apache.spark.sql.Dataset.take
  [34] org.apache.spark.sql.Dataset.getRows
  [35] org.apache.spark.sql.Dataset.showString
  [36] sun.reflect.NativeMethodAccessorImpl.invoke0
  [37] sun.reflect.NativeMethodAccessorImpl.invoke
  [38] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [39] java.lang.reflect.Method.invoke
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.AbstractCommand.invokeMethod
  [44] py4j.commands.CallCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158165169545 us
  [ 0] [tid=16145]

--- 1551158165269415 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.plans.logical.Statistics$.apply$default$4
  [10] org.apache.spark.sql.execution.LogicalRDD.computeStats
  [11] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default
  [12] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default
  [13] org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor$class.visit
  [14] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit
  [15] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [16] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$class.stats
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats
  [20] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitUnaryNode
  [21] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitFilter
  [22] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitFilter
  [23] org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor$class.visit
  [24] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit
  [25] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [26] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [27] scala.Option.getOrElse
  [28] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$class.stats
  [29] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats
  [30] org.apache.spark.sql.execution.SparkStrategies$JoinSelection$.canBroadcastByHints
  [31] org.apache.spark.sql.execution.SparkStrategies$JoinSelection$.apply
  [32] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply
  [33] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply
  [34] scala.collection.Iterator$$anon$12.nextCur
  [35] scala.collection.Iterator$$anon$12.hasNext
  [36] scala.collection.Iterator$$anon$12.hasNext
  [37] org.apache.spark.sql.catalyst.planning.QueryPlanner.plan
  [38] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply
  [39] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply
  [40] scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply
  [41] scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply
  [42] scala.collection.Iterator$class.foreach
  [43] scala.collection.AbstractIterator.foreach
  [44] scala.collection.TraversableOnce$class.foldLeft
  [45] scala.collection.AbstractIterator.foldLeft
  [46] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply
  [47] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply
  [48] scala.collection.Iterator$$anon$12.nextCur
  [49] scala.collection.Iterator$$anon$12.hasNext
  [50] org.apache.spark.sql.catalyst.planning.QueryPlanner.plan
  [51] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [52] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [53] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [54] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [55] org.apache.spark.sql.Dataset.withAction
  [56] org.apache.spark.sql.Dataset.head
  [57] org.apache.spark.sql.Dataset.take
  [58] org.apache.spark.sql.Dataset.getRows
  [59] org.apache.spark.sql.Dataset.showString
  [60] sun.reflect.NativeMethodAccessorImpl.invoke0
  [61] sun.reflect.NativeMethodAccessorImpl.invoke
  [62] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [63] java.lang.reflect.Method.invoke
  [64] py4j.reflection.MethodInvoker.invoke
  [65] py4j.reflection.ReflectionEngine.invoke
  [66] py4j.Gateway.invoke
  [67] py4j.commands.AbstractCommand.invokeMethod
  [68] py4j.commands.CallCommand.execute
  [69] py4j.GatewayConnection.run
  [70] java.lang.Thread.run
  [71] [tid=16145]

--- 1551158165369318 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.expressions.Ascending$.defaultNullOrdering
  [12] org.apache.spark.sql.catalyst.expressions.SortOrder$.apply
  [13] org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$requiredOrders$1.apply
  [14] org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$requiredOrders$1.apply
  [15] scala.collection.TraversableLike$$anonfun$map$1.apply
  [16] scala.collection.TraversableLike$$anonfun$map$1.apply
  [17] scala.collection.immutable.List.foreach
  [18] scala.collection.TraversableLike$class.map
  [19] scala.collection.immutable.List.map
  [20] org.apache.spark.sql.execution.joins.SortMergeJoinExec.requiredOrders
  [21] org.apache.spark.sql.execution.joins.SortMergeJoinExec.requiredChildOrdering
  [22] org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering
  [23] org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse
  [24] org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse
  [25] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [26] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [27] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [28] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [29] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [30] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [31] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [32] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [33] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [34] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [35] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [36] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [37] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [38] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [39] scala.collection.LinearSeqOptimized$class.foldLeft
  [40] scala.collection.immutable.List.foldLeft
  [41] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [42] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [43] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [44] org.apache.spark.sql.Dataset.withAction
  [45] org.apache.spark.sql.Dataset.head
  [46] org.apache.spark.sql.Dataset.take
  [47] org.apache.spark.sql.Dataset.getRows
  [48] org.apache.spark.sql.Dataset.showString
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16145]

--- 1551158165478560 us
  [ 0] SystemDictionary::resolve_or_fail(Symbol*, Handle, Handle, bool, Thread*)
  [ 1] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [ 2] ConstantPool::klass_ref_at(int, Thread*)
  [ 3] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 4] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 5] org.apache.spark.sql.execution.exchange.ReuseExchange$$anonfun$apply$2$$anonfun$2.<init>
  [ 6] org.apache.spark.sql.execution.exchange.ReuseExchange$$anonfun$apply$2.applyOrElse
  [ 7] org.apache.spark.sql.execution.exchange.ReuseExchange$$anonfun$apply$2.applyOrElse
  [ 8] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [ 9] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [10] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [11] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [12] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [14] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [15] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [16] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [17] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [18] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [19] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [20] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [21] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [22] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [23] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [24] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [25] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [26] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [27] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [28] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [29] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [30] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [31] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [32] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [33] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [34] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [35] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [36] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [37] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [38] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [39] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [40] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [41] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [42] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [43] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [44] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [45] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [46] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [47] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [48] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [49] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [50] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [51] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [52] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [53] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [54] org.apache.spark.sql.execution.exchange.ReuseExchange.apply
  [55] org.apache.spark.sql.execution.exchange.ReuseExchange.apply
  [56] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [57] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [58] scala.collection.LinearSeqOptimized$class.foldLeft
  [59] scala.collection.immutable.List.foldLeft
  [60] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [61] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [62] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [63] org.apache.spark.sql.Dataset.withAction
  [64] org.apache.spark.sql.Dataset.head
  [65] org.apache.spark.sql.Dataset.take
  [66] org.apache.spark.sql.Dataset.getRows
  [67] org.apache.spark.sql.Dataset.showString
  [68] sun.reflect.NativeMethodAccessorImpl.invoke0
  [69] sun.reflect.NativeMethodAccessorImpl.invoke
  [70] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [71] java.lang.reflect.Method.invoke
  [72] py4j.reflection.MethodInvoker.invoke
  [73] py4j.reflection.ReflectionEngine.invoke
  [74] py4j.Gateway.invoke
  [75] py4j.commands.AbstractCommand.invokeMethod
  [76] py4j.commands.CallCommand.execute
  [77] py4j.GatewayConnection.run
  [78] java.lang.Thread.run
  [79] [tid=16145]

--- 1551158165621337 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.ui.jobs.JobDataSource$$anonfun$25.apply
  [10] org.apache.spark.ui.jobs.JobDataSource$$anonfun$25.apply
  [11] scala.Option.map
  [12] org.apache.spark.ui.jobs.JobDataSource.org$apache$spark$ui$jobs$JobDataSource$$jobRow
  [13] org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply
  [14] org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply
  [15] scala.collection.TraversableLike$$anonfun$map$1.apply
  [16] scala.collection.TraversableLike$$anonfun$map$1.apply
  [17] scala.collection.immutable.List.foreach
  [18] scala.collection.generic.TraversableForwarder$class.foreach
  [19] scala.collection.mutable.ListBuffer.foreach
  [20] scala.collection.TraversableLike$class.map
  [21] scala.collection.AbstractTraversable.map
  [22] org.apache.spark.ui.jobs.JobDataSource.<init>
  [23] org.apache.spark.ui.jobs.JobPagedTable.<init>
  [24] org.apache.spark.ui.jobs.AllJobsPage.jobsTable
  [25] org.apache.spark.ui.jobs.AllJobsPage.render
  [26] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [27] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [28] org.apache.spark.ui.JettyUtils$$anon$3.doGet
  [29] javax.servlet.http.HttpServlet.service
  [30] javax.servlet.http.HttpServlet.service
  [31] org.spark_project.jetty.servlet.ServletHolder.handle
  [32] org.spark_project.jetty.servlet.ServletHandler.doHandle
  [33] org.spark_project.jetty.server.handler.ContextHandler.doHandle
  [34] org.spark_project.jetty.servlet.ServletHandler.doScope
  [35] org.spark_project.jetty.server.handler.ContextHandler.doScope
  [36] org.spark_project.jetty.server.handler.ScopedHandler.handle
  [37] org.spark_project.jetty.server.handler.gzip.GzipHandler.handle
  [38] org.spark_project.jetty.server.handler.ContextHandlerCollection.handle
  [39] org.spark_project.jetty.server.handler.HandlerWrapper.handle
  [40] org.spark_project.jetty.server.Server.handle
  [41] org.spark_project.jetty.server.HttpChannel.handle
  [42] org.spark_project.jetty.server.HttpConnection.onFillable
  [43] org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded
  [44] org.spark_project.jetty.io.FillInterest.fillable
  [45] org.spark_project.jetty.io.SelectChannelEndPoint$2.run
  [46] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [47] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [48] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run
  [49] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [50] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [51] java.lang.Thread.run
  [52] [tid=16206]

--- 1551158165644411 us
  [ 0] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 1] jvmti_GetClassMethods
  [ 2] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 3] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 4] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 5] InstanceKlass::initialize(Thread*)
  [ 6] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 7] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 8] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 9] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind
  [10] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer$lzycompute
  [11] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer
  [12] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [13] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [14] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [15] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [16] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [17] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [18] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [19] org.apache.spark.sql.Dataset.withAction
  [20] org.apache.spark.sql.Dataset.head
  [21] org.apache.spark.sql.Dataset.take
  [22] org.apache.spark.sql.Dataset.getRows
  [23] org.apache.spark.sql.Dataset.showString
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16145]

--- 1551158165760496 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.xml.factory.XMLLoader$class.adapter
  [10] scala.xml.XML$.adapter
  [11] scala.xml.factory.XMLLoader$class.loadXML
  [12] scala.xml.XML$.loadXML
  [13] scala.xml.factory.XMLLoader$class.loadString
  [14] scala.xml.XML$.loadString
  [15] org.apache.spark.ui.UIUtils$.makeDescription
  [16] org.apache.spark.ui.jobs.JobDataSource.org$apache$spark$ui$jobs$JobDataSource$$jobRow
  [17] org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply
  [18] org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply
  [19] scala.collection.TraversableLike$$anonfun$map$1.apply
  [20] scala.collection.TraversableLike$$anonfun$map$1.apply
  [21] scala.collection.immutable.List.foreach
  [22] scala.collection.generic.TraversableForwarder$class.foreach
  [23] scala.collection.mutable.ListBuffer.foreach
  [24] scala.collection.TraversableLike$class.map
  [25] scala.collection.AbstractTraversable.map
  [26] org.apache.spark.ui.jobs.JobDataSource.<init>
  [27] org.apache.spark.ui.jobs.JobPagedTable.<init>
  [28] org.apache.spark.ui.jobs.AllJobsPage.jobsTable
  [29] org.apache.spark.ui.jobs.AllJobsPage.render
  [30] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [31] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [32] org.apache.spark.ui.JettyUtils$$anon$3.doGet
  [33] javax.servlet.http.HttpServlet.service
  [34] javax.servlet.http.HttpServlet.service
  [35] org.spark_project.jetty.servlet.ServletHolder.handle
  [36] org.spark_project.jetty.servlet.ServletHandler.doHandle
  [37] org.spark_project.jetty.server.handler.ContextHandler.doHandle
  [38] org.spark_project.jetty.servlet.ServletHandler.doScope
  [39] org.spark_project.jetty.server.handler.ContextHandler.doScope
  [40] org.spark_project.jetty.server.handler.ScopedHandler.handle
  [41] org.spark_project.jetty.server.handler.gzip.GzipHandler.handle
  [42] org.spark_project.jetty.server.handler.ContextHandlerCollection.handle
  [43] org.spark_project.jetty.server.handler.HandlerWrapper.handle
  [44] org.spark_project.jetty.server.Server.handle
  [45] org.spark_project.jetty.server.HttpChannel.handle
  [46] org.spark_project.jetty.server.HttpConnection.onFillable
  [47] org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded
  [48] org.spark_project.jetty.io.FillInterest.fillable
  [49] org.spark_project.jetty.io.SelectChannelEndPoint$2.run
  [50] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [51] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [52] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run
  [53] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [54] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [55] java.lang.Thread.run
  [56] [tid=16206]

--- 1551158165810928 us
  [ 0] methodHandle::methodHandle(Method*)
  [ 1] jvmti_GetClassMethods
  [ 2] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 3] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 4] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 5] InstanceKlass::initialize(Thread*)
  [ 6] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 7] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 8] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 9] org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.variable
  [10] org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.isNullVariable
  [11] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [12] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [13] scala.Option.getOrElse
  [14] org.apache.spark.sql.catalyst.expressions.Expression.genCode
  [15] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [16] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [17] scala.collection.TraversableLike$$anonfun$map$1.apply
  [18] scala.collection.TraversableLike$$anonfun$map$1.apply
  [19] scala.collection.immutable.List.foreach
  [20] scala.collection.TraversableLike$class.map
  [21] scala.collection.immutable.List.map
  [22] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [23] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [25] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [26] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [27] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [28] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [29] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [30] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [31] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [32] org.apache.spark.sql.Dataset.withAction
  [33] org.apache.spark.sql.Dataset.head
  [34] org.apache.spark.sql.Dataset.take
  [35] org.apache.spark.sql.Dataset.getRows
  [36] org.apache.spark.sql.Dataset.showString
  [37] sun.reflect.NativeMethodAccessorImpl.invoke0
  [38] sun.reflect.NativeMethodAccessorImpl.invoke
  [39] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [40] java.lang.reflect.Method.invoke
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.AbstractCommand.invokeMethod
  [45] py4j.commands.CallCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16145]

--- 1551158165949519 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressions
  [10] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressionsWithCurrentInputs
  [11] org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow.doGenCode
  [12] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [13] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [14] scala.Option.getOrElse
  [15] org.apache.spark.sql.catalyst.expressions.Expression.genCode
  [16] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [17] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [18] scala.collection.TraversableLike$$anonfun$map$1.apply
  [19] scala.collection.TraversableLike$$anonfun$map$1.apply
  [20] scala.collection.immutable.List.foreach
  [21] scala.collection.TraversableLike$class.map
  [22] scala.collection.immutable.List.map
  [23] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [24] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [26] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [27] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [28] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [29] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [30] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [31] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [32] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [33] org.apache.spark.sql.Dataset.withAction
  [34] org.apache.spark.sql.Dataset.head
  [35] org.apache.spark.sql.Dataset.take
  [36] org.apache.spark.sql.Dataset.getRows
  [37] org.apache.spark.sql.Dataset.showString
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16145]

--- 1551158165959345 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.ui.SparkUI.getSparkUser
  [10] org.apache.spark.ui.jobs.JobsTab.getSparkUser
  [11] org.apache.spark.ui.jobs.AllJobsPage.render
  [12] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [13] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [14] org.apache.spark.ui.JettyUtils$$anon$3.doGet
  [15] javax.servlet.http.HttpServlet.service
  [16] javax.servlet.http.HttpServlet.service
  [17] org.spark_project.jetty.servlet.ServletHolder.handle
  [18] org.spark_project.jetty.servlet.ServletHandler.doHandle
  [19] org.spark_project.jetty.server.handler.ContextHandler.doHandle
  [20] org.spark_project.jetty.servlet.ServletHandler.doScope
  [21] org.spark_project.jetty.server.handler.ContextHandler.doScope
  [22] org.spark_project.jetty.server.handler.ScopedHandler.handle
  [23] org.spark_project.jetty.server.handler.gzip.GzipHandler.handle
  [24] org.spark_project.jetty.server.handler.ContextHandlerCollection.handle
  [25] org.spark_project.jetty.server.handler.HandlerWrapper.handle
  [26] org.spark_project.jetty.server.Server.handle
  [27] org.spark_project.jetty.server.HttpChannel.handle
  [28] org.spark_project.jetty.server.HttpConnection.onFillable
  [29] org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded
  [30] org.spark_project.jetty.io.FillInterest.fillable
  [31] org.spark_project.jetty.io.SelectChannelEndPoint$2.run
  [32] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [33] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [34] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run
  [35] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [36] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [37] java.lang.Thread.run
  [38] [tid=16206]

--- 1551158166089155 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [13] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [14] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [15] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [16] org.spark_project.guava.cache.LocalCache$Segment.get
  [17] org.spark_project.guava.cache.LocalCache.get
  [18] org.spark_project.guava.cache.LocalCache.getOrLoad
  [19] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [21] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [22] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [24] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [25] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [26] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [27] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [28] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [29] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [30] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [31] org.apache.spark.sql.Dataset.withAction
  [32] org.apache.spark.sql.Dataset.head
  [33] org.apache.spark.sql.Dataset.take
  [34] org.apache.spark.sql.Dataset.getRows
  [35] org.apache.spark.sql.Dataset.showString
  [36] sun.reflect.NativeMethodAccessorImpl.invoke0
  [37] sun.reflect.NativeMethodAccessorImpl.invoke
  [38] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [39] java.lang.reflect.Method.invoke
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.AbstractCommand.invokeMethod
  [44] py4j.commands.CallCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16145]

--- 1551158166189100 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.ClassBodyEvaluator.cook
  [10] org.codehaus.janino.SimpleCompiler.cook
  [11] org.codehaus.commons.compiler.Cookable.cook
  [12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [13] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [14] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [15] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [16] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [17] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [18] org.spark_project.guava.cache.LocalCache$Segment.get
  [19] org.spark_project.guava.cache.LocalCache.get
  [20] org.spark_project.guava.cache.LocalCache.getOrLoad
  [21] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [24] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [26] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [27] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [28] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [29] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [30] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [31] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [32] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [33] org.apache.spark.sql.Dataset.withAction
  [34] org.apache.spark.sql.Dataset.head
  [35] org.apache.spark.sql.Dataset.take
  [36] org.apache.spark.sql.Dataset.getRows
  [37] org.apache.spark.sql.Dataset.showString
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16145]

--- 1551158166290359 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] org.codehaus.janino.Java$Rvalue.setEnclosingScope
  [12] org.codehaus.janino.Java$ReturnStatement.<init>
  [13] org.codehaus.janino.Parser.parseReturnStatement
  [14] org.codehaus.janino.Parser.parseStatement
  [15] org.codehaus.janino.Parser.parseBlockStatement
  [16] org.codehaus.janino.Parser.parseBlockStatements
  [17] org.codehaus.janino.Parser.parseMethodDeclarationRest
  [18] org.codehaus.janino.Parser.parseClassBodyDeclaration
  [19] org.codehaus.janino.ClassBodyEvaluator.cook
  [20] org.codehaus.janino.SimpleCompiler.cook
  [21] org.codehaus.commons.compiler.Cookable.cook
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [25] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [26] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [27] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [28] org.spark_project.guava.cache.LocalCache$Segment.get
  [29] org.spark_project.guava.cache.LocalCache.get
  [30] org.spark_project.guava.cache.LocalCache.getOrLoad
  [31] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [36] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [39] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [40] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [41] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [42] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [43] org.apache.spark.sql.Dataset.withAction
  [44] org.apache.spark.sql.Dataset.head
  [45] org.apache.spark.sql.Dataset.take
  [46] org.apache.spark.sql.Dataset.getRows
  [47] org.apache.spark.sql.Dataset.showString
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16145]

--- 1551158166390141 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.util.AbstractTraverser.<init>
  [10] org.codehaus.janino.Java$Rvalue$1.<init>
  [11] org.codehaus.janino.Java$Rvalue.setEnclosingScope
  [12] org.codehaus.janino.Java$ReturnStatement.<init>
  [13] org.codehaus.janino.Parser.parseReturnStatement
  [14] org.codehaus.janino.Parser.parseStatement
  [15] org.codehaus.janino.Parser.parseBlockStatement
  [16] org.codehaus.janino.Parser.parseBlockStatements
  [17] org.codehaus.janino.Parser.parseMethodDeclarationRest
  [18] org.codehaus.janino.Parser.parseClassBodyDeclaration
  [19] org.codehaus.janino.ClassBodyEvaluator.cook
  [20] org.codehaus.janino.SimpleCompiler.cook
  [21] org.codehaus.commons.compiler.Cookable.cook
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [25] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [26] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [27] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [28] org.spark_project.guava.cache.LocalCache$Segment.get
  [29] org.spark_project.guava.cache.LocalCache.get
  [30] org.spark_project.guava.cache.LocalCache.getOrLoad
  [31] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [36] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [39] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [40] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [41] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [42] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [43] org.apache.spark.sql.Dataset.withAction
  [44] org.apache.spark.sql.Dataset.head
  [45] org.apache.spark.sql.Dataset.take
  [46] org.apache.spark.sql.Dataset.getRows
  [47] org.apache.spark.sql.Dataset.showString
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16145]

--- 1551158166494615 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.codehaus.janino.ClassLoaderIClassLoader.findIClass
  [11] org.codehaus.janino.IClassLoader.loadIClass
  [12] org.codehaus.janino.IClassLoader.requireType
  [13] org.codehaus.janino.IClassLoader.postConstruct
  [14] org.codehaus.janino.ClassLoaderIClassLoader.<init>
  [15] org.codehaus.janino.SimpleCompiler.cook
  [16] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [17] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [18] org.codehaus.janino.ClassBodyEvaluator.cook
  [19] org.codehaus.janino.SimpleCompiler.cook
  [20] org.codehaus.commons.compiler.Cookable.cook
  [21] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [25] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [26] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [27] org.spark_project.guava.cache.LocalCache$Segment.get
  [28] org.spark_project.guava.cache.LocalCache.get
  [29] org.spark_project.guava.cache.LocalCache.getOrLoad
  [30] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [31] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [32] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [35] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [36] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [39] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [40] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [41] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [42] org.apache.spark.sql.Dataset.withAction
  [43] org.apache.spark.sql.Dataset.head
  [44] org.apache.spark.sql.Dataset.take
  [45] org.apache.spark.sql.Dataset.getRows
  [46] org.apache.spark.sql.Dataset.showString
  [47] sun.reflect.NativeMethodAccessorImpl.invoke0
  [48] sun.reflect.NativeMethodAccessorImpl.invoke
  [49] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [50] java.lang.reflect.Method.invoke
  [51] py4j.reflection.MethodInvoker.invoke
  [52] py4j.reflection.ReflectionEngine.invoke
  [53] py4j.Gateway.invoke
  [54] py4j.commands.AbstractCommand.invokeMethod
  [55] py4j.commands.CallCommand.execute
  [56] py4j.GatewayConnection.run
  [57] java.lang.Thread.run
  [58] [tid=16145]

--- 1551158166594389 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.SimpleCompiler.cook
  [10] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [11] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [12] org.codehaus.janino.ClassBodyEvaluator.cook
  [13] org.codehaus.janino.SimpleCompiler.cook
  [14] org.codehaus.commons.compiler.Cookable.cook
  [15] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [16] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [18] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [19] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [20] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [21] org.spark_project.guava.cache.LocalCache$Segment.get
  [22] org.spark_project.guava.cache.LocalCache.get
  [23] org.spark_project.guava.cache.LocalCache.getOrLoad
  [24] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [26] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [27] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [28] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [29] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [30] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [31] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [32] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [33] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [34] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [35] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [36] org.apache.spark.sql.Dataset.withAction
  [37] org.apache.spark.sql.Dataset.head
  [38] org.apache.spark.sql.Dataset.take
  [39] org.apache.spark.sql.Dataset.getRows
  [40] org.apache.spark.sql.Dataset.showString
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16145]

--- 1551158166694144 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] java.util.regex.Pattern$CharPropertyNames.<clinit>
  [10] java.util.regex.Pattern.charPropertyNodeFor
  [11] java.util.regex.Pattern.family
  [12] java.util.regex.Pattern.sequence
  [13] java.util.regex.Pattern.expr
  [14] java.util.regex.Pattern.compile
  [15] java.util.regex.Pattern.<init>
  [16] java.util.regex.Pattern.compile
  [17] org.codehaus.janino.UnitCompiler.<clinit>
  [18] org.codehaus.janino.SimpleCompiler.cook
  [19] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [20] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [21] org.codehaus.janino.ClassBodyEvaluator.cook
  [22] org.codehaus.janino.SimpleCompiler.cook
  [23] org.codehaus.commons.compiler.Cookable.cook
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [26] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [27] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [28] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [29] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [30] org.spark_project.guava.cache.LocalCache$Segment.get
  [31] org.spark_project.guava.cache.LocalCache.get
  [32] org.spark_project.guava.cache.LocalCache.getOrLoad
  [33] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [34] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [35] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [36] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [37] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [38] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [39] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [40] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [41] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [42] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [43] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [44] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [45] org.apache.spark.sql.Dataset.withAction
  [46] org.apache.spark.sql.Dataset.head
  [47] org.apache.spark.sql.Dataset.take
  [48] org.apache.spark.sql.Dataset.getRows
  [49] org.apache.spark.sql.Dataset.showString
  [50] sun.reflect.NativeMethodAccessorImpl.invoke0
  [51] sun.reflect.NativeMethodAccessorImpl.invoke
  [52] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [53] java.lang.reflect.Method.invoke
  [54] py4j.reflection.MethodInvoker.invoke
  [55] py4j.reflection.ReflectionEngine.invoke
  [56] py4j.Gateway.invoke
  [57] py4j.commands.AbstractCommand.invokeMethod
  [58] py4j.commands.CallCommand.execute
  [59] py4j.GatewayConnection.run
  [60] java.lang.Thread.run
  [61] [tid=16145]

--- 1551158166795588 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.buildLocalVariableMap
  [10] org.codehaus.janino.UnitCompiler.buildLocalVariableMap
  [11] org.codehaus.janino.UnitCompiler.compile
  [12] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [13] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [14] org.codehaus.janino.UnitCompiler.compile2
  [15] org.codehaus.janino.UnitCompiler.compile2
  [16] org.codehaus.janino.UnitCompiler.access$400
  [17] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [18] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [19] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [20] org.codehaus.janino.UnitCompiler.compile
  [21] org.codehaus.janino.UnitCompiler.compileUnit
  [22] org.codehaus.janino.SimpleCompiler.cook
  [23] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [24] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [25] org.codehaus.janino.ClassBodyEvaluator.cook
  [26] org.codehaus.janino.SimpleCompiler.cook
  [27] org.codehaus.commons.compiler.Cookable.cook
  [28] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [29] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [30] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [31] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [32] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [33] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [34] org.spark_project.guava.cache.LocalCache$Segment.get
  [35] org.spark_project.guava.cache.LocalCache.get
  [36] org.spark_project.guava.cache.LocalCache.getOrLoad
  [37] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [38] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [39] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [40] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [41] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [42] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [43] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [44] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [45] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [46] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [47] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [48] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [49] org.apache.spark.sql.Dataset.withAction
  [50] org.apache.spark.sql.Dataset.head
  [51] org.apache.spark.sql.Dataset.take
  [52] org.apache.spark.sql.Dataset.getRows
  [53] org.apache.spark.sql.Dataset.showString
  [54] sun.reflect.NativeMethodAccessorImpl.invoke0
  [55] sun.reflect.NativeMethodAccessorImpl.invoke
  [56] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [57] java.lang.reflect.Method.invoke
  [58] py4j.reflection.MethodInvoker.invoke
  [59] py4j.reflection.ReflectionEngine.invoke
  [60] py4j.Gateway.invoke
  [61] py4j.commands.AbstractCommand.invokeMethod
  [62] py4j.commands.CallCommand.execute
  [63] py4j.GatewayConnection.run
  [64] java.lang.Thread.run
  [65] [tid=16145]

--- 1551158166897343 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.compile2
  [10] org.codehaus.janino.UnitCompiler.access$700
  [11] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [12] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [13] org.codehaus.janino.Java$MemberClassDeclaration.accept
  [14] org.codehaus.janino.UnitCompiler.compile
  [15] org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes
  [16] org.codehaus.janino.UnitCompiler.compile2
  [17] org.codehaus.janino.UnitCompiler.compile2
  [18] org.codehaus.janino.UnitCompiler.access$400
  [19] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [20] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [21] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [22] org.codehaus.janino.UnitCompiler.compile
  [23] org.codehaus.janino.UnitCompiler.compileUnit
  [24] org.codehaus.janino.SimpleCompiler.cook
  [25] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [26] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [27] org.codehaus.janino.ClassBodyEvaluator.cook
  [28] org.codehaus.janino.SimpleCompiler.cook
  [29] org.codehaus.commons.compiler.Cookable.cook
  [30] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [31] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [33] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [34] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [35] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [36] org.spark_project.guava.cache.LocalCache$Segment.get
  [37] org.spark_project.guava.cache.LocalCache.get
  [38] org.spark_project.guava.cache.LocalCache.getOrLoad
  [39] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [40] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [41] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [42] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [43] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [44] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [45] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [46] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [47] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [48] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [49] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [50] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [51] org.apache.spark.sql.Dataset.withAction
  [52] org.apache.spark.sql.Dataset.head
  [53] org.apache.spark.sql.Dataset.take
  [54] org.apache.spark.sql.Dataset.getRows
  [55] org.apache.spark.sql.Dataset.showString
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16145]

--- 1551158166997263 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.generatesCode
  [10] org.codehaus.janino.UnitCompiler.compile2
  [11] org.codehaus.janino.UnitCompiler.access$1900
  [12] org.codehaus.janino.UnitCompiler$6.visitIfStatement
  [13] org.codehaus.janino.UnitCompiler$6.visitIfStatement
  [14] org.codehaus.janino.Java$IfStatement.accept
  [15] org.codehaus.janino.UnitCompiler.compile
  [16] org.codehaus.janino.UnitCompiler.compileStatements
  [17] org.codehaus.janino.UnitCompiler.compile
  [18] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [19] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [20] org.codehaus.janino.UnitCompiler.compile2
  [21] org.codehaus.janino.UnitCompiler.compile2
  [22] org.codehaus.janino.UnitCompiler.access$700
  [23] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [24] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [25] org.codehaus.janino.Java$MemberClassDeclaration.accept
  [26] org.codehaus.janino.UnitCompiler.compile
  [27] org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes
  [28] org.codehaus.janino.UnitCompiler.compile2
  [29] org.codehaus.janino.UnitCompiler.compile2
  [30] org.codehaus.janino.UnitCompiler.access$400
  [31] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [32] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [33] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [34] org.codehaus.janino.UnitCompiler.compile
  [35] org.codehaus.janino.UnitCompiler.compileUnit
  [36] org.codehaus.janino.SimpleCompiler.cook
  [37] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [38] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [39] org.codehaus.janino.ClassBodyEvaluator.cook
  [40] org.codehaus.janino.SimpleCompiler.cook
  [41] org.codehaus.commons.compiler.Cookable.cook
  [42] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [43] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [44] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [45] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [46] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [47] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [48] org.spark_project.guava.cache.LocalCache$Segment.get
  [49] org.spark_project.guava.cache.LocalCache.get
  [50] org.spark_project.guava.cache.LocalCache.getOrLoad
  [51] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [52] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [53] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [54] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [55] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [56] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [57] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [58] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [59] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [60] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [61] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [62] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [63] org.apache.spark.sql.Dataset.withAction
  [64] org.apache.spark.sql.Dataset.head
  [65] org.apache.spark.sql.Dataset.take
  [66] org.apache.spark.sql.Dataset.getRows
  [67] org.apache.spark.sql.Dataset.showString
  [68] sun.reflect.NativeMethodAccessorImpl.invoke0
  [69] sun.reflect.NativeMethodAccessorImpl.invoke
  [70] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [71] java.lang.reflect.Method.invoke
  [72] py4j.reflection.MethodInvoker.invoke
  [73] py4j.reflection.ReflectionEngine.invoke
  [74] py4j.Gateway.invoke
  [75] py4j.commands.AbstractCommand.invokeMethod
  [76] py4j.commands.CallCommand.execute
  [77] py4j.GatewayConnection.run
  [78] java.lang.Thread.run
  [79] [tid=16145]

--- 1551158167099434 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.addMutableState$default$3
  [10] org.apache.spark.sql.execution.joins.SortMergeJoinExec.genScanner
  [11] org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce
  [12] org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply
  [13] org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply
  [14] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [15] org.apache.spark.rdd.RDDOperationScope$.withScope
  [16] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [17] org.apache.spark.sql.execution.CodegenSupport$class.produce
  [18] org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce
  [19] org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen
  [20] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [21] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [22] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [23] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [24] org.apache.spark.rdd.RDDOperationScope$.withScope
  [25] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [26] org.apache.spark.sql.execution.SparkPlan.execute
  [27] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [28] org.apache.spark.sql.execution.SparkPlan.executeTake
  [29] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [30] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [31] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [32] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [33] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [34] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [35] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [36] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [37] org.apache.spark.sql.Dataset.withAction
  [38] org.apache.spark.sql.Dataset.head
  [39] org.apache.spark.sql.Dataset.take
  [40] org.apache.spark.sql.Dataset.getRows
  [41] org.apache.spark.sql.Dataset.showString
  [42] sun.reflect.NativeMethodAccessorImpl.invoke0
  [43] sun.reflect.NativeMethodAccessorImpl.invoke
  [44] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [45] java.lang.reflect.Method.invoke
  [46] py4j.reflection.MethodInvoker.invoke
  [47] py4j.reflection.ReflectionEngine.invoke
  [48] py4j.Gateway.invoke
  [49] py4j.commands.AbstractCommand.invokeMethod
  [50] py4j.commands.CallCommand.execute
  [51] py4j.GatewayConnection.run
  [52] java.lang.Thread.run
  [53] [tid=16145]

--- 1551158167199454 us
  [ 0] ThreadProfilerMark::~ThreadProfilerMark()
  [ 1] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 2] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 3] JVM_FindClassFromBootLoader
  [ 4] Java_java_lang_ClassLoader_findBootstrapClass
  [ 5] java.lang.ClassLoader.findBootstrapClass
  [ 6] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 7] java.lang.ClassLoader.loadClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] sun.misc.Launcher$AppClassLoader.loadClass
  [10] java.lang.ClassLoader.loadClass
  [11] org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen
  [12] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [13] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [14] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [15] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [16] org.apache.spark.rdd.RDDOperationScope$.withScope
  [17] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [18] org.apache.spark.sql.execution.SparkPlan.execute
  [19] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [20] org.apache.spark.sql.execution.SparkPlan.executeTake
  [21] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [22] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [23] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [24] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [25] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [26] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [27] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [28] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [29] org.apache.spark.sql.Dataset.withAction
  [30] org.apache.spark.sql.Dataset.head
  [31] org.apache.spark.sql.Dataset.take
  [32] org.apache.spark.sql.Dataset.getRows
  [33] org.apache.spark.sql.Dataset.showString
  [34] sun.reflect.NativeMethodAccessorImpl.invoke0
  [35] sun.reflect.NativeMethodAccessorImpl.invoke
  [36] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [37] java.lang.reflect.Method.invoke
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.AbstractCommand.invokeMethod
  [42] py4j.commands.CallCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16145]

--- 1551158167300959 us
  [ 0] org.codehaus.janino.UnitCompiler.getDeclaringTypeBodyDeclaration
  [ 1] org.codehaus.janino.UnitCompiler.getDeclaringClass
  [ 2] org.codehaus.janino.UnitCompiler.compileGet2
  [ 3] org.codehaus.janino.UnitCompiler.access$10000
  [ 4] org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference
  [ 5] org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference
  [ 6] org.codehaus.janino.Java$QualifiedThisReference.accept
  [ 7] org.codehaus.janino.UnitCompiler.compileGet
  [ 8] org.codehaus.janino.UnitCompiler.compileGetValue
  [ 9] org.codehaus.janino.UnitCompiler.compileContext2
  [10] org.codehaus.janino.UnitCompiler.access$6900
  [11] org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess
  [12] org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess
  [13] org.codehaus.janino.Java$FieldAccess.accept
  [14] org.codehaus.janino.UnitCompiler$15.visitLvalue
  [15] org.codehaus.janino.UnitCompiler$15.visitLvalue
  [16] org.codehaus.janino.Java$Lvalue.accept
  [17] org.codehaus.janino.UnitCompiler.compileContext
  [18] org.codehaus.janino.UnitCompiler.compileGetValue
  [19] org.codehaus.janino.UnitCompiler.compileGet2
  [20] org.codehaus.janino.UnitCompiler.access$9100
  [21] org.codehaus.janino.UnitCompiler$16.visitMethodInvocation
  [22] org.codehaus.janino.UnitCompiler$16.visitMethodInvocation
  [23] org.codehaus.janino.Java$MethodInvocation.accept
  [24] org.codehaus.janino.UnitCompiler.compileGet
  [25] org.codehaus.janino.UnitCompiler.compileGetValue
  [26] org.codehaus.janino.UnitCompiler.compile2
  [27] org.codehaus.janino.UnitCompiler.access$6100
  [28] org.codehaus.janino.UnitCompiler$13.visitAssignment
  [29] org.codehaus.janino.UnitCompiler$13.visitAssignment
  [30] org.codehaus.janino.Java$Assignment.accept
  [31] org.codehaus.janino.UnitCompiler.compile
  [32] org.codehaus.janino.UnitCompiler.compile2
  [33] org.codehaus.janino.UnitCompiler.access$1800
  [34] org.codehaus.janino.UnitCompiler$6.visitExpressionStatement
  [35] org.codehaus.janino.UnitCompiler$6.visitExpressionStatement
  [36] org.codehaus.janino.Java$ExpressionStatement.accept
  [37] org.codehaus.janino.UnitCompiler.compile
  [38] org.codehaus.janino.UnitCompiler.compileStatements
  [39] org.codehaus.janino.UnitCompiler.compile2
  [40] org.codehaus.janino.UnitCompiler.access$1700
  [41] org.codehaus.janino.UnitCompiler$6.visitBlock
  [42] org.codehaus.janino.UnitCompiler$6.visitBlock
  [43] org.codehaus.janino.Java$Block.accept
  [44] org.codehaus.janino.UnitCompiler.compile
  [45] org.codehaus.janino.UnitCompiler.compile2
  [46] org.codehaus.janino.UnitCompiler.access$2200
  [47] org.codehaus.janino.UnitCompiler$6.visitWhileStatement
  [48] org.codehaus.janino.UnitCompiler$6.visitWhileStatement
  [49] org.codehaus.janino.Java$WhileStatement.accept
  [50] org.codehaus.janino.UnitCompiler.compile
  [51] org.codehaus.janino.UnitCompiler.compileStatements
  [52] org.codehaus.janino.UnitCompiler.compile
  [53] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [54] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [55] org.codehaus.janino.UnitCompiler.compile2
  [56] org.codehaus.janino.UnitCompiler.compile2
  [57] org.codehaus.janino.UnitCompiler.access$700
  [58] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [59] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [60] org.codehaus.janino.Java$MemberClassDeclaration.accept
  [61] org.codehaus.janino.UnitCompiler.compile
  [62] org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes
  [63] org.codehaus.janino.UnitCompiler.compile2
  [64] org.codehaus.janino.UnitCompiler.compile2
  [65] org.codehaus.janino.UnitCompiler.access$400
  [66] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [67] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [68] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [69] org.codehaus.janino.UnitCompiler.compile
  [70] org.codehaus.janino.UnitCompiler.compileUnit
  [71] org.codehaus.janino.SimpleCompiler.cook
  [72] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [73] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [74] org.codehaus.janino.ClassBodyEvaluator.cook
  [75] org.codehaus.janino.SimpleCompiler.cook
  [76] org.codehaus.commons.compiler.Cookable.cook
  [77] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [78] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [79] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [80] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [81] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [82] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [83] org.spark_project.guava.cache.LocalCache$Segment.get
  [84] org.spark_project.guava.cache.LocalCache.get
  [85] org.spark_project.guava.cache.LocalCache.getOrLoad
  [86] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [87] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [88] org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1
  [89] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [90] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [91] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [92] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [93] org.apache.spark.rdd.RDDOperationScope$.withScope
  [94] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [95] org.apache.spark.sql.execution.SparkPlan.execute
  [96] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [97] org.apache.spark.sql.execution.SparkPlan.executeTake
  [98] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [99] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [100] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [101] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [102] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [103] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [104] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [105] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [106] org.apache.spark.sql.Dataset.withAction
  [107] org.apache.spark.sql.Dataset.head
  [108] org.apache.spark.sql.Dataset.take
  [109] org.apache.spark.sql.Dataset.getRows
  [110] org.apache.spark.sql.Dataset.showString
  [111] sun.reflect.NativeMethodAccessorImpl.invoke0
  [112] sun.reflect.NativeMethodAccessorImpl.invoke
  [113] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [114] java.lang.reflect.Method.invoke
  [115] py4j.reflection.MethodInvoker.invoke
  [116] py4j.reflection.ReflectionEngine.invoke
  [117] py4j.Gateway.invoke
  [118] py4j.commands.AbstractCommand.invokeMethod
  [119] py4j.commands.CallCommand.execute
  [120] py4j.GatewayConnection.run
  [121] java.lang.Thread.run
  [122] [tid=16145]

--- 1551158167403058 us
  [ 0] __vdso_clock_gettime
  [ 1] __GI___clock_gettime
  [ 2] [unknown]
  [ 3] java.lang.Class.forName0
  [ 4] java.lang.Class.forName
  [ 5] org.codehaus.janino.ClassLoaderIClassLoader.findIClass
  [ 6] org.codehaus.janino.IClassLoader.loadIClass
  [ 7] org.codehaus.janino.IClassLoader.requireType
  [ 8] org.codehaus.janino.IClassLoader.postConstruct
  [ 9] org.codehaus.janino.ClassLoaderIClassLoader.<init>
  [10] org.codehaus.janino.SimpleCompiler.cook
  [11] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [12] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [13] org.codehaus.janino.ClassBodyEvaluator.cook
  [14] org.codehaus.janino.SimpleCompiler.cook
  [15] org.codehaus.commons.compiler.Cookable.cook
  [16] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [19] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [20] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [21] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [22] org.spark_project.guava.cache.LocalCache$Segment.get
  [23] org.spark_project.guava.cache.LocalCache.get
  [24] org.spark_project.guava.cache.LocalCache.getOrLoad
  [25] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [26] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [27] org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1
  [28] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [29] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [30] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [31] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [32] org.apache.spark.rdd.RDDOperationScope$.withScope
  [33] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [34] org.apache.spark.sql.execution.SparkPlan.execute
  [35] org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency
  [36] org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply
  [37] org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply
  [38] org.apache.spark.sql.catalyst.errors.package$.attachTree
  [39] org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute
  [40] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [41] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [42] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [43] org.apache.spark.rdd.RDDOperationScope$.withScope
  [44] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [45] org.apache.spark.sql.execution.SparkPlan.execute
  [46] org.apache.spark.sql.execution.InputAdapter.inputRDDs
  [47] org.apache.spark.sql.execution.SortExec.inputRDDs
  [48] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [49] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [50] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [51] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [52] org.apache.spark.rdd.RDDOperationScope$.withScope
  [53] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [54] org.apache.spark.sql.execution.SparkPlan.execute
  [55] org.apache.spark.sql.execution.InputAdapter.doExecute
  [56] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [57] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [58] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [59] org.apache.spark.rdd.RDDOperationScope$.withScope
  [60] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [61] org.apache.spark.sql.execution.SparkPlan.execute
  [62] org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs
  [63] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [64] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [65] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [66] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [67] org.apache.spark.rdd.RDDOperationScope$.withScope
  [68] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [69] org.apache.spark.sql.execution.SparkPlan.execute
  [70] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [71] org.apache.spark.sql.execution.SparkPlan.executeTake
  [72] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [73] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [74] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [75] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [76] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [77] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [78] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [79] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [80] org.apache.spark.sql.Dataset.withAction
  [81] org.apache.spark.sql.Dataset.head
  [82] org.apache.spark.sql.Dataset.take
  [83] org.apache.spark.sql.Dataset.getRows
  [84] org.apache.spark.sql.Dataset.showString
  [85] sun.reflect.NativeMethodAccessorImpl.invoke0
  [86] sun.reflect.NativeMethodAccessorImpl.invoke
  [87] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [88] java.lang.reflect.Method.invoke
  [89] py4j.reflection.MethodInvoker.invoke
  [90] py4j.reflection.ReflectionEngine.invoke
  [91] py4j.Gateway.invoke
  [92] py4j.commands.AbstractCommand.invokeMethod
  [93] py4j.commands.CallCommand.execute
  [94] py4j.GatewayConnection.run
  [95] java.lang.Thread.run
  [96] [tid=16145]

--- 1551158167503806 us
  [ 0] UTF8::unicode_length(char const*, int)
  [ 1] Symbol::as_unicode(int&) const
  [ 2] StringTable::intern(Symbol*, Thread*)
  [ 3] Reflection::new_method(methodHandle, bool, bool, Thread*)
  [ 4] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 5] JVM_GetClassDeclaredMethods
  [ 6] java.lang.Class.getDeclaredMethods0
  [ 7] java.lang.Class.privateGetDeclaredMethods
  [ 8] java.lang.Class.getDeclaredMethod
  [ 9] java.io.ObjectStreamClass.getPrivateMethod
  [10] java.io.ObjectStreamClass.access$1700
  [11] java.io.ObjectStreamClass$3.run
  [12] java.io.ObjectStreamClass$3.run
  [13] java.security.AccessController.doPrivileged
  [14] java.io.ObjectStreamClass.<init>
  [15] java.io.ObjectStreamClass.lookup
  [16] java.io.ObjectOutputStream.writeObject0
  [17] java.io.ObjectOutputStream.defaultWriteFields
  [18] java.io.ObjectOutputStream.writeSerialData
  [19] java.io.ObjectOutputStream.writeOrdinaryObject
  [20] java.io.ObjectOutputStream.writeObject0
  [21] java.io.ObjectOutputStream.writeObject
  [22] scala.collection.immutable.List$SerializationProxy.writeObject
  [23] sun.reflect.GeneratedMethodAccessor19.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] java.io.ObjectStreamClass.invokeWriteObject
  [27] java.io.ObjectOutputStream.writeSerialData
  [28] java.io.ObjectOutputStream.writeOrdinaryObject
  [29] java.io.ObjectOutputStream.writeObject0
  [30] java.io.ObjectOutputStream.defaultWriteFields
  [31] java.io.ObjectOutputStream.writeSerialData
  [32] java.io.ObjectOutputStream.writeOrdinaryObject
  [33] java.io.ObjectOutputStream.writeObject0
  [34] java.io.ObjectOutputStream.writeArray
  [35] java.io.ObjectOutputStream.writeObject0
  [36] java.io.ObjectOutputStream.defaultWriteFields
  [37] java.io.ObjectOutputStream.writeSerialData
  [38] java.io.ObjectOutputStream.writeOrdinaryObject
  [39] java.io.ObjectOutputStream.writeObject0
  [40] java.io.ObjectOutputStream.writeObject
  [41] org.apache.spark.serializer.JavaSerializationStream.writeObject
  [42] org.apache.spark.serializer.JavaSerializerInstance.serialize
  [43] org.apache.spark.util.ClosureCleaner$.ensureSerializable
  [44] org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean
  [45] org.apache.spark.util.ClosureCleaner$.clean
  [46] org.apache.spark.SparkContext.clean
  [47] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply
  [48] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply
  [49] org.apache.spark.rdd.RDDOperationScope$.withScope
  [50] org.apache.spark.rdd.RDDOperationScope$.withScope
  [51] org.apache.spark.rdd.RDD.withScope
  [52] org.apache.spark.rdd.RDD.mapPartitionsWithIndex
  [53] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [54] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [55] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [56] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [57] org.apache.spark.rdd.RDDOperationScope$.withScope
  [58] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [59] org.apache.spark.sql.execution.SparkPlan.execute
  [60] org.apache.spark.sql.execution.InputAdapter.doExecute
  [61] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [62] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [63] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [64] org.apache.spark.rdd.RDDOperationScope$.withScope
  [65] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [66] org.apache.spark.sql.execution.SparkPlan.execute
  [67] org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs
  [68] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [69] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [70] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [71] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [72] org.apache.spark.rdd.RDDOperationScope$.withScope
  [73] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [74] org.apache.spark.sql.execution.SparkPlan.execute
  [75] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [76] org.apache.spark.sql.execution.SparkPlan.executeTake
  [77] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [78] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [79] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [80] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [81] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [82] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [83] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [84] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [85] org.apache.spark.sql.Dataset.withAction
  [86] org.apache.spark.sql.Dataset.head
  [87] org.apache.spark.sql.Dataset.take
  [88] org.apache.spark.sql.Dataset.getRows
  [89] org.apache.spark.sql.Dataset.showString
  [90] sun.reflect.NativeMethodAccessorImpl.invoke0
  [91] sun.reflect.NativeMethodAccessorImpl.invoke
  [92] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [93] java.lang.reflect.Method.invoke
  [94] py4j.reflection.MethodInvoker.invoke
  [95] py4j.reflection.ReflectionEngine.invoke
  [96] py4j.Gateway.invoke
  [97] py4j.commands.AbstractCommand.invokeMethod
  [98] py4j.commands.CallCommand.execute
  [99] py4j.GatewayConnection.run
  [100] java.lang.Thread.run
  [101] [tid=16145]

--- 1551158167606968 us
  [ 0] MallocTracker::record_free(void*)
  [ 1] os::free(void*, MemoryType)
  [ 2] PlaceholderTable::find_and_remove(int, unsigned int, Symbol*, ClassLoaderData*, PlaceholderTable::classloadAction, Thread*)
  [ 3] SystemDictionary::resolve_super_or_fail(Symbol*, Symbol*, Handle, Handle, bool, Thread*)
  [ 4] ClassFileParser::parse_interfaces(int, Handle, Symbol*, bool*, Thread*)
  [ 5] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 6] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 7] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 8] JVM_DefineClassWithSource
  [ 9] Java_java_lang_ClassLoader_defineClass1
  [10] java.lang.ClassLoader.defineClass1
  [11] java.lang.ClassLoader.defineClass
  [12] java.security.SecureClassLoader.defineClass
  [13] java.net.URLClassLoader.defineClass
  [14] java.net.URLClassLoader.access$100
  [15] java.net.URLClassLoader$1.run
  [16] java.net.URLClassLoader$1.run
  [17] java.security.AccessController.doPrivileged
  [18] java.net.URLClassLoader.findClass
  [19] java.lang.ClassLoader.loadClass
  [20] sun.misc.Launcher$AppClassLoader.loadClass
  [21] java.lang.ClassLoader.loadClass
  [22] org.apache.spark.rdd.ZippedPartitionsPartition.<init>
  [23] org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$3.apply
  [24] org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$3.apply
  [25] scala.Array$.tabulate
  [26] org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions
  [27] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [28] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.rdd.RDD.partitions
  [31] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [32] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [33] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [34] scala.Option.getOrElse
  [35] org.apache.spark.rdd.RDD.partitions
  [36] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [37] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [38] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [39] scala.Option.getOrElse
  [40] org.apache.spark.rdd.RDD.partitions
  [41] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [42] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [43] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [44] scala.Option.getOrElse
  [45] org.apache.spark.rdd.RDD.partitions
  [46] org.apache.spark.sql.execution.SparkPlan.executeTake
  [47] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [48] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [49] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [50] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [51] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [52] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [53] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [54] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [55] org.apache.spark.sql.Dataset.withAction
  [56] org.apache.spark.sql.Dataset.head
  [57] org.apache.spark.sql.Dataset.take
  [58] org.apache.spark.sql.Dataset.getRows
  [59] org.apache.spark.sql.Dataset.showString
  [60] sun.reflect.NativeMethodAccessorImpl.invoke0
  [61] sun.reflect.NativeMethodAccessorImpl.invoke
  [62] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [63] java.lang.reflect.Method.invoke
  [64] py4j.reflection.MethodInvoker.invoke
  [65] py4j.reflection.ReflectionEngine.invoke
  [66] py4j.Gateway.invoke
  [67] py4j.commands.AbstractCommand.invokeMethod
  [68] py4j.commands.CallCommand.execute
  [69] py4j.GatewayConnection.run
  [70] java.lang.Thread.run
  [71] [tid=16145]

--- 1551158167636895 us
  [ 0] BacktraceBuilder::push(Method*, int, Thread*)
  [ 1] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle, Thread*)
  [ 2] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle)
  [ 3] JVM_FillInStackTrace
  [ 4] Java_java_lang_Throwable_fillInStackTrace
  [ 5] java.lang.Throwable.fillInStackTrace
  [ 6] java.lang.Throwable.fillInStackTrace
  [ 7] java.lang.Throwable.<init>
  [ 8] java.lang.Exception.<init>
  [ 9] java.lang.ReflectiveOperationException.<init>
  [10] java.lang.ClassNotFoundException.<init>
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] org.apache.spark.MapOutputTrackerMaster.getNumAvailableOutputs
  [17] org.apache.spark.scheduler.ShuffleMapStage.numAvailableOutputs
  [18] org.apache.spark.scheduler.ShuffleMapStage.isAvailable
  [19] org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply
  [20] org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply
  [21] scala.collection.immutable.List.foreach
  [22] org.apache.spark.scheduler.DAGScheduler.visit$1
  [23] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getMissingParentStages
  [24] org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$6.apply
  [25] org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$6.apply
  [26] org.apache.spark.internal.Logging$class.logInfo
  [27] org.apache.spark.scheduler.DAGScheduler.logInfo
  [28] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [29] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [30] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [31] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [32] org.apache.spark.util.EventLoop$$anon$1.run
  [33] [tid=16221]

--- 1551158172944031 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.ui.scope.RDDOperationGraph$.makeDotFile
  [10] org.apache.spark.ui.UIUtils$$anonfun$showDagViz$1.apply
  [11] org.apache.spark.ui.UIUtils$$anonfun$showDagViz$1.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.mutable.ResizableArray$class.foreach
  [15] scala.collection.mutable.ArrayBuffer.foreach
  [16] scala.collection.TraversableLike$class.map
  [17] scala.collection.AbstractTraversable.map
  [18] org.apache.spark.ui.UIUtils$.showDagViz
  [19] org.apache.spark.ui.UIUtils$.showDagVizForJob
  [20] org.apache.spark.ui.jobs.JobPage.render
  [21] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [22] org.apache.spark.ui.WebUI$$anonfun$2.apply
  [23] org.apache.spark.ui.JettyUtils$$anon$3.doGet
  [24] javax.servlet.http.HttpServlet.service
  [25] javax.servlet.http.HttpServlet.service
  [26] org.spark_project.jetty.servlet.ServletHolder.handle
  [27] org.spark_project.jetty.servlet.ServletHandler.doHandle
  [28] org.spark_project.jetty.server.handler.ContextHandler.doHandle
  [29] org.spark_project.jetty.servlet.ServletHandler.doScope
  [30] org.spark_project.jetty.server.handler.ContextHandler.doScope
  [31] org.spark_project.jetty.server.handler.ScopedHandler.handle
  [32] org.spark_project.jetty.server.handler.gzip.GzipHandler.handle
  [33] org.spark_project.jetty.server.handler.ContextHandlerCollection.handle
  [34] org.spark_project.jetty.server.handler.HandlerWrapper.handle
  [35] org.spark_project.jetty.server.Server.handle
  [36] org.spark_project.jetty.server.HttpChannel.handle
  [37] org.spark_project.jetty.server.HttpConnection.onFillable
  [38] org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded
  [39] org.spark_project.jetty.io.FillInterest.fillable
  [40] org.spark_project.jetty.io.SelectChannelEndPoint$2.run
  [41] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [42] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [43] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run
  [44] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [45] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [46] java.lang.Thread.run
  [47] [tid=16202]

--- 1551158173209055 us
  [ 0] deflate_slow
  [ 1] deflate
  [ 2] Java_java_util_zip_Deflater_deflateBytes
  [ 3] java.util.zip.Deflater.deflateBytes
  [ 4] java.util.zip.Deflater.deflate
  [ 5] org.spark_project.jetty.server.handler.gzip.GzipHttpOutputInterceptor$GzipBufferCB.process
  [ 6] org.spark_project.jetty.util.IteratingCallback.processing
  [ 7] org.spark_project.jetty.util.IteratingCallback.iterate
  [ 8] org.spark_project.jetty.server.handler.gzip.GzipHttpOutputInterceptor.gzip
  [ 9] org.spark_project.jetty.server.handler.gzip.GzipHttpOutputInterceptor.commit
  [10] org.spark_project.jetty.server.handler.gzip.GzipHttpOutputInterceptor.write
  [11] org.spark_project.jetty.server.HttpOutput.write
  [12] org.spark_project.jetty.server.HttpOutput$InputStreamWritingCB.process
  [13] org.spark_project.jetty.util.IteratingCallback.processing
  [14] org.spark_project.jetty.util.IteratingCallback.iterate
  [15] org.spark_project.jetty.server.HttpOutput.sendContent
  [16] org.spark_project.jetty.server.HttpOutput.sendContent
  [17] org.spark_project.jetty.servlet.DefaultServlet.sendData
  [18] org.spark_project.jetty.servlet.DefaultServlet.doGet
  [19] javax.servlet.http.HttpServlet.service
  [20] javax.servlet.http.HttpServlet.service
  [21] org.spark_project.jetty.servlet.ServletHolder.handle
  [22] org.spark_project.jetty.servlet.ServletHandler.doHandle
  [23] org.spark_project.jetty.server.handler.ContextHandler.doHandle
  [24] org.spark_project.jetty.servlet.ServletHandler.doScope
  [25] org.spark_project.jetty.server.handler.ContextHandler.doScope
  [26] org.spark_project.jetty.server.handler.ScopedHandler.handle
  [27] org.spark_project.jetty.server.handler.gzip.GzipHandler.handle
  [28] org.spark_project.jetty.server.handler.ContextHandlerCollection.handle
  [29] org.spark_project.jetty.server.handler.HandlerWrapper.handle
  [30] org.spark_project.jetty.server.Server.handle
  [31] org.spark_project.jetty.server.HttpChannel.handle
  [32] org.spark_project.jetty.server.HttpConnection.onFillable
  [33] org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded
  [34] org.spark_project.jetty.io.FillInterest.fillable
  [35] org.spark_project.jetty.io.SelectChannelEndPoint$2.run
  [36] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume
  [37] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume
  [38] org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run
  [39] org.spark_project.jetty.util.thread.QueuedThreadPool.runJob
  [40] org.spark_project.jetty.util.thread.QueuedThreadPool$2.run
  [41] java.lang.Thread.run
  [42] [tid=16206]

--- 1551158174609745 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto.internalGetFieldAccessorTable
  [13] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [14] com.google.protobuf.GeneratedMessage.getAllFields
  [15] com.google.protobuf.TextFormat$Printer.print
  [16] com.google.protobuf.TextFormat$Printer.access$400
  [17] com.google.protobuf.TextFormat.shortDebugString
  [18] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [19] com.sun.proxy.$Proxy13.renewLease
  [20] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [26] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [27] com.sun.proxy.$Proxy14.renewLease
  [28] org.apache.hadoop.hdfs.DFSClient.renewLease
  [29] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [30] org.apache.hadoop.hdfs.LeaseRenewer.run
  [31] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [32] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [33] java.lang.Thread.run
  [34] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16255]

--- 1551158174711568 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>
  [13] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto.internalGetFieldAccessorTable
  [14] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [15] com.google.protobuf.GeneratedMessage.getAllFields
  [16] com.google.protobuf.TextFormat$Printer.print
  [17] com.google.protobuf.TextFormat$Printer.access$400
  [18] com.google.protobuf.TextFormat.shortDebugString
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [20] com.sun.proxy.$Proxy13.renewLease
  [21] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [27] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [28] com.sun.proxy.$Proxy14.renewLease
  [29] org.apache.hadoop.hdfs.DFSClient.renewLease
  [30] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [31] org.apache.hadoop.hdfs.LeaseRenewer.run
  [32] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [33] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [34] java.lang.Thread.run
  [35] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16255]

--- 1551158174794690 us
  [ 0] SpinPause
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16067]

--- 1551158174798100 us
  [ 0] StringTable::unlink_or_oops_do(BoolObjectClosure*, OopClosure*, int*, int*)
  [ 1] PSScavenge::invoke_no_policy()
  [ 2] PSScavenge::invoke()
  [ 3] ParallelScavengeHeap::failed_mem_allocate(unsigned long)
  [ 4] VM_ParallelGCFailedAllocation::doit()
  [ 5] VM_Operation::evaluate()
  [ 6] VMThread::evaluate_operation(VM_Operation*) [clone .constprop.44]
  [ 7] VMThread::loop()
  [ 8] VMThread::run()
  [ 9] java_start(Thread*)
  [10] start_thread
  [11] [tid=16092]

--- 1551158174829021 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredMethods
  [ 9] java.lang.Class.getDeclaredMethods0
  [10] java.lang.Class.privateGetDeclaredMethods
  [11] java.lang.Class.privateGetMethodRecursive
  [12] java.lang.Class.getMethod0
  [13] java.lang.Class.getMethod
  [14] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.getReturnProtoType
  [15] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [16] com.sun.proxy.$Proxy13.renewLease
  [17] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [23] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [24] com.sun.proxy.$Proxy14.renewLease
  [25] org.apache.hadoop.hdfs.DFSClient.renewLease
  [26] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [27] org.apache.hadoop.hdfs.LeaseRenewer.run
  [28] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [29] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [30] java.lang.Thread.run
  [31] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16255]

--- 1551158178046322 us
  [ 0] VirtualSpace::reserved_size() const
  [ 1] AdvancedThresholdPolicy::loop_predicate(int, int, CompLevel)
  [ 2] AdvancedThresholdPolicy::method_back_branch_event(methodHandle, methodHandle, int, CompLevel, nmethod*, JavaThread*)
  [ 3] SimpleThresholdPolicy::event(methodHandle, methodHandle, int, int, CompLevel, nmethod*, JavaThread*)
  [ 4] InterpreterRuntime::frequency_counter_overflow_inner(JavaThread*, unsigned char*)
  [ 5] InterpreterRuntime::frequency_counter_overflow(JavaThread*, unsigned char*)
  [ 6] scala.collection.mutable.HashTable$class.resize
  [ 7] scala.collection.mutable.HashTable$class.scala$collection$mutable$HashTable$$addEntry0
  [ 8] scala.collection.mutable.HashTable$class.findOrAddEntry
  [ 9] scala.collection.mutable.LinkedHashMap.findOrAddEntry
  [10] scala.collection.mutable.LinkedHashMap.put
  [11] scala.collection.mutable.LinkedHashMap.$plus$eq
  [12] scala.collection.mutable.LinkedHashMap.$plus$eq
  [13] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [14] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [15] scala.collection.IndexedSeqOptimized$class.foreach
  [16] scala.collection.mutable.WrappedArray.foreach
  [17] scala.collection.generic.Growable$class.$plus$plus$eq
  [18] scala.collection.mutable.AbstractMap.$plus$plus$eq
  [19] scala.collection.generic.GenMapFactory.apply
  [20] org.apache.spark.executor.TaskMetrics.nameToAccums$lzycompute
  [21] org.apache.spark.executor.TaskMetrics.nameToAccums
  [22] org.apache.spark.executor.TaskMetrics$$anonfun$fromAccumulatorInfos$2.apply
  [23] org.apache.spark.executor.TaskMetrics$$anonfun$fromAccumulatorInfos$2.apply
  [24] scala.collection.Iterator$class.foreach
  [25] scala.collection.AbstractIterator.foreach
  [26] scala.collection.IterableLike$class.foreach
  [27] scala.collection.AbstractIterable.foreach
  [28] org.apache.spark.executor.TaskMetrics$.fromAccumulatorInfos
  [29] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1$$anonfun$apply$19.apply
  [30] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1$$anonfun$apply$19.apply
  [31] scala.Option.foreach
  [32] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1.apply
  [33] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1.apply
  [34] scala.collection.IndexedSeqOptimized$class.foreach
  [35] scala.collection.mutable.WrappedArray.foreach
  [36] org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate
  [37] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [38] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [39] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [40] org.apache.spark.util.ListenerBus$class.postToAll
  [41] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [42] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [43] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [44] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [45] scala.util.DynamicVariable.withValue
  [46] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [47] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [48] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [49] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [50] [tid=16265]

--- 1551158201674059 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] io.netty.buffer.AbstractByteBufAllocator.toLeakAwareBuffer
  [11] io.netty.buffer.PooledByteBufAllocator.newHeapBuffer
  [12] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [13] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [14] org.apache.spark.network.protocol.MessageEncoder.encode
  [15] org.apache.spark.network.protocol.MessageEncoder.encode
  [16] io.netty.handler.codec.MessageToMessageEncoder.write
  [17] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [18] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [19] io.netty.channel.AbstractChannelHandlerContext.write
  [20] io.netty.channel.AbstractChannelHandlerContext.write
  [21] io.netty.handler.timeout.IdleStateHandler.write
  [22] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [23] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [24] io.netty.channel.AbstractChannelHandlerContext.access$1900
  [25] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write
  [26] io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write
  [27] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run
  [28] io.netty.util.concurrent.AbstractEventExecutor.safeExecute
  [29] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks
  [30] io.netty.channel.nio.NioEventLoop.run
  [31] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [32] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [33] java.lang.Thread.run
  [34] [rpc-server-3-5 tid=16251]

--- 1551158317649163 us
  [ 0] __audit_syscall_entry_[k]
  [ 1] syscall_trace_enter_[k]
  [ 2] do_syscall_64_[k]
  [ 3] entry_SYSCALL_64_after_hwframe_[k]
  [ 4] gettimeofday
  [ 5] [unknown]
  [ 6] sun.nio.ch.EPollArrayWrapper.epollWait
  [ 7] sun.nio.ch.EPollArrayWrapper.poll
  [ 8] sun.nio.ch.EPollSelectorImpl.doSelect
  [ 9] sun.nio.ch.SelectorImpl.lockAndDoSelect
  [10] sun.nio.ch.SelectorImpl.select
  [11] org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select
  [12] org.apache.hadoop.net.SocketIOWithTimeout.doIO
  [13] org.apache.hadoop.net.SocketInputStream.read
  [14] org.apache.hadoop.net.SocketInputStream.read
  [15] org.apache.hadoop.net.SocketInputStream.read
  [16] java.io.FilterInputStream.read
  [17] java.io.FilterInputStream.read
  [18] org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed
  [19] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields
  [20] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [21] [tid=16272]

--- 1551158349390149 us
  [ 0] java.util.regex.Pattern$Branch.match
  [ 1] java.util.regex.Pattern$BmpCharProperty.match
  [ 2] java.util.regex.Pattern$Start.match
  [ 3] java.util.regex.Matcher.search
  [ 4] java.util.regex.Matcher.find
  [ 5] java.util.Formatter.parse
  [ 6] java.util.Formatter.format
  [ 7] java.util.Formatter.format
  [ 8] java.lang.String.format
  [ 9] scala.collection.immutable.StringLike$class.format
  [10] scala.collection.immutable.StringOps.format
  [11] org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$9.apply
  [12] org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$9.apply
  [13] org.apache.spark.internal.Logging$class.logDebug
  [14] org.apache.spark.scheduler.TaskSchedulerImpl.logDebug
  [15] org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply
  [16] org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply
  [17] scala.collection.mutable.ResizableArray$class.foreach
  [18] scala.collection.mutable.ArrayBuffer.foreach
  [19] org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers
  [20] org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers
  [21] org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse
  [22] org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp
  [23] org.apache.spark.rpc.netty.Inbox.safelyCall
  [24] org.apache.spark.rpc.netty.Inbox.process
  [25] org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run
  [26] java.util.concurrent.ThreadPoolExecutor.runWorker
  [27] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [28] java.lang.Thread.run
  [29] [tid=16166]

--- 1551158388040508 us
  [ 0] java.nio.Buffer.checkBounds
  [ 1] java.nio.HeapByteBuffer.get
  [ 2] org.apache.spark.util.ByteBufferInputStream.read
  [ 3] java.io.ObjectInputStream$PeekInputStream.read
  [ 4] java.io.ObjectInputStream$BlockDataInputStream.read
  [ 5] java.io.ObjectInputStream$BlockDataInputStream.readFully
  [ 6] java.io.ObjectInputStream.defaultReadFields
  [ 7] java.io.ObjectInputStream.readSerialData
  [ 8] java.io.ObjectInputStream.readOrdinaryObject
  [ 9] java.io.ObjectInputStream.readObject0
  [10] java.io.ObjectInputStream.defaultReadFields
  [11] java.io.ObjectInputStream.defaultReadObject
  [12] org.apache.spark.util.AccumulatorV2$$anonfun$readObject$1.apply$mcV$sp
  [13] org.apache.spark.util.AccumulatorV2$$anonfun$readObject$1.apply
  [14] org.apache.spark.util.AccumulatorV2$$anonfun$readObject$1.apply
  [15] org.apache.spark.util.Utils$.tryOrIOException
  [16] org.apache.spark.util.AccumulatorV2.readObject
  [17] sun.reflect.GeneratedMethodAccessor8.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] java.io.ObjectStreamClass.invokeReadObject
  [21] java.io.ObjectInputStream.readSerialData
  [22] java.io.ObjectInputStream.readOrdinaryObject
  [23] java.io.ObjectInputStream.readObject0
  [24] java.io.ObjectInputStream.readArray
  [25] java.io.ObjectInputStream.readObject0
  [26] java.io.ObjectInputStream.defaultReadFields
  [27] java.io.ObjectInputStream.readSerialData
  [28] java.io.ObjectInputStream.readOrdinaryObject
  [29] java.io.ObjectInputStream.readObject0
  [30] java.io.ObjectInputStream.defaultReadFields
  [31] java.io.ObjectInputStream.readSerialData
  [32] java.io.ObjectInputStream.readOrdinaryObject
  [33] java.io.ObjectInputStream.readObject0
  [34] java.io.ObjectInputStream.readArray
  [35] java.io.ObjectInputStream.readObject0
  [36] java.io.ObjectInputStream.defaultReadFields
  [37] java.io.ObjectInputStream.readSerialData
  [38] java.io.ObjectInputStream.readOrdinaryObject
  [39] java.io.ObjectInputStream.readObject0
  [40] java.io.ObjectInputStream.readObject
  [41] org.apache.spark.serializer.JavaDeserializationStream.readObject
  [42] org.apache.spark.serializer.JavaSerializerInstance.deserialize
  [43] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply
  [44] scala.util.DynamicVariable.withValue
  [45] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [46] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply
  [47] scala.util.DynamicVariable.withValue
  [48] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [49] org.apache.spark.rpc.netty.RequestMessage$.apply
  [50] org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive
  [51] org.apache.spark.rpc.netty.NettyRpcHandler.receive
  [52] org.apache.spark.network.server.TransportRequestHandler.processRpcRequest
  [53] org.apache.spark.network.server.TransportRequestHandler.handle
  [54] org.apache.spark.network.server.TransportChannelHandler.channelRead
  [55] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [56] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [57] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [58] io.netty.handler.timeout.IdleStateHandler.channelRead
  [59] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [60] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [61] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [62] io.netty.handler.codec.MessageToMessageDecoder.channelRead
  [63] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [64] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [65] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [66] org.apache.spark.network.util.TransportFrameDecoder.channelRead
  [67] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [68] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [69] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [70] io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead
  [71] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [72] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [73] io.netty.channel.DefaultChannelPipeline.fireChannelRead
  [74] io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read
  [75] io.netty.channel.nio.NioEventLoop.processSelectedKey
  [76] io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized
  [77] io.netty.channel.nio.NioEventLoop.processSelectedKeys
  [78] io.netty.channel.nio.NioEventLoop.run
  [79] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [80] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [81] java.lang.Thread.run
  [82] [rpc-server-3-4 tid=16250]

--- 1551158448008590 us
  [ 0] scala.runtime.BoxesRunTime.equalsNumObject
  [ 1] scala.collection.mutable.HashTable$class.elemEquals
  [ 2] scala.collection.mutable.HashMap.elemEquals
  [ 3] scala.collection.mutable.HashTable$class.scala$collection$mutable$HashTable$$findEntry0
  [ 4] scala.collection.mutable.HashTable$class.findEntry
  [ 5] scala.collection.mutable.HashMap.findEntry
  [ 6] scala.collection.mutable.HashMap.get
  [ 7] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1.apply
  [ 8] org.apache.spark.status.AppStatusListener$$anonfun$onExecutorMetricsUpdate$1.apply
  [ 9] scala.collection.IndexedSeqOptimized$class.foreach
  [10] scala.collection.mutable.WrappedArray.foreach
  [11] org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate
  [12] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [13] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [14] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [15] org.apache.spark.util.ListenerBus$class.postToAll
  [16] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [17] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [18] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [19] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [20] scala.util.DynamicVariable.withValue
  [21] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [22] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [23] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [24] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [25] [tid=16265]

--- 1551158451652519 us
  [ 0] jshort_disjoint_arraycopy
  [ 1] java.lang.AbstractStringBuilder.append
  [ 2] java.lang.StringBuffer.append
  [ 3] org.slf4j.helpers.MessageFormatter.safeObjectAppend
  [ 4] org.slf4j.helpers.MessageFormatter.deeplyAppendParameter
  [ 5] org.slf4j.helpers.MessageFormatter.arrayFormat
  [ 6] org.slf4j.helpers.MessageFormatter.format
  [ 7] org.slf4j.impl.Log4jLoggerAdapter.trace
  [ 8] org.apache.spark.network.server.TransportRequestHandler.lambda$respond$2
  [ 9] org.apache.spark.network.server.TransportRequestHandler$$Lambda$17.3961551.operationComplete
  [10] io.netty.util.concurrent.DefaultPromise.notifyListener0
  [11] io.netty.util.concurrent.DefaultPromise.notifyListeners0
  [12] io.netty.util.concurrent.DefaultPromise.notifyListenersNow
  [13] io.netty.util.concurrent.DefaultPromise.notifyListeners
  [14] io.netty.util.concurrent.DefaultPromise.trySuccess
  [15] io.netty.util.internal.PromiseNotificationUtil.trySuccess
  [16] io.netty.channel.ChannelOutboundBuffer.safeSuccess
  [17] io.netty.channel.ChannelOutboundBuffer.remove
  [18] io.netty.channel.nio.AbstractNioByteChannel.doWrite
  [19] io.netty.channel.socket.nio.NioSocketChannel.doWrite
  [20] io.netty.channel.AbstractChannel$AbstractUnsafe.flush0
  [21] io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0
  [22] io.netty.channel.AbstractChannel$AbstractUnsafe.flush
  [23] io.netty.channel.DefaultChannelPipeline$HeadContext.flush
  [24] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [25] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [26] io.netty.channel.AbstractChannelHandlerContext.flush
  [27] io.netty.channel.ChannelOutboundHandlerAdapter.flush
  [28] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [29] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [30] io.netty.channel.AbstractChannelHandlerContext.flush
  [31] io.netty.channel.ChannelDuplexHandler.flush
  [32] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [33] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [34] io.netty.channel.AbstractChannelHandlerContext.access$1500
  [35] io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write
  [36] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run
  [37] io.netty.util.concurrent.AbstractEventExecutor.safeExecute
  [38] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks
  [39] io.netty.channel.nio.NioEventLoop.run
  [40] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [41] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [42] java.lang.Thread.run
  [43] [rpc-server-3-5 tid=16251]

--- 1551158465611311 us
  [ 0] java.math.MutableBigInteger.divide
  [ 1] java.math.BigInteger.smallToString
  [ 2] java.math.BigInteger.toString
  [ 3] java.math.BigInteger.toString
  [ 4] com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.writeNumber
  [ 5] org.json4s.jackson.JValueSerializer.serialize
  [ 6] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [ 7] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [ 8] scala.collection.immutable.List.foreach
  [ 9] org.json4s.jackson.JValueSerializer.serialize
  [10] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [11] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [12] scala.collection.immutable.List.foreach
  [13] org.json4s.jackson.JValueSerializer.serialize
  [14] org.json4s.jackson.JValueSerializer.serialize
  [15] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [16] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [17] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [18] org.json4s.jackson.JsonMethods$class.compact
  [19] org.json4s.jackson.JsonMethods$.compact
  [20] org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$1.apply
  [21] org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$1.apply
  [22] scala.Option.foreach
  [23] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [24] org.apache.spark.scheduler.EventLoggingListener.onTaskEnd
  [25] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [26] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [27] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [28] org.apache.spark.util.ListenerBus$class.postToAll
  [29] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [30] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [31] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [32] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [33] scala.util.DynamicVariable.withValue
  [34] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [36] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [37] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [38] [tid=16267]

--- 1551158541930702 us
  [ 0] klassVtable::update_inherited_vtable(InstanceKlass*, methodHandle, int, int, bool, Thread*)
  [ 1] klassVtable::initialize_vtable(bool, Thread*)
  [ 2] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 3] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 4] InstanceKlass::initialize(Thread*)
  [ 5] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 6] org.apache.spark.sql.execution.CacheManager.clearCache
  [ 7] org.apache.spark.sql.internal.CatalogImpl.clearCache
  [ 8] sun.reflect.NativeMethodAccessorImpl.invoke0
  [ 9] sun.reflect.NativeMethodAccessorImpl.invoke
  [10] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [11] java.lang.reflect.Method.invoke
  [12] py4j.reflection.MethodInvoker.invoke
  [13] py4j.reflection.ReflectionEngine.invoke
  [14] py4j.Gateway.invoke
  [15] py4j.commands.AbstractCommand.invokeMethod
  [16] py4j.commands.CallCommand.execute
  [17] py4j.GatewayConnection.run
  [18] java.lang.Thread.run
  [19] [tid=16145]

--- 1551158542092283 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto.newBuilder
  [13] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rename
  [14] sun.reflect.NativeMethodAccessorImpl.invoke0
  [15] sun.reflect.NativeMethodAccessorImpl.invoke
  [16] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [17] java.lang.reflect.Method.invoke
  [18] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [19] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [20] com.sun.proxy.$Proxy14.rename
  [21] org.apache.hadoop.hdfs.DFSClient.rename
  [22] org.apache.hadoop.hdfs.DistributedFileSystem.rename
  [23] org.apache.spark.scheduler.EventLoggingListener.stop
  [24] org.apache.spark.SparkContext$$anonfun$stop$8$$anonfun$apply$mcV$sp$6.apply
  [25] org.apache.spark.SparkContext$$anonfun$stop$8$$anonfun$apply$mcV$sp$6.apply
  [26] scala.Option.foreach
  [27] org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp
  [28] org.apache.spark.util.Utils$.tryLogNonFatalError
  [29] org.apache.spark.SparkContext.stop
  [30] org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp
  [31] org.apache.spark.util.SparkShutdownHook.run
  [32] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [33] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [34] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [35] org.apache.spark.util.Utils$.logUncaughtExceptions
  [36] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp
  [37] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [38] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [39] scala.util.Try$.apply
  [40] org.apache.spark.util.SparkShutdownHookManager.runAll
  [41] org.apache.spark.util.SparkShutdownHookManager$$anon$2.run
  [42] org.apache.hadoop.util.ShutdownHookManager$1.run
  [43] [tid=16580]

--- 1551158542209363 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.scheduler.OutputCommitCoordinator$$anonfun$stop$1.apply
  [12] org.apache.spark.scheduler.OutputCommitCoordinator$$anonfun$stop$1.apply
  [13] scala.Option.foreach
  [14] org.apache.spark.scheduler.OutputCommitCoordinator.stop
  [15] org.apache.spark.SparkEnv.stop
  [16] org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp
  [17] org.apache.spark.util.Utils$.tryLogNonFatalError
  [18] org.apache.spark.SparkContext.stop
  [19] org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp
  [20] org.apache.spark.util.SparkShutdownHook.run
  [21] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [22] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [23] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [24] org.apache.spark.util.Utils$.logUncaughtExceptions
  [25] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp
  [26] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [27] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [28] scala.util.Try$.apply
  [29] org.apache.spark.util.SparkShutdownHookManager.runAll
  [30] org.apache.spark.util.SparkShutdownHookManager$$anon$2.run
  [31] org.apache.hadoop.util.ShutdownHookManager$1.run
  [32] [tid=16580]

--- 1551158542898452 us
  [ 0] [DestroyJavaVM tid=16065]

--- 1551158542998153 us
  [ 0] [DestroyJavaVM tid=16065]

--- 1551158543097865 us
  [ 0] [DestroyJavaVM tid=16065]

--- 1551158543197578 us
  [ 0] [DestroyJavaVM tid=16065]

--- 1551158543297303 us
  [ 0] [DestroyJavaVM tid=16065]

