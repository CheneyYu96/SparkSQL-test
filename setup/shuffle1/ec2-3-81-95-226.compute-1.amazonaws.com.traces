--- 1551158138198545 us
  [ 0] java.util.zip.ZipFile.getEntry
  [ 1] java.util.zip.ZipFile.getEntry
  [ 2] java.util.jar.JarFile.getEntry
  [ 3] java.util.jar.JarFile.getJarEntry
  [ 4] sun.misc.URLClassPath$JarLoader.getResource
  [ 5] sun.misc.URLClassPath.getResource
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] java.lang.ClassLoader.defineClass1
  [14] java.lang.ClassLoader.defineClass
  [15] java.security.SecureClassLoader.defineClass
  [16] java.net.URLClassLoader.defineClass
  [17] java.net.URLClassLoader.access$100
  [18] java.net.URLClassLoader$1.run
  [19] java.net.URLClassLoader$1.run
  [20] java.security.AccessController.doPrivileged
  [21] java.net.URLClassLoader.findClass
  [22] java.lang.ClassLoader.loadClass
  [23] sun.misc.Launcher$AppClassLoader.loadClass
  [24] java.lang.ClassLoader.loadClass
  [25] java.lang.ClassLoader.defineClass1
  [26] java.lang.ClassLoader.defineClass
  [27] java.security.SecureClassLoader.defineClass
  [28] java.net.URLClassLoader.defineClass
  [29] java.net.URLClassLoader.access$100
  [30] java.net.URLClassLoader$1.run
  [31] java.net.URLClassLoader$1.run
  [32] java.security.AccessController.doPrivileged
  [33] java.net.URLClassLoader.findClass
  [34] java.lang.ClassLoader.loadClass
  [35] sun.misc.Launcher$AppClassLoader.loadClass
  [36] java.lang.ClassLoader.loadClass
  [37] java.lang.ClassLoader.defineClass1
  [38] java.lang.ClassLoader.defineClass
  [39] java.security.SecureClassLoader.defineClass
  [40] java.net.URLClassLoader.defineClass
  [41] java.net.URLClassLoader.access$100
  [42] java.net.URLClassLoader$1.run
  [43] java.net.URLClassLoader$1.run
  [44] java.security.AccessController.doPrivileged
  [45] java.net.URLClassLoader.findClass
  [46] java.lang.ClassLoader.loadClass
  [47] sun.misc.Launcher$AppClassLoader.loadClass
  [48] java.lang.ClassLoader.loadClass
  [49] java.lang.ClassLoader.defineClass1
  [50] java.lang.ClassLoader.defineClass
  [51] java.security.SecureClassLoader.defineClass
  [52] java.net.URLClassLoader.defineClass
  [53] java.net.URLClassLoader.access$100
  [54] java.net.URLClassLoader$1.run
  [55] java.net.URLClassLoader$1.run
  [56] java.security.AccessController.doPrivileged
  [57] java.net.URLClassLoader.findClass
  [58] java.lang.ClassLoader.loadClass
  [59] sun.misc.Launcher$AppClassLoader.loadClass
  [60] java.lang.ClassLoader.loadClass
  [61] java.lang.Class.getDeclaredMethods0
  [62] java.lang.Class.privateGetDeclaredMethods
  [63] java.lang.Class.privateGetMethodRecursive
  [64] java.lang.Class.getMethod0
  [65] java.lang.Class.getMethod
  [66] sun.launcher.LauncherHelper.validateMainClass
  [67] sun.launcher.LauncherHelper.checkAndLoadMain
  [68] [DestroyJavaVM tid=16064]

--- 1551158138204490 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] java.lang.ClassLoader.defineClass1
  [16] java.lang.ClassLoader.defineClass
  [17] java.security.SecureClassLoader.defineClass
  [18] java.net.URLClassLoader.defineClass
  [19] java.net.URLClassLoader.access$100
  [20] java.net.URLClassLoader$1.run
  [21] java.net.URLClassLoader$1.run
  [22] java.security.AccessController.doPrivileged
  [23] java.net.URLClassLoader.findClass
  [24] java.lang.ClassLoader.loadClass
  [25] sun.misc.Launcher$AppClassLoader.loadClass
  [26] java.lang.ClassLoader.loadClass
  [27] java.lang.ClassLoader.defineClass1
  [28] java.lang.ClassLoader.defineClass
  [29] java.security.SecureClassLoader.defineClass
  [30] java.net.URLClassLoader.defineClass
  [31] java.net.URLClassLoader.access$100
  [32] java.net.URLClassLoader$1.run
  [33] java.net.URLClassLoader$1.run
  [34] java.security.AccessController.doPrivileged
  [35] java.net.URLClassLoader.findClass
  [36] java.lang.ClassLoader.loadClass
  [37] sun.misc.Launcher$AppClassLoader.loadClass
  [38] java.lang.ClassLoader.loadClass
  [39] java.lang.Class.getDeclaredMethods0
  [40] java.lang.Class.privateGetDeclaredMethods
  [41] java.lang.Class.privateGetMethodRecursive
  [42] java.lang.Class.getMethod0
  [43] java.lang.Class.getMethod
  [44] sun.launcher.LauncherHelper.validateMainClass
  [45] sun.launcher.LauncherHelper.checkAndLoadMain
  [46] [DestroyJavaVM tid=16064]

--- 1551158138302512 us
  [ 0] scala.collection.convert.WrapAsScala$class.mapAsScalaMap
  [ 1] scala.collection.convert.WrapAsScala$.mapAsScalaMap
  [ 2] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [ 3] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [ 4] scala.collection.convert.Decorators$AsScala.asScala
  [ 5] scala.sys.package$.env
  [ 6] org.apache.spark.util.Utils$.<init>
  [ 7] org.apache.spark.util.Utils$.<clinit>
  [ 8] org.apache.spark.internal.Logging$.<init>
  [ 9] org.apache.spark.internal.Logging$.<clinit>
  [10] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [11] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [12] org.apache.spark.deploy.SparkSubmit.doSubmit
  [13] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [14] org.apache.spark.deploy.SparkSubmit$.main
  [15] org.apache.spark.deploy.SparkSubmit.main
  [16] [DestroyJavaVM tid=16064]

--- 1551158138308456 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.collection.AbstractTraversable.<init>
  [16] scala.collection.AbstractIterable.<init>
  [17] scala.collection.AbstractMap.<init>
  [18] scala.collection.mutable.AbstractMap.<init>
  [19] scala.collection.convert.Wrappers$JMapWrapper.<init>
  [20] scala.collection.convert.WrapAsScala$class.mapAsScalaMap
  [21] scala.collection.convert.WrapAsScala$.mapAsScalaMap
  [22] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [23] scala.collection.convert.DecorateAsScala$$anonfun$mapAsScalaMapConverter$1.apply
  [24] scala.collection.convert.Decorators$AsScala.asScala
  [25] scala.sys.package$.env
  [26] org.apache.spark.util.Utils$.<init>
  [27] org.apache.spark.util.Utils$.<clinit>
  [28] org.apache.spark.internal.Logging$.<init>
  [29] org.apache.spark.internal.Logging$.<clinit>
  [30] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [31] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [32] org.apache.spark.deploy.SparkSubmit.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$.main
  [35] org.apache.spark.deploy.SparkSubmit.main
  [36] [DestroyJavaVM tid=16064]

--- 1551158138402916 us
  [ 0] scala.sys.SystemProperties$.bool
  [ 1] scala.sys.SystemProperties$.noTraceSupression$lzycompute
  [ 2] scala.sys.SystemProperties$.noTraceSupression
  [ 3] scala.util.control.NoStackTrace$.<init>
  [ 4] scala.util.control.NoStackTrace$.<clinit>
  [ 5] scala.util.control.NoStackTrace$class.fillInStackTrace
  [ 6] scala.util.control.BreakControl.fillInStackTrace
  [ 7] java.lang.Throwable.<init>
  [ 8] scala.util.control.BreakControl.<init>
  [ 9] scala.util.control.Breaks.<init>
  [10] scala.collection.Traversable$.<init>
  [11] scala.collection.Traversable$.<clinit>
  [12] scala.package$.<init>
  [13] scala.package$.<clinit>
  [14] scala.Predef$.<init>
  [15] scala.Predef$.<clinit>
  [16] scala.collection.immutable.Map$Map4.updated
  [17] scala.collection.immutable.Map$Map4.$plus
  [18] scala.collection.immutable.Map$Map4.$plus
  [19] scala.collection.mutable.MapBuilder.$plus$eq
  [20] scala.collection.mutable.MapBuilder.$plus$eq
  [21] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [22] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [23] scala.collection.mutable.ResizableArray$class.foreach
  [24] scala.collection.mutable.ArrayBuffer.foreach
  [25] scala.collection.generic.Growable$class.$plus$plus$eq
  [26] scala.collection.mutable.MapBuilder.$plus$plus$eq
  [27] scala.collection.generic.GenMapFactory.apply
  [28] scala.sys.package$.env
  [29] org.apache.spark.util.Utils$.<init>
  [30] org.apache.spark.util.Utils$.<clinit>
  [31] org.apache.spark.internal.Logging$.<init>
  [32] org.apache.spark.internal.Logging$.<clinit>
  [33] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [34] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [35] org.apache.spark.deploy.SparkSubmit.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [37] org.apache.spark.deploy.SparkSubmit$.main
  [38] org.apache.spark.deploy.SparkSubmit.main
  [39] [DestroyJavaVM tid=16064]

--- 1551158138408839 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.sys.SystemProperties$.propertyHelp$lzycompute
  [16] scala.sys.SystemProperties$.propertyHelp
  [17] scala.sys.SystemProperties$.addHelp
  [18] scala.sys.SystemProperties$.bool
  [19] scala.sys.SystemProperties$.noTraceSupression$lzycompute
  [20] scala.sys.SystemProperties$.noTraceSupression
  [21] scala.util.control.NoStackTrace$.<init>
  [22] scala.util.control.NoStackTrace$.<clinit>
  [23] scala.util.control.NoStackTrace$class.fillInStackTrace
  [24] scala.util.control.BreakControl.fillInStackTrace
  [25] java.lang.Throwable.<init>
  [26] scala.util.control.BreakControl.<init>
  [27] scala.util.control.Breaks.<init>
  [28] scala.collection.Traversable$.<init>
  [29] scala.collection.Traversable$.<clinit>
  [30] scala.package$.<init>
  [31] scala.package$.<clinit>
  [32] scala.Predef$.<init>
  [33] scala.Predef$.<clinit>
  [34] scala.collection.immutable.Map$Map4.updated
  [35] scala.collection.immutable.Map$Map4.$plus
  [36] scala.collection.immutable.Map$Map4.$plus
  [37] scala.collection.mutable.MapBuilder.$plus$eq
  [38] scala.collection.mutable.MapBuilder.$plus$eq
  [39] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [40] scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply
  [41] scala.collection.mutable.ResizableArray$class.foreach
  [42] scala.collection.mutable.ArrayBuffer.foreach
  [43] scala.collection.generic.Growable$class.$plus$plus$eq
  [44] scala.collection.mutable.MapBuilder.$plus$plus$eq
  [45] scala.collection.generic.GenMapFactory.apply
  [46] scala.sys.package$.env
  [47] org.apache.spark.util.Utils$.<init>
  [48] org.apache.spark.util.Utils$.<clinit>
  [49] org.apache.spark.internal.Logging$.<init>
  [50] org.apache.spark.internal.Logging$.<clinit>
  [51] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [52] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [53] org.apache.spark.deploy.SparkSubmit.doSubmit
  [54] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [55] org.apache.spark.deploy.SparkSubmit$.main
  [56] org.apache.spark.deploy.SparkSubmit.main
  [57] [DestroyJavaVM tid=16064]

--- 1551158138503686 us
  [ 0] java.util.regex.Pattern$Single.<init>
  [ 1] java.util.regex.Pattern.newSingle
  [ 2] java.util.regex.Pattern.atom
  [ 3] java.util.regex.Pattern.sequence
  [ 4] java.util.regex.Pattern.expr
  [ 5] java.util.regex.Pattern.compile
  [ 6] java.util.regex.Pattern.<init>
  [ 7] java.util.regex.Pattern.compile
  [ 8] scala.util.matching.Regex.<init>
  [ 9] scala.collection.immutable.StringLike$class.r
  [10] scala.collection.immutable.StringOps.r
  [11] scala.collection.immutable.StringLike$class.r
  [12] scala.collection.immutable.StringOps.r
  [13] org.apache.spark.util.Utils$.<init>
  [14] org.apache.spark.util.Utils$.<clinit>
  [15] org.apache.spark.internal.Logging$.<init>
  [16] org.apache.spark.internal.Logging$.<clinit>
  [17] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [18] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [19] org.apache.spark.deploy.SparkSubmit.doSubmit
  [20] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [21] org.apache.spark.deploy.SparkSubmit$.main
  [22] org.apache.spark.deploy.SparkSubmit.main
  [23] [DestroyJavaVM tid=16064]

--- 1551158138509610 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] java.lang.Byte.valueOf
  [12] org.apache.commons.lang3.math.NumberUtils.<clinit>
  [13] org.apache.commons.lang3.JavaVersion.toFloatVersion
  [14] org.apache.commons.lang3.JavaVersion.maxVersion
  [15] org.apache.commons.lang3.JavaVersion.<clinit>
  [16] org.apache.commons.lang3.SystemUtils.<clinit>
  [17] org.apache.spark.util.Utils$.<init>
  [18] org.apache.spark.util.Utils$.<clinit>
  [19] org.apache.spark.internal.Logging$.<init>
  [20] org.apache.spark.internal.Logging$.<clinit>
  [21] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [22] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [23] org.apache.spark.deploy.SparkSubmit.doSubmit
  [24] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [25] org.apache.spark.deploy.SparkSubmit$.main
  [26] org.apache.spark.deploy.SparkSubmit.main
  [27] [DestroyJavaVM tid=16064]

--- 1551158138616775 us
  [ 0] java.util.concurrent.ConcurrentHashMap.keySet
  [ 1] java.util.concurrent.ConcurrentHashMap.keySet
  [ 2] sun.util.resources.ParallelListResourceBundle.handleKeySet
  [ 3] sun.util.resources.ParallelListResourceBundle.keySet
  [ 4] sun.util.resources.ParallelListResourceBundle.containsKey
  [ 5] java.text.DateFormatSymbols.initializeData
  [ 6] java.text.DateFormatSymbols.<init>
  [ 7] sun.util.locale.provider.DateFormatSymbolsProviderImpl.getInstance
  [ 8] java.text.DateFormatSymbols.getProviderInstance
  [ 9] java.text.DateFormatSymbols.getInstanceRef
  [10] java.text.SimpleDateFormat.<init>
  [11] java.text.SimpleDateFormat.<init>
  [12] org.apache.log4j.helpers.PatternParser.finalizeConverter
  [13] org.apache.log4j.helpers.PatternParser.parse
  [14] org.apache.log4j.PatternLayout.setConversionPattern
  [15] sun.reflect.NativeMethodAccessorImpl.invoke0
  [16] sun.reflect.NativeMethodAccessorImpl.invoke
  [17] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [18] java.lang.reflect.Method.invoke
  [19] org.apache.log4j.config.PropertySetter.setProperty
  [20] org.apache.log4j.config.PropertySetter.setProperty
  [21] org.apache.log4j.config.PropertySetter.setProperties
  [22] org.apache.log4j.config.PropertySetter.setProperties
  [23] org.apache.log4j.PropertyConfigurator.parseAppender
  [24] org.apache.log4j.PropertyConfigurator.parseCategory
  [25] org.apache.log4j.PropertyConfigurator.configureRootCategory
  [26] org.apache.log4j.PropertyConfigurator.doConfigure
  [27] org.apache.log4j.PropertyConfigurator.doConfigure
  [28] org.apache.log4j.helpers.OptionConverter.selectAndConfigure
  [29] org.apache.log4j.LogManager.<clinit>
  [30] org.apache.spark.internal.Logging$class.initializeLogging
  [31] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [32] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [33] org.apache.spark.deploy.SparkSubmit.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [35] org.apache.spark.deploy.SparkSubmit$.main
  [36] org.apache.spark.deploy.SparkSubmit.main
  [37] [DestroyJavaVM tid=16064]

--- 1551158138622731 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] sun.util.locale.provider.NumberFormatProviderImpl.getInstance
  [10] sun.util.locale.provider.NumberFormatProviderImpl.getIntegerInstance
  [11] java.text.NumberFormat.getInstance
  [12] java.text.NumberFormat.getInstance
  [13] java.text.NumberFormat.getIntegerInstance
  [14] java.text.SimpleDateFormat.initialize
  [15] java.text.SimpleDateFormat.<init>
  [16] java.text.SimpleDateFormat.<init>
  [17] org.apache.log4j.helpers.PatternParser.finalizeConverter
  [18] org.apache.log4j.helpers.PatternParser.parse
  [19] org.apache.log4j.PatternLayout.setConversionPattern
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] org.apache.log4j.config.PropertySetter.setProperty
  [25] org.apache.log4j.config.PropertySetter.setProperty
  [26] org.apache.log4j.config.PropertySetter.setProperties
  [27] org.apache.log4j.config.PropertySetter.setProperties
  [28] org.apache.log4j.PropertyConfigurator.parseAppender
  [29] org.apache.log4j.PropertyConfigurator.parseCategory
  [30] org.apache.log4j.PropertyConfigurator.configureRootCategory
  [31] org.apache.log4j.PropertyConfigurator.doConfigure
  [32] org.apache.log4j.PropertyConfigurator.doConfigure
  [33] org.apache.log4j.helpers.OptionConverter.selectAndConfigure
  [34] org.apache.log4j.LogManager.<clinit>
  [35] org.apache.spark.internal.Logging$class.initializeLogging
  [36] org.apache.spark.internal.Logging$class.initializeLogIfNecessary
  [37] org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary
  [38] org.apache.spark.deploy.SparkSubmit.doSubmit
  [39] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [40] org.apache.spark.deploy.SparkSubmit$.main
  [41] org.apache.spark.deploy.SparkSubmit.main
  [42] [DestroyJavaVM tid=16064]

--- 1551158138727263 us
  [ 0] org.apache.spark.deploy.SparkSubmitArguments.loadEnvironmentArguments
  [ 1] org.apache.spark.deploy.SparkSubmitArguments.<init>
  [ 2] org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$1.<init>
  [ 3] org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments
  [ 4] org.apache.spark.deploy.SparkSubmit.doSubmit
  [ 5] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [ 6] org.apache.spark.deploy.SparkSubmit$.main
  [ 7] org.apache.spark.deploy.SparkSubmit.main
  [ 8] [DestroyJavaVM tid=16064]

--- 1551158138734285 us
  [ 0] KlassDepChange::is_klass_change() const
  [ 1] CodeCache::mark_for_deoptimization(DepChange&)
  [ 2] Universe::flush_dependents_on(instanceKlassHandle)
  [ 3] SystemDictionary::define_instance_class(instanceKlassHandle, Thread*)
  [ 4] SystemDictionary::find_or_define_instance_class(Symbol*, Handle, instanceKlassHandle, Thread*)
  [ 5] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 6] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 7] JVM_DefineClassWithSource
  [ 8] Java_java_lang_ClassLoader_defineClass1
  [ 9] java.lang.ClassLoader.defineClass1
  [10] java.lang.ClassLoader.defineClass
  [11] java.security.SecureClassLoader.defineClass
  [12] java.net.URLClassLoader.defineClass
  [13] java.net.URLClassLoader.access$100
  [14] java.net.URLClassLoader$1.run
  [15] java.net.URLClassLoader$1.run
  [16] java.security.AccessController.doPrivileged
  [17] java.net.URLClassLoader.findClass
  [18] java.lang.ClassLoader.loadClass
  [19] sun.misc.Launcher$AppClassLoader.loadClass
  [20] java.lang.ClassLoader.loadClass
  [21] org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments
  [22] org.apache.spark.deploy.SparkSubmitArguments.validateArguments
  [23] org.apache.spark.deploy.SparkSubmitArguments.<init>
  [24] org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$1.<init>
  [25] org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16064]

--- 1551158138852750 us
  [ 0] org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers
  [ 1] org.apache.spark.deploy.SparkSubmitUtils$.buildIvySettings
  [ 2] org.apache.spark.deploy.DependencyUtils$.resolveMavenDependencies
  [ 3] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [ 4] org.apache.spark.deploy.SparkSubmit.submit
  [ 5] org.apache.spark.deploy.SparkSubmit.doSubmit
  [ 6] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [ 7] org.apache.spark.deploy.SparkSubmit$.main
  [ 8] org.apache.spark.deploy.SparkSubmit.main
  [ 9] [DestroyJavaVM tid=16064]

--- 1551158138858733 us
  [ 0] exception_handler_for_pc_helper(JavaThread*, oopDesc*, unsigned char*, nmethod*&) [clone .constprop.50]
  [ 1] Runtime1::exception_handler_for_pc(JavaThread*)
  [ 2] java.lang.ClassLoader.loadClass
  [ 3] sun.misc.Launcher$AppClassLoader.loadClass
  [ 4] java.lang.ClassLoader.loadClass
  [ 5] java.lang.ClassLoader.defineClass1
  [ 6] java.lang.ClassLoader.defineClass
  [ 7] java.security.SecureClassLoader.defineClass
  [ 8] java.net.URLClassLoader.defineClass
  [ 9] java.net.URLClassLoader.access$100
  [10] java.net.URLClassLoader$1.run
  [11] java.net.URLClassLoader$1.run
  [12] java.security.AccessController.doPrivileged
  [13] java.net.URLClassLoader.findClass
  [14] java.lang.ClassLoader.loadClass
  [15] sun.misc.Launcher$AppClassLoader.loadClass
  [16] java.lang.ClassLoader.loadClass
  [17] org.apache.ivy.plugins.resolver.BasicResolver.<init>
  [18] org.apache.ivy.plugins.resolver.AbstractPatternsBasedResolver.<init>
  [19] org.apache.ivy.plugins.resolver.RepositoryResolver.<init>
  [20] org.apache.ivy.plugins.resolver.URLResolver.<init>
  [21] org.apache.ivy.plugins.resolver.IBiblioResolver.<init>
  [22] org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers
  [23] org.apache.spark.deploy.SparkSubmitUtils$.buildIvySettings
  [24] org.apache.spark.deploy.DependencyUtils$.resolveMavenDependencies
  [25] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [26] org.apache.spark.deploy.SparkSubmit.submit
  [27] org.apache.spark.deploy.SparkSubmit.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [29] org.apache.spark.deploy.SparkSubmit$.main
  [30] org.apache.spark.deploy.SparkSubmit.main
  [31] [DestroyJavaVM tid=16064]

--- 1551158138953383 us
  [ 0] java.lang.ClassLoader.findBootstrapClass
  [ 1] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 2] java.lang.ClassLoader.loadClass
  [ 3] java.lang.ClassLoader.loadClass
  [ 4] sun.misc.Launcher$AppClassLoader.loadClass
  [ 5] java.lang.ClassLoader.loadClass
  [ 6] org.apache.spark.internal.config.package$.<init>
  [ 7] org.apache.spark.internal.config.package$.<clinit>
  [ 8] org.apache.spark.SparkConf$.<init>
  [ 9] org.apache.spark.SparkConf$.<clinit>
  [10] org.apache.spark.SparkConf.set
  [11] org.apache.spark.SparkConf.set
  [12] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$1.apply
  [13] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$1.apply
  [14] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [15] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [16] scala.collection.mutable.HashTable$class.foreachEntry
  [17] scala.collection.mutable.HashMap.foreachEntry
  [18] scala.collection.mutable.HashMap.foreach
  [19] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [20] org.apache.spark.deploy.SparkSubmit.submit
  [21] org.apache.spark.deploy.SparkSubmit.doSubmit
  [22] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [23] org.apache.spark.deploy.SparkSubmit$.main
  [24] org.apache.spark.deploy.SparkSubmit.main
  [25] [DestroyJavaVM tid=16064]

--- 1551158138959366 us
  [ 0] LinkResolver::check_field_accessability(KlassHandle, KlassHandle, KlassHandle, fieldDescriptor&, Thread*)
  [ 1] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 2] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 3] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 4] scala.collection.immutable.TrieIterator.<init>
  [ 5] scala.collection.immutable.HashMap$HashTrieMap$$anon$1.<init>
  [ 6] scala.collection.immutable.HashMap$HashTrieMap.iterator
  [ 7] scala.collection.MapLike$$anon$1.<init>
  [ 8] scala.collection.MapLike$class.keysIterator
  [ 9] scala.collection.AbstractMap.keysIterator
  [10] scala.collection.MapLike$DefaultKeySet.foreach
  [11] scala.collection.TraversableLike$class.flatMap
  [12] scala.collection.AbstractTraversable.flatMap
  [13] org.apache.spark.SparkConf$.<init>
  [14] org.apache.spark.SparkConf$.<clinit>
  [15] org.apache.spark.SparkConf.set
  [16] org.apache.spark.SparkConf.set
  [17] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$1.apply
  [18] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$1.apply
  [19] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [20] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [21] scala.collection.mutable.HashTable$class.foreachEntry
  [22] scala.collection.mutable.HashMap.foreachEntry
  [23] scala.collection.mutable.HashMap.foreach
  [24] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [25] org.apache.spark.deploy.SparkSubmit.submit
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16064]

--- 1551158139053201 us
  [ 0] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$ElementStack.<init>
  [ 1] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.<init>
  [ 2] org.apache.xerces.impl.XMLDocumentScannerImpl.<init>
  [ 3] org.apache.xerces.impl.XMLNSDocumentScannerImpl.<init>
  [ 4] org.apache.xerces.parsers.XML11Configuration.<init>
  [ 5] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 6] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [ 7] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [ 8] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [ 9] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [10] java.lang.reflect.Constructor.newInstance
  [11] java.lang.Class.newInstance
  [12] org.apache.xerces.parsers.ObjectFactory.newInstance
  [13] org.apache.xerces.parsers.ObjectFactory.createObject
  [14] org.apache.xerces.parsers.ObjectFactory.createObject
  [15] org.apache.xerces.parsers.DOMParser.<init>
  [16] org.apache.xerces.parsers.DOMParser.<init>
  [17] org.apache.xerces.jaxp.DocumentBuilderImpl.<init>
  [18] org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder
  [19] org.apache.hadoop.conf.Configuration.loadResource
  [20] org.apache.hadoop.conf.Configuration.loadResources
  [21] org.apache.hadoop.conf.Configuration.getProps
  [22] org.apache.hadoop.conf.Configuration.set
  [23] org.apache.hadoop.conf.Configuration.set
  [24] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [25] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [26] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [28] scala.Option.getOrElse
  [29] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [30] org.apache.spark.deploy.SparkSubmit.submit
  [31] org.apache.spark.deploy.SparkSubmit.doSubmit
  [32] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$.main
  [34] org.apache.spark.deploy.SparkSubmit.main
  [35] [DestroyJavaVM tid=16064]

--- 1551158139059173 us
  [ 0] inflate_table
  [ 1] inflate
  [ 2] Java_java_util_zip_Inflater_inflateBytes
  [ 3] java.util.zip.Inflater.inflateBytes
  [ 4] java.util.zip.Inflater.inflate
  [ 5] java.util.zip.InflaterInputStream.read
  [ 6] sun.misc.Resource.getBytes
  [ 7] java.net.URLClassLoader.defineClass
  [ 8] java.net.URLClassLoader.access$100
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] org.apache.xerces.impl.XMLDocumentScannerImpl.<init>
  [17] org.apache.xerces.impl.XMLNSDocumentScannerImpl.<init>
  [18] org.apache.xerces.parsers.XML11Configuration.<init>
  [19] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [20] org.apache.xerces.parsers.XIncludeAwareParserConfiguration.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] java.lang.Class.newInstance
  [26] org.apache.xerces.parsers.ObjectFactory.newInstance
  [27] org.apache.xerces.parsers.ObjectFactory.createObject
  [28] org.apache.xerces.parsers.ObjectFactory.createObject
  [29] org.apache.xerces.parsers.DOMParser.<init>
  [30] org.apache.xerces.parsers.DOMParser.<init>
  [31] org.apache.xerces.jaxp.DocumentBuilderImpl.<init>
  [32] org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder
  [33] org.apache.hadoop.conf.Configuration.loadResource
  [34] org.apache.hadoop.conf.Configuration.loadResources
  [35] org.apache.hadoop.conf.Configuration.getProps
  [36] org.apache.hadoop.conf.Configuration.set
  [37] org.apache.hadoop.conf.Configuration.set
  [38] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [39] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [40] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [41] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [42] scala.Option.getOrElse
  [43] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [44] org.apache.spark.deploy.SparkSubmit.submit
  [45] org.apache.spark.deploy.SparkSubmit.doSubmit
  [46] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [47] org.apache.spark.deploy.SparkSubmit$.main
  [48] org.apache.spark.deploy.SparkSubmit.main
  [49] [DestroyJavaVM tid=16064]

--- 1551158139161349 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] com.google.common.collect.Interners.newStrongInterner
  [13] org.apache.hadoop.util.StringInterner.<clinit>
  [14] org.apache.hadoop.conf.Configuration.loadResource
  [15] org.apache.hadoop.conf.Configuration.loadResources
  [16] org.apache.hadoop.conf.Configuration.getProps
  [17] org.apache.hadoop.conf.Configuration.set
  [18] org.apache.hadoop.conf.Configuration.set
  [19] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [20] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [21] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [22] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [23] scala.Option.getOrElse
  [24] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [25] org.apache.spark.deploy.SparkSubmit.submit
  [26] org.apache.spark.deploy.SparkSubmit.doSubmit
  [27] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [28] org.apache.spark.deploy.SparkSubmit$.main
  [29] org.apache.spark.deploy.SparkSubmit.main
  [30] [DestroyJavaVM tid=16064]

--- 1551158139167341 us
  [ 0] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 1] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 2] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 3] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 4] JVM_DefineClassWithSource
  [ 5] Java_java_lang_ClassLoader_defineClass1
  [ 6] java.lang.ClassLoader.defineClass1
  [ 7] java.lang.ClassLoader.defineClass
  [ 8] java.security.SecureClassLoader.defineClass
  [ 9] java.net.URLClassLoader.defineClass
  [10] java.net.URLClassLoader.access$100
  [11] java.net.URLClassLoader$1.run
  [12] java.net.URLClassLoader$1.run
  [13] java.security.AccessController.doPrivileged
  [14] java.net.URLClassLoader.findClass
  [15] java.lang.ClassLoader.loadClass
  [16] sun.misc.Launcher$AppClassLoader.loadClass
  [17] java.lang.ClassLoader.loadClass
  [18] com.google.common.collect.Interners$WeakInterner.<init>
  [19] com.google.common.collect.Interners$WeakInterner.<init>
  [20] com.google.common.collect.Interners.newWeakInterner
  [21] org.apache.hadoop.util.StringInterner.<clinit>
  [22] org.apache.hadoop.conf.Configuration.loadResource
  [23] org.apache.hadoop.conf.Configuration.loadResources
  [24] org.apache.hadoop.conf.Configuration.getProps
  [25] org.apache.hadoop.conf.Configuration.set
  [26] org.apache.hadoop.conf.Configuration.set
  [27] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [28] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [29] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [30] org.apache.spark.deploy.SparkSubmit$$anonfun$2.apply
  [31] scala.Option.getOrElse
  [32] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [33] org.apache.spark.deploy.SparkSubmit.submit
  [34] org.apache.spark.deploy.SparkSubmit.doSubmit
  [35] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$.main
  [37] org.apache.spark.deploy.SparkSubmit.main
  [38] [DestroyJavaVM tid=16064]

--- 1551158139277273 us
  [ 0] java.lang.ClassLoader.defineClass1
  [ 1] java.lang.ClassLoader.defineClass
  [ 2] java.security.SecureClassLoader.defineClass
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] org.apache.hadoop.metrics2.lib.DefaultMetricsFactory.getInstance
  [13] org.apache.hadoop.metrics2.lib.DefaultMetricsFactory.getAnnotatedMetricsFactory
  [14] org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder
  [15] org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register
  [16] org.apache.hadoop.metrics2.MetricsSystem.register
  [17] org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create
  [18] org.apache.hadoop.security.UserGroupInformation.<clinit>
  [19] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [20] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [21] scala.Option.getOrElse
  [22] org.apache.spark.util.Utils$.getCurrentUserName
  [23] org.apache.spark.SecurityManager.<init>
  [24] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [25] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [26] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [28] scala.Option.map
  [29] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [30] org.apache.spark.deploy.SparkSubmit.submit
  [31] org.apache.spark.deploy.SparkSubmit.doSubmit
  [32] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$.main
  [34] org.apache.spark.deploy.SparkSubmit.main
  [35] [DestroyJavaVM tid=16064]

--- 1551158139283240 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] sun.management.ManagementFactoryHelper.getThreadMXBean
  [10] java.lang.management.ManagementFactory.getThreadMXBean
  [11] org.apache.hadoop.util.ReflectionUtils.<clinit>
  [12] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.initRegistry
  [13] org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.<init>
  [14] org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder
  [15] org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register
  [16] org.apache.hadoop.metrics2.MetricsSystem.register
  [17] org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create
  [18] org.apache.hadoop.security.UserGroupInformation.<clinit>
  [19] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [20] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [21] scala.Option.getOrElse
  [22] org.apache.spark.util.Utils$.getCurrentUserName
  [23] org.apache.spark.SecurityManager.<init>
  [24] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [25] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [26] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [28] scala.Option.map
  [29] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [30] org.apache.spark.deploy.SparkSubmit.submit
  [31] org.apache.spark.deploy.SparkSubmit.doSubmit
  [32] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$.main
  [34] org.apache.spark.deploy.SparkSubmit.main
  [35] [DestroyJavaVM tid=16064]

--- 1551158139378785 us
  [ 0] java.lang.invoke.LambdaForm$BasicType.<clinit>
  [ 1] java.lang.invoke.LambdaForm.<clinit>
  [ 2] java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm
  [ 3] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [ 4] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [ 5] java.lang.invoke.DirectMethodHandle.make
  [ 6] java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon
  [ 7] java.lang.invoke.MethodHandles$Lookup.getDirectMethodNoSecurityManager
  [ 8] java.lang.invoke.MethodHandles$Lookup.getDirectMethodForConstant
  [ 9] java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant
  [10] java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant
  [11] java.lang.UNIXProcess$Platform.get
  [12] java.lang.UNIXProcess.<clinit>
  [13] java.lang.ProcessImpl.start
  [14] java.lang.ProcessBuilder.start
  [15] org.apache.hadoop.util.Shell.runCommand
  [16] org.apache.hadoop.util.Shell.run
  [17] org.apache.hadoop.util.Shell$ShellCommandExecutor.execute
  [18] org.apache.hadoop.util.Shell.isSetsidSupported
  [19] org.apache.hadoop.util.Shell.<clinit>
  [20] org.apache.hadoop.util.StringUtils.<clinit>
  [21] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [22] org.apache.hadoop.security.UserGroupInformation.initialize
  [23] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [24] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [25] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [26] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [27] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [28] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.util.Utils$.getCurrentUserName
  [31] org.apache.spark.SecurityManager.<init>
  [32] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [33] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [34] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [35] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [36] scala.Option.map
  [37] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [38] org.apache.spark.deploy.SparkSubmit.submit
  [39] org.apache.spark.deploy.SparkSubmit.doSubmit
  [40] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [41] org.apache.spark.deploy.SparkSubmit$.main
  [42] org.apache.spark.deploy.SparkSubmit.main
  [43] [DestroyJavaVM tid=16064]

--- 1551158139384716 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] java.lang.invoke.LambdaForm.compileToBytecode
  [13] java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm
  [14] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [15] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [16] java.lang.invoke.DirectMethodHandle.make
  [17] java.lang.invoke.DirectMethodHandle.make
  [18] java.lang.invoke.DirectMethodHandle.make
  [19] java.lang.invoke.LambdaForm$NamedFunction.resolve
  [20] java.lang.invoke.DirectMethodHandle$Lazy.<clinit>
  [21] java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm
  [22] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [23] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [24] java.lang.invoke.DirectMethodHandle.make
  [25] java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon
  [26] java.lang.invoke.MethodHandles$Lookup.getDirectMethod
  [27] java.lang.invoke.MethodHandles$Lookup.findStatic
  [28] java.lang.invoke.BoundMethodHandle$Factory.makeCbmhCtor
  [29] java.lang.invoke.BoundMethodHandle$Factory.makeCtors
  [30] java.lang.invoke.BoundMethodHandle$SpeciesData.initForBootstrap
  [31] java.lang.invoke.BoundMethodHandle$SpeciesData.<clinit>
  [32] java.lang.invoke.BoundMethodHandle.<clinit>
  [33] java.lang.invoke.LambdaForm.createIdentityForms
  [34] java.lang.invoke.LambdaForm.<clinit>
  [35] java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm
  [36] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [37] java.lang.invoke.DirectMethodHandle.preparedLambdaForm
  [38] java.lang.invoke.DirectMethodHandle.make
  [39] java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon
  [40] java.lang.invoke.MethodHandles$Lookup.getDirectMethodNoSecurityManager
  [41] java.lang.invoke.MethodHandles$Lookup.getDirectMethodForConstant
  [42] java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant
  [43] java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant
  [44] java.lang.UNIXProcess$Platform.get
  [45] java.lang.UNIXProcess.<clinit>
  [46] java.lang.ProcessImpl.start
  [47] java.lang.ProcessBuilder.start
  [48] org.apache.hadoop.util.Shell.runCommand
  [49] org.apache.hadoop.util.Shell.run
  [50] org.apache.hadoop.util.Shell$ShellCommandExecutor.execute
  [51] org.apache.hadoop.util.Shell.isSetsidSupported
  [52] org.apache.hadoop.util.Shell.<clinit>
  [53] org.apache.hadoop.util.StringUtils.<clinit>
  [54] org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod
  [55] org.apache.hadoop.security.UserGroupInformation.initialize
  [56] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [57] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [58] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [59] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [60] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [61] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [62] scala.Option.getOrElse
  [63] org.apache.spark.util.Utils$.getCurrentUserName
  [64] org.apache.spark.SecurityManager.<init>
  [65] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [66] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [67] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [68] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [69] scala.Option.map
  [70] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [71] org.apache.spark.deploy.SparkSubmit.submit
  [72] org.apache.spark.deploy.SparkSubmit.doSubmit
  [73] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [74] org.apache.spark.deploy.SparkSubmit$.main
  [75] org.apache.spark.deploy.SparkSubmit.main
  [76] [DestroyJavaVM tid=16064]

--- 1551158139480965 us
  [ 0] org.apache.commons.collections.map.UnmodifiableMap.entrySet
  [ 1] org.apache.hadoop.conf.Configuration$DeprecationContext.<init>
  [ 2] org.apache.hadoop.conf.Configuration.addDeprecations
  [ 3] org.apache.hadoop.mapreduce.util.ConfigUtil.addDeprecatedKeys
  [ 4] org.apache.hadoop.mapreduce.util.ConfigUtil.loadResources
  [ 5] org.apache.hadoop.mapred.JobConf.<clinit>
  [ 6] java.lang.Class.forName0
  [ 7] java.lang.Class.forName
  [ 8] org.apache.hadoop.conf.Configuration.getClassByNameOrNull
  [ 9] org.apache.hadoop.util.ReflectionUtils.setJobConf
  [10] org.apache.hadoop.util.ReflectionUtils.setConf
  [11] org.apache.hadoop.util.ReflectionUtils.newInstance
  [12] org.apache.hadoop.security.Groups.<init>
  [13] org.apache.hadoop.security.Groups.<init>
  [14] org.apache.hadoop.security.Groups.getUserToGroupsMappingService
  [15] org.apache.hadoop.security.UserGroupInformation.initialize
  [16] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [17] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [18] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [19] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [20] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [21] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.util.Utils$.getCurrentUserName
  [24] org.apache.spark.SecurityManager.<init>
  [25] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [26] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [27] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [28] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [29] scala.Option.map
  [30] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [31] org.apache.spark.deploy.SparkSubmit.submit
  [32] org.apache.spark.deploy.SparkSubmit.doSubmit
  [33] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [34] org.apache.spark.deploy.SparkSubmit$.main
  [35] org.apache.spark.deploy.SparkSubmit.main
  [36] [DestroyJavaVM tid=16064]

--- 1551158139486919 us
  [ 0] inflate_fast
  [ 1] inflate
  [ 2] Java_java_util_zip_Inflater_inflateBytes
  [ 3] java.util.zip.Inflater.inflateBytes
  [ 4] java.util.zip.Inflater.inflate
  [ 5] java.util.zip.InflaterInputStream.read
  [ 6] java.io.FilterInputStream.read
  [ 7] org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read
  [ 8] org.apache.xerces.impl.io.UTF8Reader.read
  [ 9] org.apache.xerces.impl.XMLEntityScanner.load
  [10] org.apache.xerces.impl.XMLEntityScanner.scanContent
  [11] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent
  [12] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch
  [13] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument
  [14] org.apache.xerces.parsers.XML11Configuration.parse
  [15] org.apache.xerces.parsers.XML11Configuration.parse
  [16] org.apache.xerces.parsers.XMLParser.parse
  [17] org.apache.xerces.parsers.DOMParser.parse
  [18] org.apache.xerces.jaxp.DocumentBuilderImpl.parse
  [19] javax.xml.parsers.DocumentBuilder.parse
  [20] org.apache.hadoop.conf.Configuration.parse
  [21] org.apache.hadoop.conf.Configuration.parse
  [22] org.apache.hadoop.conf.Configuration.loadResource
  [23] org.apache.hadoop.conf.Configuration.loadResources
  [24] org.apache.hadoop.conf.Configuration.getProps
  [25] org.apache.hadoop.conf.Configuration.get
  [26] org.apache.hadoop.conf.Configuration.getTrimmed
  [27] org.apache.hadoop.conf.Configuration.getLong
  [28] org.apache.hadoop.security.Groups.<init>
  [29] org.apache.hadoop.security.Groups.<init>
  [30] org.apache.hadoop.security.Groups.getUserToGroupsMappingService
  [31] org.apache.hadoop.security.UserGroupInformation.initialize
  [32] org.apache.hadoop.security.UserGroupInformation.ensureInitialized
  [33] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject
  [34] org.apache.hadoop.security.UserGroupInformation.getLoginUser
  [35] org.apache.hadoop.security.UserGroupInformation.getCurrentUser
  [36] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [37] org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply
  [38] scala.Option.getOrElse
  [39] org.apache.spark.util.Utils$.getCurrentUserName
  [40] org.apache.spark.SecurityManager.<init>
  [41] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [42] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [43] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [44] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [45] scala.Option.map
  [46] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [47] org.apache.spark.deploy.SparkSubmit.submit
  [48] org.apache.spark.deploy.SparkSubmit.doSubmit
  [49] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [50] org.apache.spark.deploy.SparkSubmit$.main
  [51] org.apache.spark.deploy.SparkSubmit.main
  [52] [DestroyJavaVM tid=16064]

--- 1551158139581363 us
  [ 0] sun.reflect.ClassFileAssembler.emitConstantPoolUTF8
  [ 1] sun.reflect.MethodAccessorGenerator.generate
  [ 2] sun.reflect.MethodAccessorGenerator.generateConstructor
  [ 3] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [ 4] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [ 5] java.lang.reflect.Constructor.newInstance
  [ 6] java.lang.Class.newInstance
  [ 7] java.util.ServiceLoader$LazyIterator.nextService
  [ 8] java.util.ServiceLoader$LazyIterator.next
  [ 9] java.util.ServiceLoader$1.next
  [10] javax.xml.parsers.FactoryFinder$1.run
  [11] java.security.AccessController.doPrivileged
  [12] javax.xml.parsers.FactoryFinder.findServiceProvider
  [13] javax.xml.parsers.FactoryFinder.find
  [14] javax.xml.parsers.DocumentBuilderFactory.newInstance
  [15] org.apache.hadoop.conf.Configuration.loadResource
  [16] org.apache.hadoop.conf.Configuration.loadResources
  [17] org.apache.hadoop.conf.Configuration.getProps
  [18] org.apache.hadoop.conf.Configuration.set
  [19] org.apache.hadoop.conf.Configuration.set
  [20] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [21] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [22] org.apache.spark.deploy.SparkHadoopUtil.newConfiguration
  [23] org.apache.spark.deploy.SparkHadoopUtil.<init>
  [24] org.apache.spark.deploy.SparkHadoopUtil$.instance$lzycompute
  [25] org.apache.spark.deploy.SparkHadoopUtil$.instance
  [26] org.apache.spark.deploy.SparkHadoopUtil$.get
  [27] org.apache.spark.SecurityManager.<init>
  [28] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [29] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [30] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [31] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [32] scala.Option.map
  [33] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [34] org.apache.spark.deploy.SparkSubmit.submit
  [35] org.apache.spark.deploy.SparkSubmit.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [37] org.apache.spark.deploy.SparkSubmit$.main
  [38] org.apache.spark.deploy.SparkSubmit.main
  [39] [DestroyJavaVM tid=16064]

--- 1551158139587308 us
  [ 0] org.apache.xerces.dom.DeferredDocumentImpl.setChunkIndex
  [ 1] org.apache.xerces.dom.DeferredDocumentImpl.appendChild
  [ 2] org.apache.xerces.parsers.AbstractDOMParser.startElement
  [ 3] org.apache.xerces.xinclude.XIncludeHandler.startElement
  [ 4] org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement
  [ 5] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch
  [ 6] org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument
  [ 7] org.apache.xerces.parsers.XML11Configuration.parse
  [ 8] org.apache.xerces.parsers.XML11Configuration.parse
  [ 9] org.apache.xerces.parsers.XMLParser.parse
  [10] org.apache.xerces.parsers.DOMParser.parse
  [11] org.apache.xerces.jaxp.DocumentBuilderImpl.parse
  [12] javax.xml.parsers.DocumentBuilder.parse
  [13] org.apache.hadoop.conf.Configuration.parse
  [14] org.apache.hadoop.conf.Configuration.parse
  [15] org.apache.hadoop.conf.Configuration.loadResource
  [16] org.apache.hadoop.conf.Configuration.loadResources
  [17] org.apache.hadoop.conf.Configuration.getProps
  [18] org.apache.hadoop.conf.Configuration.set
  [19] org.apache.hadoop.conf.Configuration.set
  [20] org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopConfigurations
  [21] org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration
  [22] org.apache.spark.deploy.SparkHadoopUtil.newConfiguration
  [23] org.apache.spark.deploy.SparkHadoopUtil.<init>
  [24] org.apache.spark.deploy.SparkHadoopUtil$.instance$lzycompute
  [25] org.apache.spark.deploy.SparkHadoopUtil$.instance
  [26] org.apache.spark.deploy.SparkHadoopUtil$.get
  [27] org.apache.spark.SecurityManager.<init>
  [28] org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1
  [29] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1
  [30] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [31] org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply
  [32] scala.Option.map
  [33] org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment
  [34] org.apache.spark.deploy.SparkSubmit.submit
  [35] org.apache.spark.deploy.SparkSubmit.doSubmit
  [36] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [37] org.apache.spark.deploy.SparkSubmit$.main
  [38] org.apache.spark.deploy.SparkSubmit.main
  [39] [DestroyJavaVM tid=16064]

--- 1551158139683431 us
  [ 0] java.util.Arrays.copyOfRange
  [ 1] java.lang.String.<init>
  [ 2] java.lang.String.substring
  [ 3] sun.net.www.protocol.jar.Handler.parseURL
  [ 4] java.net.URL.<init>
  [ 5] java.net.URL.<init>
  [ 6] sun.misc.URLClassPath$JarLoader.checkResource
  [ 7] sun.misc.URLClassPath$JarLoader.getResource
  [ 8] sun.misc.URLClassPath.getResource
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] org.apache.spark.util.Utils$.nonLocalPaths
  [17] org.apache.spark.deploy.PythonRunner$.formatPath
  [18] org.apache.spark.deploy.PythonRunner$.main
  [19] org.apache.spark.deploy.PythonRunner.main
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] org.apache.spark.deploy.JavaMainApplication.start
  [25] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain
  [26] org.apache.spark.deploy.SparkSubmit.doRunMain$1
  [27] org.apache.spark.deploy.SparkSubmit.submit
  [28] org.apache.spark.deploy.SparkSubmit.doSubmit
  [29] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [30] org.apache.spark.deploy.SparkSubmit$.main
  [31] org.apache.spark.deploy.SparkSubmit.main
  [32] [DestroyJavaVM tid=16064]

--- 1551158139691110 us
  [ 0] java.util.zip.InflaterInputStream.close
  [ 1] java.util.zip.ZipFile$ZipFileInflaterInputStream.close
  [ 2] sun.misc.Resource.getBytes
  [ 3] java.net.URLClassLoader.defineClass
  [ 4] java.net.URLClassLoader.access$100
  [ 5] java.net.URLClassLoader$1.run
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.security.AccessController.doPrivileged
  [ 8] java.net.URLClassLoader.findClass
  [ 9] java.lang.ClassLoader.loadClass
  [10] sun.misc.Launcher$AppClassLoader.loadClass
  [11] java.lang.ClassLoader.loadClass
  [12] java.lang.ClassLoader.defineClass1
  [13] java.lang.ClassLoader.defineClass
  [14] java.security.SecureClassLoader.defineClass
  [15] java.net.URLClassLoader.defineClass
  [16] java.net.URLClassLoader.access$100
  [17] java.net.URLClassLoader$1.run
  [18] java.net.URLClassLoader$1.run
  [19] java.security.AccessController.doPrivileged
  [20] java.net.URLClassLoader.findClass
  [21] java.lang.ClassLoader.loadClass
  [22] sun.misc.Launcher$AppClassLoader.loadClass
  [23] java.lang.ClassLoader.loadClass
  [24] py4j.GatewayServer$GatewayServerBuilder.<init>
  [25] py4j.GatewayServer$GatewayServerBuilder.<init>
  [26] org.apache.spark.deploy.PythonRunner$.main
  [27] org.apache.spark.deploy.PythonRunner.main
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] org.apache.spark.deploy.JavaMainApplication.start
  [33] org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain
  [34] org.apache.spark.deploy.SparkSubmit.doRunMain$1
  [35] org.apache.spark.deploy.SparkSubmit.submit
  [36] org.apache.spark.deploy.SparkSubmit.doSubmit
  [37] org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit
  [38] org.apache.spark.deploy.SparkSubmit$.main
  [39] org.apache.spark.deploy.SparkSubmit.main
  [40] [DestroyJavaVM tid=16064]

--- 1551158140074542 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] org.apache.spark.SparkContext.<init>
  [21] org.apache.spark.api.java.JavaSparkContext.<init>
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [23] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [24] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [25] java.lang.reflect.Constructor.newInstance
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.ConstructorCommand.invokeConstructor
  [30] py4j.commands.ConstructorCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16146]

--- 1551158140193666 us
  [ 0] __lock_text_start_[k]
  [ 1] try_to_wake_up_[k]
  [ 2] wake_up_q_[k]
  [ 3] futex_wake_[k]
  [ 4] do_futex_[k]
  [ 5] sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __pthread_cond_signal
  [ 9] java.util.concurrent.ConcurrentHashMap.get
  [10] java.util.concurrent.ConcurrentHashMap.containsKey
  [11] java.util.Collections$SetFromMap.contains
  [12] org.apache.hadoop.conf.Configuration.loadProperty
  [13] org.apache.hadoop.conf.Configuration.loadResource
  [14] org.apache.hadoop.conf.Configuration.loadResources
  [15] org.apache.hadoop.conf.Configuration.getProps
  [16] org.apache.hadoop.conf.Configuration.get
  [17] org.apache.hadoop.conf.Configuration.getStringCollection
  [18] org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders
  [19] org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders
  [20] org.apache.hadoop.conf.Configuration.getPassword
  [21] org.apache.spark.SSLOptions$$anonfun$8.apply
  [22] org.apache.spark.SSLOptions$$anonfun$8.apply
  [23] scala.Option.orElse
  [24] org.apache.spark.SSLOptions$.parse
  [25] org.apache.spark.SecurityManager.<init>
  [26] org.apache.spark.SparkEnv$.create
  [27] org.apache.spark.SparkEnv$.createDriverEnv
  [28] org.apache.spark.SparkContext.createSparkEnv
  [29] org.apache.spark.SparkContext.<init>
  [30] org.apache.spark.api.java.JavaSparkContext.<init>
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [33] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [34] java.lang.reflect.Constructor.newInstance
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.ConstructorCommand.invokeConstructor
  [39] py4j.commands.ConstructorCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16146]

--- 1551158140294652 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] io.netty.channel.nio.NioEventLoopGroup.<init>
  [21] io.netty.channel.nio.NioEventLoopGroup.<init>
  [22] org.apache.spark.network.util.NettyUtils.createEventLoop
  [23] org.apache.spark.network.client.TransportClientFactory.<init>
  [24] org.apache.spark.network.TransportContext.createClientFactory
  [25] org.apache.spark.rpc.netty.NettyRpcEnv.<init>
  [26] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [27] org.apache.spark.rpc.RpcEnv$.create
  [28] org.apache.spark.SparkEnv$.create
  [29] org.apache.spark.SparkEnv$.createDriverEnv
  [30] org.apache.spark.SparkContext.createSparkEnv
  [31] org.apache.spark.SparkContext.<init>
  [32] org.apache.spark.api.java.JavaSparkContext.<init>
  [33] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [34] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [35] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [36] java.lang.reflect.Constructor.newInstance
  [37] py4j.reflection.MethodInvoker.invoke
  [38] py4j.reflection.ReflectionEngine.invoke
  [39] py4j.Gateway.invoke
  [40] py4j.commands.ConstructorCommand.invokeConstructor
  [41] py4j.commands.ConstructorCommand.execute
  [42] py4j.GatewayConnection.run
  [43] java.lang.Thread.run
  [44] [tid=16146]

--- 1551158140422496 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.network.util.NettyUtils.createPooledByteBufAllocator
  [11] org.apache.spark.network.client.TransportClientFactory.<init>
  [12] org.apache.spark.network.TransportContext.createClientFactory
  [13] org.apache.spark.rpc.netty.NettyRpcEnv.<init>
  [14] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [15] org.apache.spark.rpc.RpcEnv$.create
  [16] org.apache.spark.SparkEnv$.create
  [17] org.apache.spark.SparkEnv$.createDriverEnv
  [18] org.apache.spark.SparkContext.createSparkEnv
  [19] org.apache.spark.SparkContext.<init>
  [20] org.apache.spark.api.java.JavaSparkContext.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.ConstructorCommand.invokeConstructor
  [29] py4j.commands.ConstructorCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158140522580 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] io.netty.channel.AbstractChannel.newChannelPipeline
  [16] io.netty.channel.AbstractChannel.<init>
  [17] io.netty.channel.nio.AbstractNioChannel.<init>
  [18] io.netty.channel.nio.AbstractNioMessageChannel.<init>
  [19] io.netty.channel.socket.nio.NioServerSocketChannel.<init>
  [20] io.netty.channel.socket.nio.NioServerSocketChannel.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] io.netty.channel.ReflectiveChannelFactory.newChannel
  [26] io.netty.bootstrap.AbstractBootstrap.initAndRegister
  [27] io.netty.bootstrap.AbstractBootstrap.doBind
  [28] io.netty.bootstrap.AbstractBootstrap.bind
  [29] org.apache.spark.network.server.TransportServer.init
  [30] org.apache.spark.network.server.TransportServer.<init>
  [31] org.apache.spark.network.TransportContext.createServer
  [32] org.apache.spark.rpc.netty.NettyRpcEnv.startServer
  [33] org.apache.spark.rpc.netty.NettyRpcEnvFactory$$anonfun$4.apply
  [34] org.apache.spark.rpc.netty.NettyRpcEnvFactory$$anonfun$4.apply
  [35] org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp
  [36] scala.collection.immutable.Range.foreach$mVc$sp
  [37] org.apache.spark.util.Utils$.startServiceOnPort
  [38] org.apache.spark.rpc.netty.NettyRpcEnvFactory.create
  [39] org.apache.spark.rpc.RpcEnv$.create
  [40] org.apache.spark.SparkEnv$.create
  [41] org.apache.spark.SparkEnv$.createDriverEnv
  [42] org.apache.spark.SparkContext.createSparkEnv
  [43] org.apache.spark.SparkContext.<init>
  [44] org.apache.spark.api.java.JavaSparkContext.<init>
  [45] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [46] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [47] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [48] java.lang.reflect.Constructor.newInstance
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.ConstructorCommand.invokeConstructor
  [53] py4j.commands.ConstructorCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16146]

--- 1551158140642084 us
  [ 0] malloc
  [ 1] Symbol::operator new(unsigned long, int, Thread*)
  [ 2] SymbolTable::basic_add(ClassLoaderData*, constantPoolHandle, int, char const**, int*, int*, unsigned int*, Thread*)
  [ 3] SymbolTable::add(ClassLoaderData*, constantPoolHandle, int, char const**, int*, int*, unsigned int*, Thread*)
  [ 4] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 5] ClassFileParser::parse_constant_pool(Thread*)
  [ 6] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 7] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 8] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 9] JVM_DefineClassWithSource
  [10] Java_java_lang_ClassLoader_defineClass1
  [11] java.lang.ClassLoader.defineClass1
  [12] java.lang.ClassLoader.defineClass
  [13] java.security.SecureClassLoader.defineClass
  [14] java.net.URLClassLoader.defineClass
  [15] java.net.URLClassLoader.access$100
  [16] java.net.URLClassLoader$1.run
  [17] java.net.URLClassLoader$1.run
  [18] java.security.AccessController.doPrivileged
  [19] java.net.URLClassLoader.findClass
  [20] java.lang.ClassLoader.loadClass
  [21] sun.misc.Launcher$AppClassLoader.loadClass
  [22] java.lang.ClassLoader.loadClass
  [23] org.apache.spark.SparkEnv$.create
  [24] org.apache.spark.SparkEnv$.createDriverEnv
  [25] org.apache.spark.SparkContext.createSparkEnv
  [26] org.apache.spark.SparkContext.<init>
  [27] org.apache.spark.api.java.JavaSparkContext.<init>
  [28] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [30] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [31] java.lang.reflect.Constructor.newInstance
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.ConstructorCommand.invokeConstructor
  [36] py4j.commands.ConstructorCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16146]

--- 1551158140766493 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.math.BigDecimal$.exact
  [10] scala.math.BigDecimal$.exact
  [11] scala.math.BigDecimal$.apply
  [12] org.apache.spark.util.Utils$.bytesToString
  [13] org.apache.spark.util.Utils$.bytesToString
  [14] org.apache.spark.storage.memory.MemoryStore$$anonfun$2.apply
  [15] org.apache.spark.storage.memory.MemoryStore$$anonfun$2.apply
  [16] org.apache.spark.internal.Logging$class.logInfo
  [17] org.apache.spark.storage.memory.MemoryStore.logInfo
  [18] org.apache.spark.storage.memory.MemoryStore.<init>
  [19] org.apache.spark.storage.BlockManager.<init>
  [20] org.apache.spark.SparkEnv$.create
  [21] org.apache.spark.SparkEnv$.createDriverEnv
  [22] org.apache.spark.SparkContext.createSparkEnv
  [23] org.apache.spark.SparkContext.<init>
  [24] org.apache.spark.api.java.JavaSparkContext.<init>
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [26] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [27] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [28] java.lang.reflect.Constructor.newInstance
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.ConstructorCommand.invokeConstructor
  [33] py4j.commands.ConstructorCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158140875218 us
  [ 0] scala.collection.TraversableLike$$anonfun$filterImpl$1.apply
  [ 1] scala.collection.IndexedSeqOptimized$class.foreach
  [ 2] scala.collection.immutable.StringOps.foreach
  [ 3] scala.collection.TraversableLike$class.filterImpl
  [ 4] scala.collection.TraversableLike$class.filter
  [ 5] scala.collection.immutable.StringOps.filter
  [ 6] org.apache.spark.ui.jobs.AllJobsPage.<init>
  [ 7] org.apache.spark.ui.jobs.JobsTab.<init>
  [ 8] org.apache.spark.ui.SparkUI.initialize
  [ 9] org.apache.spark.ui.SparkUI.<init>
  [10] org.apache.spark.ui.SparkUI$.create
  [11] org.apache.spark.SparkContext.<init>
  [12] org.apache.spark.api.java.JavaSparkContext.<init>
  [13] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [15] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [16] java.lang.reflect.Constructor.newInstance
  [17] py4j.reflection.MethodInvoker.invoke
  [18] py4j.reflection.ReflectionEngine.invoke
  [19] py4j.Gateway.invoke
  [20] py4j.commands.ConstructorCommand.invokeConstructor
  [21] py4j.commands.ConstructorCommand.execute
  [22] py4j.GatewayConnection.run
  [23] java.lang.Thread.run
  [24] [tid=16146]

--- 1551158140975135 us
  [ 0] __vdso_clock_gettime
  [ 1] __GI___clock_gettime
  [ 2] [unknown]
  [ 3] java.lang.ClassLoader.findBootstrapClass
  [ 4] java.lang.ClassLoader.findBootstrapClassOrNull
  [ 5] java.lang.ClassLoader.loadClass
  [ 6] java.lang.ClassLoader.loadClass
  [ 7] sun.misc.Launcher$AppClassLoader.loadClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] org.spark_project.jetty.servlet.ServletContextHandler.newServletHandler
  [10] org.spark_project.jetty.servlet.ServletContextHandler.getServletHandler
  [11] org.spark_project.jetty.servlet.ServletContextHandler.relinkHandlers
  [12] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [13] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [14] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [15] org.spark_project.jetty.servlet.ServletContextHandler.<init>
  [16] org.apache.spark.ui.JettyUtils$.createServletHandler
  [17] org.apache.spark.ui.JettyUtils$.createServletHandler
  [18] org.apache.spark.ui.WebUI.attachPage
  [19] org.apache.spark.ui.WebUI$$anonfun$attachTab$1.apply
  [20] org.apache.spark.ui.WebUI$$anonfun$attachTab$1.apply
  [21] scala.collection.mutable.ResizableArray$class.foreach
  [22] scala.collection.mutable.ArrayBuffer.foreach
  [23] org.apache.spark.ui.WebUI.attachTab
  [24] org.apache.spark.ui.SparkUI.initialize
  [25] org.apache.spark.ui.SparkUI.<init>
  [26] org.apache.spark.ui.SparkUI$.create
  [27] org.apache.spark.SparkContext.<init>
  [28] org.apache.spark.api.java.JavaSparkContext.<init>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.ConstructorCommand.invokeConstructor
  [37] py4j.commands.ConstructorCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16146]

--- 1551158141076411 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.spark_project.jetty.http.HttpGenerator.<clinit>
  [10] org.spark_project.jetty.server.Server.doStart
  [11] org.spark_project.jetty.util.component.AbstractLifeCycle.start
  [12] org.apache.spark.ui.JettyUtils$.startJettyServer
  [13] org.apache.spark.ui.WebUI.bind
  [14] org.apache.spark.SparkContext$$anonfun$11.apply
  [15] org.apache.spark.SparkContext$$anonfun$11.apply
  [16] scala.Option.foreach
  [17] org.apache.spark.SparkContext.<init>
  [18] org.apache.spark.api.java.JavaSparkContext.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.ConstructorCommand.invokeConstructor
  [27] py4j.commands.ConstructorCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158141177581 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] org.spark_project.jetty.servlet.ServletHolder.getRegistration
  [13] org.spark_project.jetty.servlet.ServletHolder.initMultiPart
  [14] org.spark_project.jetty.servlet.ServletHolder.initServlet
  [15] org.spark_project.jetty.servlet.ServletHolder.initialize
  [16] org.spark_project.jetty.servlet.ServletHandler.initialize
  [17] org.spark_project.jetty.servlet.ServletContextHandler.startContext
  [18] org.spark_project.jetty.server.handler.ContextHandler.doStart
  [19] org.spark_project.jetty.servlet.ServletContextHandler.doStart
  [20] org.spark_project.jetty.util.component.AbstractLifeCycle.start
  [21] org.spark_project.jetty.util.component.ContainerLifeCycle.start
  [22] org.spark_project.jetty.util.component.ContainerLifeCycle.doStart
  [23] org.spark_project.jetty.server.handler.AbstractHandler.doStart
  [24] org.spark_project.jetty.server.handler.gzip.GzipHandler.doStart
  [25] org.spark_project.jetty.util.component.AbstractLifeCycle.start
  [26] org.apache.spark.ui.JettyUtils$$anonfun$startJettyServer$1.apply
  [27] org.apache.spark.ui.JettyUtils$$anonfun$startJettyServer$1.apply
  [28] scala.collection.mutable.ResizableArray$class.foreach
  [29] scala.collection.mutable.ArrayBuffer.foreach
  [30] org.apache.spark.ui.JettyUtils$.startJettyServer
  [31] org.apache.spark.ui.WebUI.bind
  [32] org.apache.spark.SparkContext$$anonfun$11.apply
  [33] org.apache.spark.SparkContext$$anonfun$11.apply
  [34] scala.Option.foreach
  [35] org.apache.spark.SparkContext.<init>
  [36] org.apache.spark.api.java.JavaSparkContext.<init>
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [39] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [40] java.lang.reflect.Constructor.newInstance
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.ConstructorCommand.invokeConstructor
  [45] py4j.commands.ConstructorCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16146]

--- 1551158141291617 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.Enumeration$ValueSet$.<init>
  [10] scala.Enumeration.ValueSet$lzycompute
  [11] scala.Enumeration.ValueSet
  [12] scala.Enumeration.values
  [13] scala.Enumeration.withName
  [14] org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree1$1
  [15] org.apache.spark.scheduler.TaskSchedulerImpl.<init>
  [16] org.apache.spark.scheduler.TaskSchedulerImpl.<init>
  [17] org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler
  [18] org.apache.spark.SparkContext.<init>
  [19] org.apache.spark.api.java.JavaSparkContext.<init>
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [22] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [23] java.lang.reflect.Constructor.newInstance
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.ConstructorCommand.invokeConstructor
  [28] py4j.commands.ConstructorCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158141406464 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.collection.mutable.Builder$class.mapResult
  [10] scala.collection.mutable.ArrayBuffer.mapResult
  [11] scala.collection.mutable.ArraySeq$.newBuilder
  [12] scala.FallbackArrayBuilding$$anon$1.apply
  [13] scala.collection.TraversableLike$class.builder$1
  [14] scala.collection.TraversableLike$class.map
  [15] scala.collection.mutable.ArrayOps$ofRef.map
  [16] org.apache.spark.util.Utils$.sparkJavaOpts
  [17] org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start
  [18] org.apache.spark.scheduler.TaskSchedulerImpl.start
  [19] org.apache.spark.SparkContext.<init>
  [20] org.apache.spark.api.java.JavaSparkContext.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.ConstructorCommand.invokeConstructor
  [29] py4j.commands.ConstructorCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158141660621 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] io.netty.buffer.PoolArena$HeapArena.newChunk
  [10] io.netty.buffer.PoolArena.allocateNormal
  [11] io.netty.buffer.PoolArena.allocate
  [12] io.netty.buffer.PoolArena.allocate
  [13] io.netty.buffer.PooledByteBufAllocator.newHeapBuffer
  [14] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [15] io.netty.buffer.AbstractByteBufAllocator.heapBuffer
  [16] org.apache.spark.network.protocol.MessageEncoder.encode
  [17] org.apache.spark.network.protocol.MessageEncoder.encode
  [18] io.netty.handler.codec.MessageToMessageEncoder.write
  [19] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [20] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [21] io.netty.channel.AbstractChannelHandlerContext.write
  [22] io.netty.channel.AbstractChannelHandlerContext.write
  [23] io.netty.handler.timeout.IdleStateHandler.write
  [24] io.netty.channel.AbstractChannelHandlerContext.invokeWrite0
  [25] io.netty.channel.AbstractChannelHandlerContext.invokeWrite
  [26] io.netty.channel.AbstractChannelHandlerContext.access$1900
  [27] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write
  [28] io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write
  [29] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run
  [30] io.netty.util.concurrent.AbstractEventExecutor.safeExecute
  [31] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks
  [32] io.netty.channel.nio.NioEventLoop.run
  [33] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [34] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [35] java.lang.Thread.run
  [36] [rpc-client-1-1 tid=16232]

--- 1551158141765032 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClass
  [ 7] Unsafe_DefineClass_impl(JNIEnv_*, _jstring*, _jbyteArray*, int, int, _jobject*, _jobject*)
  [ 8] Unsafe_DefineClass
  [ 9] sun.misc.Unsafe.defineClass
  [10] sun.reflect.ClassDefiner.defineClass
  [11] sun.reflect.MethodAccessorGenerator$1.run
  [12] sun.reflect.MethodAccessorGenerator$1.run
  [13] java.security.AccessController.doPrivileged
  [14] sun.reflect.MethodAccessorGenerator.generate
  [15] sun.reflect.MethodAccessorGenerator.generateSerializationConstructor
  [16] sun.reflect.ReflectionFactory.generateConstructor
  [17] sun.reflect.ReflectionFactory.newConstructorForSerialization
  [18] java.io.ObjectStreamClass.getSerializableConstructor
  [19] java.io.ObjectStreamClass.access$1500
  [20] java.io.ObjectStreamClass$3.run
  [21] java.io.ObjectStreamClass$3.run
  [22] java.security.AccessController.doPrivileged
  [23] java.io.ObjectStreamClass.<init>
  [24] java.io.ObjectStreamClass.lookup
  [25] java.io.ObjectOutputStream.writeObject0
  [26] java.io.ObjectOutputStream.defaultWriteFields
  [27] java.io.ObjectOutputStream.writeSerialData
  [28] java.io.ObjectOutputStream.writeOrdinaryObject
  [29] java.io.ObjectOutputStream.writeObject0
  [30] java.io.ObjectOutputStream.defaultWriteFields
  [31] java.io.ObjectOutputStream.writeSerialData
  [32] java.io.ObjectOutputStream.writeOrdinaryObject
  [33] java.io.ObjectOutputStream.writeObject0
  [34] java.io.ObjectOutputStream.writeObject
  [35] org.apache.spark.serializer.JavaSerializationStream.writeObject
  [36] org.apache.spark.rpc.netty.RequestMessage.serialize
  [37] org.apache.spark.rpc.netty.NettyRpcEnv.send
  [38] org.apache.spark.rpc.netty.NettyRpcEndpointRef.send
  [39] org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1.run
  [40] java.util.concurrent.Executors$RunnableAdapter.call
  [41] java.util.concurrent.FutureTask.run
  [42] java.util.concurrent.ThreadPoolExecutor.runWorker
  [43] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [44] java.lang.Thread.run
  [45] [tid=16225]

--- 1551158141902640 us
  [ 0] __unqueue_futex_[k]
  [ 1] mark_wake_futex_[k]
  [ 2] futex_wake_[k]
  [ 3] do_futex_[k]
  [ 4] sys_futex_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] __pthread_cond_signal
  [ 8] org.spark_project.guava.cache.LocalCache$AccessQueue.<init>
  [ 9] org.spark_project.guava.cache.LocalCache$Segment.<init>
  [10] org.spark_project.guava.cache.LocalCache.createSegment
  [11] org.spark_project.guava.cache.LocalCache.<init>
  [12] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.<init>
  [13] org.spark_project.guava.cache.CacheBuilder.build
  [14] org.apache.spark.storage.BlockManagerId$.<init>
  [15] org.apache.spark.storage.BlockManagerId$.<clinit>
  [16] org.apache.spark.storage.BlockManager.initialize
  [17] org.apache.spark.SparkContext.<init>
  [18] org.apache.spark.api.java.JavaSparkContext.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.ConstructorCommand.invokeConstructor
  [27] py4j.commands.ConstructorCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158141903773 us
  [ 0] frame::sender(RegisterMap*) const
  [ 1] SharedRuntime::handle_ic_miss_helper(JavaThread*, Thread*)
  [ 2] SharedRuntime::handle_wrong_method_ic_miss(JavaThread*)
  [ 3] java.io.DataInputStream.readInt
  [ 4] java.io.ObjectInputStream$BlockDataInputStream.readInt
  [ 5] java.io.ObjectInputStream.readInt
  [ 6] scala.collection.mutable.HashTable$class.init
  [ 7] scala.collection.mutable.HashMap.init
  [ 8] scala.collection.mutable.HashMap.readObject
  [ 9] sun.reflect.NativeMethodAccessorImpl.invoke0
  [10] sun.reflect.NativeMethodAccessorImpl.invoke
  [11] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [12] java.lang.reflect.Method.invoke
  [13] java.io.ObjectStreamClass.invokeReadObject
  [14] java.io.ObjectInputStream.readSerialData
  [15] java.io.ObjectInputStream.readOrdinaryObject
  [16] java.io.ObjectInputStream.readObject0
  [17] java.io.ObjectInputStream.defaultReadFields
  [18] java.io.ObjectInputStream.readSerialData
  [19] java.io.ObjectInputStream.readOrdinaryObject
  [20] java.io.ObjectInputStream.readObject0
  [21] java.io.ObjectInputStream.defaultReadFields
  [22] java.io.ObjectInputStream.readSerialData
  [23] java.io.ObjectInputStream.readOrdinaryObject
  [24] java.io.ObjectInputStream.readObject0
  [25] java.io.ObjectInputStream.defaultReadFields
  [26] java.io.ObjectInputStream.readSerialData
  [27] java.io.ObjectInputStream.readOrdinaryObject
  [28] java.io.ObjectInputStream.readObject0
  [29] java.io.ObjectInputStream.readObject
  [30] org.apache.spark.serializer.JavaDeserializationStream.readObject
  [31] org.apache.spark.serializer.JavaSerializerInstance.deserialize
  [32] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply
  [33] scala.util.DynamicVariable.withValue
  [34] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [35] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply
  [36] scala.util.DynamicVariable.withValue
  [37] org.apache.spark.rpc.netty.NettyRpcEnv.deserialize
  [38] org.apache.spark.rpc.netty.RequestMessage$.apply
  [39] org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive
  [40] org.apache.spark.rpc.netty.NettyRpcHandler.receive
  [41] org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage
  [42] org.apache.spark.network.server.TransportRequestHandler.handle
  [43] org.apache.spark.network.server.TransportChannelHandler.channelRead
  [44] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [45] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [46] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [47] io.netty.handler.timeout.IdleStateHandler.channelRead
  [48] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [49] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [50] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [51] io.netty.handler.codec.MessageToMessageDecoder.channelRead
  [52] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [53] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [54] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [55] org.apache.spark.network.util.TransportFrameDecoder.channelRead
  [56] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [57] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [58] io.netty.channel.AbstractChannelHandlerContext.fireChannelRead
  [59] io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead
  [60] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [61] io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead
  [62] io.netty.channel.DefaultChannelPipeline.fireChannelRead
  [63] io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read
  [64] io.netty.channel.nio.NioEventLoop.processSelectedKey
  [65] io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized
  [66] io.netty.channel.nio.NioEventLoop.processSelectedKeys
  [67] io.netty.channel.nio.NioEventLoop.run
  [68] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [69] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [70] java.lang.Thread.run
  [71] [rpc-client-1-1 tid=16232]

--- 1551158142023867 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] com.fasterxml.jackson.databind.ObjectMapper.<clinit>
  [13] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [19] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [20] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [21] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [22] scala.collection.mutable.HashTable$class.foreachEntry
  [23] scala.collection.mutable.HashMap.foreachEntry
  [24] scala.collection.mutable.HashMap.foreach
  [25] org.apache.spark.metrics.MetricsSystem.registerSinks
  [26] org.apache.spark.metrics.MetricsSystem.start
  [27] org.apache.spark.SparkContext.<init>
  [28] org.apache.spark.api.java.JavaSparkContext.<init>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.ConstructorCommand.invokeConstructor
  [37] py4j.commands.ConstructorCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16146]

--- 1551158142124028 us
  [ 0] inflate
  [ 1] Java_java_util_zip_Inflater_inflateBytes
  [ 2] java.util.zip.Inflater.inflateBytes
  [ 3] java.util.zip.Inflater.inflate
  [ 4] java.util.zip.InflaterInputStream.read
  [ 5] sun.misc.Resource.getBytes
  [ 6] java.net.URLClassLoader.defineClass
  [ 7] java.net.URLClassLoader.access$100
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.<clinit>
  [16] com.fasterxml.jackson.databind.ObjectMapper.defaultClassIntrospector
  [17] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [18] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [19] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [22] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [23] java.lang.reflect.Constructor.newInstance
  [24] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [25] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [26] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [27] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [28] scala.collection.mutable.HashTable$class.foreachEntry
  [29] scala.collection.mutable.HashMap.foreachEntry
  [30] scala.collection.mutable.HashMap.foreach
  [31] org.apache.spark.metrics.MetricsSystem.registerSinks
  [32] org.apache.spark.metrics.MetricsSystem.start
  [33] org.apache.spark.SparkContext.<init>
  [34] org.apache.spark.api.java.JavaSparkContext.<init>
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [37] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [38] java.lang.reflect.Constructor.newInstance
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.ConstructorCommand.invokeConstructor
  [43] py4j.commands.ConstructorCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16146]

--- 1551158142223791 us
  [ 0] java.security.CodeSource.hashCode
  [ 1] java.util.HashMap.getNode
  [ 2] java.util.HashMap.get
  [ 3] java.security.SecureClassLoader.getProtectionDomain
  [ 4] java.security.SecureClassLoader.defineClass
  [ 5] java.net.URLClassLoader.defineClass
  [ 6] java.net.URLClassLoader.access$100
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [15] com.fasterxml.jackson.databind.ObjectMapper.<init>
  [16] org.apache.spark.metrics.sink.MetricsServlet.<init>
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [18] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [19] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [20] java.lang.reflect.Constructor.newInstance
  [21] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [22] org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply
  [23] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [24] scala.collection.mutable.HashMap$$anonfun$foreach$1.apply
  [25] scala.collection.mutable.HashTable$class.foreachEntry
  [26] scala.collection.mutable.HashMap.foreachEntry
  [27] scala.collection.mutable.HashMap.foreach
  [28] org.apache.spark.metrics.MetricsSystem.registerSinks
  [29] org.apache.spark.metrics.MetricsSystem.start
  [30] org.apache.spark.SparkContext.<init>
  [31] org.apache.spark.api.java.JavaSparkContext.<init>
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [33] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [34] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [35] java.lang.reflect.Constructor.newInstance
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.ConstructorCommand.invokeConstructor
  [40] py4j.commands.ConstructorCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158142330588 us
  [ 0] org.apache.hadoop.conf.Configuration.loadResource
  [ 1] org.apache.hadoop.conf.Configuration.loadResources
  [ 2] org.apache.hadoop.conf.Configuration.getProps
  [ 3] org.apache.hadoop.conf.Configuration.get
  [ 4] org.apache.hadoop.conf.Configuration.getTrimmed
  [ 5] org.apache.hadoop.conf.Configuration.getBoolean
  [ 6] org.apache.hadoop.fs.FileSystem.get
  [ 7] org.apache.spark.util.Utils$.getHadoopFileSystem
  [ 8] org.apache.spark.scheduler.EventLoggingListener.<init>
  [ 9] org.apache.spark.SparkContext.<init>
  [10] org.apache.spark.api.java.JavaSparkContext.<init>
  [11] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [13] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [14] java.lang.reflect.Constructor.newInstance
  [15] py4j.reflection.MethodInvoker.invoke
  [16] py4j.reflection.ReflectionEngine.invoke
  [17] py4j.Gateway.invoke
  [18] py4j.commands.ConstructorCommand.invokeConstructor
  [19] py4j.commands.ConstructorCommand.execute
  [20] py4j.GatewayConnection.run
  [21] java.lang.Thread.run
  [22] [tid=16146]

--- 1551158142439824 us
  [ 0] ClassFileParser::valid_klass_reference_at(int)
  [ 1] ClassFileParser::parse_constant_pool(Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 4] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 5] JVM_DefineClassWithSource
  [ 6] Java_java_lang_ClassLoader_defineClass1
  [ 7] java.lang.ClassLoader.defineClass1
  [ 8] java.lang.ClassLoader.defineClass
  [ 9] java.security.SecureClassLoader.defineClass
  [10] java.net.URLClassLoader.defineClass
  [11] java.net.URLClassLoader.access$100
  [12] java.net.URLClassLoader$1.run
  [13] java.net.URLClassLoader$1.run
  [14] java.security.AccessController.doPrivileged
  [15] java.net.URLClassLoader.findClass
  [16] java.lang.ClassLoader.loadClass
  [17] sun.misc.Launcher$AppClassLoader.loadClass
  [18] java.lang.ClassLoader.loadClass
  [19] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [22] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [23] java.lang.reflect.Constructor.newInstance
  [24] java.lang.Class.newInstance
  [25] java.util.ServiceLoader$LazyIterator.nextService
  [26] java.util.ServiceLoader$LazyIterator.next
  [27] java.util.ServiceLoader$1.next
  [28] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [29] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [30] org.apache.hadoop.fs.FileSystem.createFileSystem
  [31] org.apache.hadoop.fs.FileSystem.access$200
  [32] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [33] org.apache.hadoop.fs.FileSystem$Cache.get
  [34] org.apache.hadoop.fs.FileSystem.get
  [35] org.apache.spark.util.Utils$.getHadoopFileSystem
  [36] org.apache.spark.scheduler.EventLoggingListener.<init>
  [37] org.apache.spark.SparkContext.<init>
  [38] org.apache.spark.api.java.JavaSparkContext.<init>
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [40] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [41] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [42] java.lang.reflect.Constructor.newInstance
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.ConstructorCommand.invokeConstructor
  [47] py4j.commands.ConstructorCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16146]

--- 1551158142558500 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.codehaus.jackson.map.ObjectMapper.<init>
  [13] org.codehaus.jackson.map.ObjectMapper.<init>
  [14] org.codehaus.jackson.map.ObjectMapper.<init>
  [15] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [18] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [19] java.lang.reflect.Constructor.newInstance
  [20] java.lang.Class.newInstance
  [21] java.util.ServiceLoader$LazyIterator.nextService
  [22] java.util.ServiceLoader$LazyIterator.next
  [23] java.util.ServiceLoader$1.next
  [24] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [25] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [26] org.apache.hadoop.fs.FileSystem.createFileSystem
  [27] org.apache.hadoop.fs.FileSystem.access$200
  [28] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [29] org.apache.hadoop.fs.FileSystem$Cache.get
  [30] org.apache.hadoop.fs.FileSystem.get
  [31] org.apache.spark.util.Utils$.getHadoopFileSystem
  [32] org.apache.spark.scheduler.EventLoggingListener.<init>
  [33] org.apache.spark.SparkContext.<init>
  [34] org.apache.spark.api.java.JavaSparkContext.<init>
  [35] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [37] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [38] java.lang.reflect.Constructor.newInstance
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.ConstructorCommand.invokeConstructor
  [43] py4j.commands.ConstructorCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16146]

--- 1551158142659793 us
  [ 0] org.codehaus.jackson.map.type.TypeBindings.<clinit>
  [ 1] org.codehaus.jackson.map.type.TypeFactory._constructType
  [ 2] org.codehaus.jackson.map.type.TypeFactory.constructType
  [ 3] org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers.add
  [ 4] org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers.<init>
  [ 5] org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers.<clinit>
  [ 6] org.codehaus.jackson.map.deser.BasicDeserializerFactory.<clinit>
  [ 7] org.codehaus.jackson.map.deser.StdDeserializerProvider.<init>
  [ 8] org.codehaus.jackson.map.ObjectMapper.<init>
  [ 9] org.codehaus.jackson.map.ObjectMapper.<init>
  [10] org.codehaus.jackson.map.ObjectMapper.<init>
  [11] org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [13] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [14] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [15] java.lang.reflect.Constructor.newInstance
  [16] java.lang.Class.newInstance
  [17] java.util.ServiceLoader$LazyIterator.nextService
  [18] java.util.ServiceLoader$LazyIterator.next
  [19] java.util.ServiceLoader$1.next
  [20] org.apache.hadoop.fs.FileSystem.loadFileSystems
  [21] org.apache.hadoop.fs.FileSystem.getFileSystemClass
  [22] org.apache.hadoop.fs.FileSystem.createFileSystem
  [23] org.apache.hadoop.fs.FileSystem.access$200
  [24] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [25] org.apache.hadoop.fs.FileSystem$Cache.get
  [26] org.apache.hadoop.fs.FileSystem.get
  [27] org.apache.spark.util.Utils$.getHadoopFileSystem
  [28] org.apache.spark.scheduler.EventLoggingListener.<init>
  [29] org.apache.spark.SparkContext.<init>
  [30] org.apache.spark.api.java.JavaSparkContext.<init>
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [33] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [34] java.lang.reflect.Constructor.newInstance
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.ConstructorCommand.invokeConstructor
  [39] py4j.commands.ConstructorCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16146]

--- 1551158142762486 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [10] org.apache.hadoop.fs.FileSystem.createFileSystem
  [11] org.apache.hadoop.fs.FileSystem.access$200
  [12] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [13] org.apache.hadoop.fs.FileSystem$Cache.get
  [14] org.apache.hadoop.fs.FileSystem.get
  [15] org.apache.spark.util.Utils$.getHadoopFileSystem
  [16] org.apache.spark.scheduler.EventLoggingListener.<init>
  [17] org.apache.spark.SparkContext.<init>
  [18] org.apache.spark.api.java.JavaSparkContext.<init>
  [19] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [20] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [21] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [22] java.lang.reflect.Constructor.newInstance
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.ConstructorCommand.invokeConstructor
  [27] py4j.commands.ConstructorCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158142884230 us
  [ 0] constantPoolHandle::remove()
  [ 1] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 2] ClassVerifier::verify_class(Thread*)
  [ 3] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 4] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 5] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 6] InstanceKlass::initialize(Thread*)
  [ 7] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 8] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [ 9] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [11] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [12] org.apache.hadoop.hdfs.DFSClient.<init>
  [13] org.apache.hadoop.hdfs.DFSClient.<init>
  [14] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [15] org.apache.hadoop.fs.FileSystem.createFileSystem
  [16] org.apache.hadoop.fs.FileSystem.access$200
  [17] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [18] org.apache.hadoop.fs.FileSystem$Cache.get
  [19] org.apache.hadoop.fs.FileSystem.get
  [20] org.apache.spark.util.Utils$.getHadoopFileSystem
  [21] org.apache.spark.scheduler.EventLoggingListener.<init>
  [22] org.apache.spark.SparkContext.<init>
  [23] org.apache.spark.api.java.JavaSparkContext.<init>
  [24] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [25] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [26] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [27] java.lang.reflect.Constructor.newInstance
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.ConstructorCommand.invokeConstructor
  [32] py4j.commands.ConstructorCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158143067039 us
  [ 0] InstanceKlass::do_local_static_fields_impl(instanceKlassHandle, void (*)(fieldDescriptor*, Handle, Thread*), Handle, Thread*)
  [ 1] java_lang_Class::create_mirror(KlassHandle, Handle, Handle, Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 4] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 5] JVM_DefineClassWithSource
  [ 6] Java_java_lang_ClassLoader_defineClass1
  [ 7] java.lang.ClassLoader.defineClass1
  [ 8] java.lang.ClassLoader.defineClass
  [ 9] java.security.SecureClassLoader.defineClass
  [10] java.net.URLClassLoader.defineClass
  [11] java.net.URLClassLoader.access$100
  [12] java.net.URLClassLoader$1.run
  [13] java.net.URLClassLoader$1.run
  [14] java.security.AccessController.doPrivileged
  [15] java.net.URLClassLoader.findClass
  [16] java.lang.ClassLoader.loadClass
  [17] sun.misc.Launcher$AppClassLoader.loadClass
  [18] java.lang.ClassLoader.loadClass
  [19] java.lang.Class.getDeclaredMethods0
  [20] java.lang.Class.privateGetDeclaredMethods
  [21] java.lang.Class.privateGetPublicMethods
  [22] java.lang.Class.privateGetPublicMethods
  [23] java.lang.Class.getMethods
  [24] sun.misc.ProxyGenerator.generateClassFile
  [25] sun.misc.ProxyGenerator.generateProxyClass
  [26] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [27] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [28] java.lang.reflect.WeakCache$Factory.get
  [29] java.lang.reflect.WeakCache.get
  [30] java.lang.reflect.Proxy.getProxyClass0
  [31] java.lang.reflect.Proxy.newProxyInstance
  [32] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [33] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [34] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [35] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [36] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [37] org.apache.hadoop.hdfs.DFSClient.<init>
  [38] org.apache.hadoop.hdfs.DFSClient.<init>
  [39] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [40] org.apache.hadoop.fs.FileSystem.createFileSystem
  [41] org.apache.hadoop.fs.FileSystem.access$200
  [42] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [43] org.apache.hadoop.fs.FileSystem$Cache.get
  [44] org.apache.hadoop.fs.FileSystem.get
  [45] org.apache.spark.util.Utils$.getHadoopFileSystem
  [46] org.apache.spark.scheduler.EventLoggingListener.<init>
  [47] org.apache.spark.SparkContext.<init>
  [48] org.apache.spark.api.java.JavaSparkContext.<init>
  [49] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [50] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [51] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [52] java.lang.reflect.Constructor.newInstance
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.ConstructorCommand.invokeConstructor
  [57] py4j.commands.ConstructorCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16146]

--- 1551158143185397 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] java.lang.Class.getDeclaredMethods0
  [16] java.lang.Class.privateGetDeclaredMethods
  [17] java.lang.Class.privateGetPublicMethods
  [18] java.lang.Class.privateGetPublicMethods
  [19] java.lang.Class.getMethods
  [20] sun.misc.ProxyGenerator.generateClassFile
  [21] sun.misc.ProxyGenerator.generateProxyClass
  [22] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [23] java.lang.reflect.Proxy$ProxyClassFactory.apply
  [24] java.lang.reflect.WeakCache$Factory.get
  [25] java.lang.reflect.WeakCache.get
  [26] java.lang.reflect.Proxy.getProxyClass0
  [27] java.lang.reflect.Proxy.newProxyInstance
  [28] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [29] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [30] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [31] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [32] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [33] org.apache.hadoop.hdfs.DFSClient.<init>
  [34] org.apache.hadoop.hdfs.DFSClient.<init>
  [35] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [36] org.apache.hadoop.fs.FileSystem.createFileSystem
  [37] org.apache.hadoop.fs.FileSystem.access$200
  [38] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [39] org.apache.hadoop.fs.FileSystem$Cache.get
  [40] org.apache.hadoop.fs.FileSystem.get
  [41] org.apache.spark.util.Utils$.getHadoopFileSystem
  [42] org.apache.spark.scheduler.EventLoggingListener.<init>
  [43] org.apache.spark.SparkContext.<init>
  [44] org.apache.spark.api.java.JavaSparkContext.<init>
  [45] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [46] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [47] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [48] java.lang.reflect.Constructor.newInstance
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.ConstructorCommand.invokeConstructor
  [53] py4j.commands.ConstructorCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16146]

--- 1551158143291907 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [10] JVM_FindClassFromCaller
  [11] Java_java_lang_Class_forName0
  [12] java.lang.Class.forName0
  [13] java.lang.Class.forName
  [14] com.sun.proxy.$Proxy13.<clinit>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] java.lang.reflect.Proxy.newProxyInstance
  [20] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [21] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DFSClient.<init>
  [27] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [28] org.apache.hadoop.fs.FileSystem.createFileSystem
  [29] org.apache.hadoop.fs.FileSystem.access$200
  [30] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [31] org.apache.hadoop.fs.FileSystem$Cache.get
  [32] org.apache.hadoop.fs.FileSystem.get
  [33] org.apache.spark.util.Utils$.getHadoopFileSystem
  [34] org.apache.spark.scheduler.EventLoggingListener.<init>
  [35] org.apache.spark.SparkContext.<init>
  [36] org.apache.spark.api.java.JavaSparkContext.<init>
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [39] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [40] java.lang.reflect.Constructor.newInstance
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.ConstructorCommand.invokeConstructor
  [45] py4j.commands.ConstructorCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16146]

--- 1551158143391645 us
  [ 0] __pthread_mutex_unlock_usercnt
  [ 1] ChunkPool::allocate(unsigned long, AllocFailStrategy::AllocFailEnum)
  [ 2] Arena::grow(unsigned long, AllocFailStrategy::AllocFailEnum)
  [ 3] ClassFileParser::parse_method(bool, AccessFlags*, Thread*)
  [ 4] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 5] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 6] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 7] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 8] JVM_DefineClassWithSource
  [ 9] Java_java_lang_ClassLoader_defineClass1
  [10] java.lang.ClassLoader.defineClass1
  [11] java.lang.ClassLoader.defineClass
  [12] java.security.SecureClassLoader.defineClass
  [13] java.net.URLClassLoader.defineClass
  [14] java.net.URLClassLoader.access$100
  [15] java.net.URLClassLoader$1.run
  [16] java.net.URLClassLoader$1.run
  [17] java.security.AccessController.doPrivileged
  [18] java.net.URLClassLoader.findClass
  [19] java.lang.ClassLoader.loadClass
  [20] sun.misc.Launcher$AppClassLoader.loadClass
  [21] java.lang.ClassLoader.loadClass
  [22] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto.initFields
  [23] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto.<clinit>
  [24] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto.initFields
  [25] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto.<clinit>
  [26] java.lang.Class.forName0
  [27] java.lang.Class.forName
  [28] com.sun.proxy.$Proxy13.<clinit>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] java.lang.reflect.Proxy.newProxyInstance
  [34] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [35] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [36] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [37] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [38] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [39] org.apache.hadoop.hdfs.DFSClient.<init>
  [40] org.apache.hadoop.hdfs.DFSClient.<init>
  [41] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [42] org.apache.hadoop.fs.FileSystem.createFileSystem
  [43] org.apache.hadoop.fs.FileSystem.access$200
  [44] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [45] org.apache.hadoop.fs.FileSystem$Cache.get
  [46] org.apache.hadoop.fs.FileSystem.get
  [47] org.apache.spark.util.Utils$.getHadoopFileSystem
  [48] org.apache.spark.scheduler.EventLoggingListener.<init>
  [49] org.apache.spark.SparkContext.<init>
  [50] org.apache.spark.api.java.JavaSparkContext.<init>
  [51] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [52] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [53] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [54] java.lang.reflect.Constructor.newInstance
  [55] py4j.reflection.MethodInvoker.invoke
  [56] py4j.reflection.ReflectionEngine.invoke
  [57] py4j.Gateway.invoke
  [58] py4j.commands.ConstructorCommand.invokeConstructor
  [59] py4j.commands.ConstructorCommand.execute
  [60] py4j.GatewayConnection.run
  [61] java.lang.Thread.run
  [62] [tid=16146]

--- 1551158143493288 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto.initFields
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto.<clinit>
  [13] java.lang.Class.forName0
  [14] java.lang.Class.forName
  [15] com.sun.proxy.$Proxy13.<clinit>
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [17] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [18] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [19] java.lang.reflect.Constructor.newInstance
  [20] java.lang.reflect.Proxy.newProxyInstance
  [21] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [22] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [25] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [26] org.apache.hadoop.hdfs.DFSClient.<init>
  [27] org.apache.hadoop.hdfs.DFSClient.<init>
  [28] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [29] org.apache.hadoop.fs.FileSystem.createFileSystem
  [30] org.apache.hadoop.fs.FileSystem.access$200
  [31] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [32] org.apache.hadoop.fs.FileSystem$Cache.get
  [33] org.apache.hadoop.fs.FileSystem.get
  [34] org.apache.spark.util.Utils$.getHadoopFileSystem
  [35] org.apache.spark.scheduler.EventLoggingListener.<init>
  [36] org.apache.spark.SparkContext.<init>
  [37] org.apache.spark.api.java.JavaSparkContext.<init>
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [40] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [41] java.lang.reflect.Constructor.newInstance
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.ConstructorCommand.invokeConstructor
  [46] py4j.commands.ConstructorCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158143593017 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy13.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [20] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [27] org.apache.hadoop.fs.FileSystem.createFileSystem
  [28] org.apache.hadoop.fs.FileSystem.access$200
  [29] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [30] org.apache.hadoop.fs.FileSystem$Cache.get
  [31] org.apache.hadoop.fs.FileSystem.get
  [32] org.apache.spark.util.Utils$.getHadoopFileSystem
  [33] org.apache.spark.scheduler.EventLoggingListener.<init>
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16146]

--- 1551158143694246 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [10] JVM_FindClassFromCaller
  [11] Java_java_lang_Class_forName0
  [12] java.lang.Class.forName0
  [13] java.lang.Class.forName
  [14] com.sun.proxy.$Proxy13.<clinit>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] java.lang.reflect.Proxy.newProxyInstance
  [20] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [21] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [24] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DFSClient.<init>
  [27] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [28] org.apache.hadoop.fs.FileSystem.createFileSystem
  [29] org.apache.hadoop.fs.FileSystem.access$200
  [30] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [31] org.apache.hadoop.fs.FileSystem$Cache.get
  [32] org.apache.hadoop.fs.FileSystem.get
  [33] org.apache.spark.util.Utils$.getHadoopFileSystem
  [34] org.apache.spark.scheduler.EventLoggingListener.<init>
  [35] org.apache.spark.SparkContext.<init>
  [36] org.apache.spark.api.java.JavaSparkContext.<init>
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [39] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [40] java.lang.reflect.Constructor.newInstance
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.ConstructorCommand.invokeConstructor
  [45] py4j.commands.ConstructorCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16146]

--- 1551158143794475 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] com.sun.proxy.$Proxy13.<clinit>
  [14] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [16] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [17] java.lang.reflect.Constructor.newInstance
  [18] java.lang.reflect.Proxy.newProxyInstance
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy
  [20] org.apache.hadoop.ipc.RPC.getProtocolProxy
  [21] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [22] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [23] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [24] org.apache.hadoop.hdfs.DFSClient.<init>
  [25] org.apache.hadoop.hdfs.DFSClient.<init>
  [26] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [27] org.apache.hadoop.fs.FileSystem.createFileSystem
  [28] org.apache.hadoop.fs.FileSystem.access$200
  [29] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [30] org.apache.hadoop.fs.FileSystem$Cache.get
  [31] org.apache.hadoop.fs.FileSystem.get
  [32] org.apache.spark.util.Utils$.getHadoopFileSystem
  [33] org.apache.spark.scheduler.EventLoggingListener.<init>
  [34] org.apache.spark.SparkContext.<init>
  [35] org.apache.spark.api.java.JavaSparkContext.<init>
  [36] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [37] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [38] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [39] java.lang.reflect.Constructor.newInstance
  [40] py4j.reflection.MethodInvoker.invoke
  [41] py4j.reflection.ReflectionEngine.invoke
  [42] py4j.Gateway.invoke
  [43] py4j.commands.ConstructorCommand.invokeConstructor
  [44] py4j.commands.ConstructorCommand.execute
  [45] py4j.GatewayConnection.run
  [46] java.lang.Thread.run
  [47] [tid=16146]

--- 1551158143904664 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructor0
  [12] java.lang.Class.getConstructor
  [13] java.lang.reflect.Proxy.newProxyInstance
  [14] org.apache.hadoop.io.retry.RetryProxy.create
  [15] org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol
  [16] org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy
  [17] org.apache.hadoop.hdfs.NameNodeProxies.createProxy
  [18] org.apache.hadoop.hdfs.DFSClient.<init>
  [19] org.apache.hadoop.hdfs.DFSClient.<init>
  [20] org.apache.hadoop.hdfs.DistributedFileSystem.initialize
  [21] org.apache.hadoop.fs.FileSystem.createFileSystem
  [22] org.apache.hadoop.fs.FileSystem.access$200
  [23] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [24] org.apache.hadoop.fs.FileSystem$Cache.get
  [25] org.apache.hadoop.fs.FileSystem.get
  [26] org.apache.spark.util.Utils$.getHadoopFileSystem
  [27] org.apache.spark.scheduler.EventLoggingListener.<init>
  [28] org.apache.spark.SparkContext.<init>
  [29] org.apache.spark.api.java.JavaSparkContext.<init>
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [32] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [33] java.lang.reflect.Constructor.newInstance
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.ConstructorCommand.invokeConstructor
  [38] py4j.commands.ConstructorCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16146]

--- 1551158144038131 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequestHeader
  [13] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [14] com.sun.proxy.$Proxy13.getFileInfo
  [15] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [21] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [22] com.sun.proxy.$Proxy14.getFileInfo
  [23] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [24] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [25] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [26] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [27] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [28] org.apache.spark.scheduler.EventLoggingListener.start
  [29] org.apache.spark.SparkContext.<init>
  [30] org.apache.spark.api.java.JavaSparkContext.<init>
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [33] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [34] java.lang.reflect.Constructor.newInstance
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.ConstructorCommand.invokeConstructor
  [39] py4j.commands.ConstructorCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16146]

--- 1551158144143819 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto.newBuilder
  [13] org.apache.hadoop.util.ProtoUtil.makeIpcConnectionContext
  [14] org.apache.hadoop.ipc.Client$Connection.writeConnectionContext
  [15] org.apache.hadoop.ipc.Client$Connection.setupIOstreams
  [16] org.apache.hadoop.ipc.Client$Connection.access$2900
  [17] org.apache.hadoop.ipc.Client.getConnection
  [18] org.apache.hadoop.ipc.Client.call
  [19] org.apache.hadoop.ipc.Client.call
  [20] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [21] com.sun.proxy.$Proxy13.getFileInfo
  [22] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [28] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [29] com.sun.proxy.$Proxy14.getFileInfo
  [30] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [31] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [32] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [33] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [34] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [35] org.apache.spark.scheduler.EventLoggingListener.start
  [36] org.apache.spark.SparkContext.<init>
  [37] org.apache.spark.api.java.JavaSparkContext.<init>
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [40] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [41] java.lang.reflect.Constructor.newInstance
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.ConstructorCommand.invokeConstructor
  [46] py4j.commands.ConstructorCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158144307322 us
  [ 0] jni_GetArrayLength
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [21] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [22] com.sun.proxy.$Proxy14.getFileInfo
  [23] org.apache.hadoop.hdfs.DFSClient.getFileInfo
  [24] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [25] org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall
  [26] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [27] org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus
  [28] org.apache.spark.scheduler.EventLoggingListener.start
  [29] org.apache.spark.SparkContext.<init>
  [30] org.apache.spark.api.java.JavaSparkContext.<init>
  [31] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [32] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [33] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [34] java.lang.reflect.Constructor.newInstance
  [35] py4j.reflection.MethodInvoker.invoke
  [36] py4j.reflection.ReflectionEngine.invoke
  [37] py4j.Gateway.invoke
  [38] py4j.commands.ConstructorCommand.invokeConstructor
  [39] py4j.commands.ConstructorCommand.execute
  [40] py4j.GatewayConnection.run
  [41] java.lang.Thread.run
  [42] [tid=16146]

--- 1551158144412123 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocolPB.PBHelper$1.<clinit>
  [13] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [14] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [15] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [21] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [22] com.sun.proxy.$Proxy14.create
  [23] org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate
  [24] org.apache.hadoop.hdfs.DFSClient.create
  [25] org.apache.hadoop.hdfs.DFSClient.create
  [26] org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall
  [27] org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall
  [28] org.apache.hadoop.fs.FileSystemLinkResolver.resolve
  [29] org.apache.hadoop.hdfs.DistributedFileSystem.create
  [30] org.apache.hadoop.hdfs.DistributedFileSystem.create
  [31] org.apache.hadoop.fs.FileSystem.create
  [32] org.apache.hadoop.fs.FileSystem.create
  [33] org.apache.hadoop.fs.FileSystem.create
  [34] org.apache.hadoop.fs.FileSystem.create
  [35] org.apache.spark.scheduler.EventLoggingListener.start
  [36] org.apache.spark.SparkContext.<init>
  [37] org.apache.spark.api.java.JavaSparkContext.<init>
  [38] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [39] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [40] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [41] java.lang.reflect.Constructor.newInstance
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.ConstructorCommand.invokeConstructor
  [46] py4j.commands.ConstructorCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158144539907 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [12] org.apache.spark.scheduler.EventLoggingListener.start
  [13] org.apache.spark.SparkContext.<init>
  [14] org.apache.spark.api.java.JavaSparkContext.<init>
  [15] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [16] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [17] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [18] java.lang.reflect.Constructor.newInstance
  [19] py4j.reflection.MethodInvoker.invoke
  [20] py4j.reflection.ReflectionEngine.invoke
  [21] py4j.Gateway.invoke
  [22] py4j.commands.ConstructorCommand.invokeConstructor
  [23] py4j.commands.ConstructorCommand.execute
  [24] py4j.GatewayConnection.run
  [25] java.lang.Thread.run
  [26] [tid=16146]

--- 1551158144640680 us
  [ 0] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 1] JVM_DefineClassWithSource
  [ 2] Java_java_lang_ClassLoader_defineClass1
  [ 3] java.lang.ClassLoader.defineClass1
  [ 4] java.lang.ClassLoader.defineClass
  [ 5] java.security.SecureClassLoader.defineClass
  [ 6] java.net.URLClassLoader.defineClass
  [ 7] java.net.URLClassLoader.access$100
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] com.fasterxml.jackson.module.scala.ser.OptionSerializerModule$$anonfun$10.apply
  [16] com.fasterxml.jackson.module.scala.ser.OptionSerializerModule$$anonfun$10.apply
  [17] com.fasterxml.jackson.module.scala.JacksonModule$$anonfun$setupModule$1.apply
  [18] com.fasterxml.jackson.module.scala.JacksonModule$$anonfun$setupModule$1.apply
  [19] scala.collection.immutable.List.foreach
  [20] com.fasterxml.jackson.module.scala.JacksonModule$class.setupModule
  [21] com.fasterxml.jackson.module.scala.DefaultScalaModule.setupModule
  [22] com.fasterxml.jackson.databind.ObjectMapper.registerModule
  [23] org.apache.spark.util.JsonProtocol$.<init>
  [24] org.apache.spark.util.JsonProtocol$.<clinit>
  [25] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [26] org.apache.spark.scheduler.EventLoggingListener.start
  [27] org.apache.spark.SparkContext.<init>
  [28] org.apache.spark.api.java.JavaSparkContext.<init>
  [29] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [30] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [31] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [32] java.lang.reflect.Constructor.newInstance
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.ConstructorCommand.invokeConstructor
  [37] py4j.commands.ConstructorCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16146]

--- 1551158144749390 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] com.fasterxml.jackson.core.JsonFactory._createGenerator
  [13] com.fasterxml.jackson.core.JsonFactory.createGenerator
  [14] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [15] org.json4s.jackson.JsonMethods$class.compact
  [16] org.json4s.jackson.JsonMethods$.compact
  [17] org.apache.spark.scheduler.EventLoggingListener$.initEventLog
  [18] org.apache.spark.scheduler.EventLoggingListener.start
  [19] org.apache.spark.SparkContext.<init>
  [20] org.apache.spark.api.java.JavaSparkContext.<init>
  [21] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [22] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [23] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [24] java.lang.reflect.Constructor.newInstance
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.ConstructorCommand.invokeConstructor
  [29] py4j.commands.ConstructorCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158144865276 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.SparkContext.<init>
  [10] org.apache.spark.api.java.JavaSparkContext.<init>
  [11] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [12] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [13] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [14] java.lang.reflect.Constructor.newInstance
  [15] py4j.reflection.MethodInvoker.invoke
  [16] py4j.reflection.ReflectionEngine.invoke
  [17] py4j.Gateway.invoke
  [18] py4j.commands.ConstructorCommand.invokeConstructor
  [19] py4j.commands.ConstructorCommand.execute
  [20] py4j.GatewayConnection.run
  [21] java.lang.Thread.run
  [22] [tid=16146]

--- 1551158145091690 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] py4j.reflection.CurrentThreadClassLoadingStrategy.classForName
  [14] py4j.reflection.ReflectionUtil.classForName
  [15] py4j.reflection.TypeUtil.forName
  [16] py4j.commands.ReflectionCommand.getUnknownMember
  [17] py4j.commands.ReflectionCommand.execute
  [18] py4j.GatewayConnection.run
  [19] java.lang.Thread.run
  [20] [tid=16146]

--- 1551158145144011 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [11] org.apache.hadoop.hdfs.protocolPB.PBHelper.convert
  [12] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock
  [13] sun.reflect.NativeMethodAccessorImpl.invoke0
  [14] sun.reflect.NativeMethodAccessorImpl.invoke
  [15] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [16] java.lang.reflect.Method.invoke
  [17] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [18] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [19] com.sun.proxy.$Proxy14.addBlock
  [20] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock
  [21] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [22] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [23] [tid=16254]

--- 1551158145301874 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [10] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [11] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [13] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto.initFields
  [14] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto.<clinit>
  [15] org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.buildClientHeader
  [16] org.apache.hadoop.hdfs.protocol.datatransfer.Sender.writeBlock
  [17] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream
  [18] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [19] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [20] [tid=16254]

--- 1551158145319190 us
  [ 0] frame::interpreter_frame_method() const
  [ 1] org.apache.spark.sql.internal.SQLConf$.<init>
  [ 2] org.apache.spark.sql.internal.SQLConf$.<clinit>
  [ 3] org.apache.spark.sql.SparkSession.<init>
  [ 4] org.apache.spark.sql.SparkSession.<init>
  [ 5] sun.reflect.NativeConstructorAccessorImpl.newInstance0
  [ 6] sun.reflect.NativeConstructorAccessorImpl.newInstance
  [ 7] sun.reflect.DelegatingConstructorAccessorImpl.newInstance
  [ 8] java.lang.reflect.Constructor.newInstance
  [ 9] py4j.reflection.MethodInvoker.invoke
  [10] py4j.reflection.ReflectionEngine.invoke
  [11] py4j.Gateway.invoke
  [12] py4j.commands.ConstructorCommand.invokeConstructor
  [13] py4j.commands.ConstructorCommand.execute
  [14] py4j.GatewayConnection.run
  [15] java.lang.Thread.run
  [16] [tid=16146]

--- 1551158145417628 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto.newBuilder
  [13] org.apache.hadoop.hdfs.protocol.datatransfer.Sender.writeBlock
  [14] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream
  [15] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream
  [16] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [17] [tid=16254]

--- 1551158145541153 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.ui.SQLAppStatusListener.<init>
  [10] org.apache.spark.sql.internal.SharedState.<init>
  [11] org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply
  [12] org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply
  [13] scala.Option.getOrElse
  [14] org.apache.spark.sql.SparkSession.sharedState$lzycompute
  [15] org.apache.spark.sql.SparkSession.sharedState
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [17] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [18] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [19] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [22] org.apache.spark.sql.SparkSession.sessionState
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16146]

--- 1551158145665167 us
  [ 0] do_syscall_64_[k]
  [ 1] entry_SYSCALL_64_after_hwframe_[k]
  [ 2] __vdso_clock_gettime
  [ 3] __GI___clock_gettime
  [ 4] [unknown]
  [ 5] java.lang.ClassLoader.defineClass1
  [ 6] java.lang.ClassLoader.defineClass
  [ 7] java.security.SecureClassLoader.defineClass
  [ 8] java.net.URLClassLoader.defineClass
  [ 9] java.net.URLClassLoader.access$100
  [10] java.net.URLClassLoader$1.run
  [11] java.net.URLClassLoader$1.run
  [12] java.security.AccessController.doPrivileged
  [13] java.net.URLClassLoader.findClass
  [14] java.lang.ClassLoader.loadClass
  [15] sun.misc.Launcher$AppClassLoader.loadClass
  [16] java.lang.ClassLoader.loadClass
  [17] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [18] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [19] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [24] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [25] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [26] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [27] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [28] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [31] org.apache.spark.sql.SparkSession.sessionState
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158145779263 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.DFSPacket.writeTo
  [13] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run
  [14] [tid=16254]

--- 1551158145814286 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158145964412 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146091986 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146201537 us
  [ 0] do_syscall_64_[k]
  [ 1] entry_SYSCALL_64_after_hwframe_[k]
  [ 2] __vdso_clock_gettime
  [ 3] __GI___clock_gettime
  [ 4] [unknown]
  [ 5] java.lang.Class.getDeclaredConstructors0
  [ 6] java.lang.Class.privateGetDeclaredConstructors
  [ 7] java.lang.Class.getConstructors
  [ 8] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [ 9] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [10] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [11] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [12] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [13] scala.Option.getOrElse
  [14] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [17] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [18] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [19] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [22] org.apache.spark.sql.SparkSession.sessionState
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16146]

--- 1551158146312166 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146422220 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146522726 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146625772 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146728134 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [14] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [15] scala.Option.getOrElse
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [19] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [20] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [21] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [22] scala.Option.getOrElse
  [23] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [24] org.apache.spark.sql.SparkSession.sessionState
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158146829828 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructors
  [12] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [13] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [14] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [21] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [22] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [23] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [26] org.apache.spark.sql.SparkSession.sessionState
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158146933943 us
  [ 0] __lock_text_start_[k]
  [ 1] pagevec_lru_move_fn_[k]
  [ 2] __lru_cache_add_[k]
  [ 3] handle_pte_fault_[k]
  [ 4] __handle_mm_fault_[k]
  [ 5] handle_mm_fault_[k]
  [ 6] __do_page_fault_[k]
  [ 7] page_fault_[k]
  [ 8] java.util.Arrays.copyOfRange
  [ 9] java.lang.String.<init>
  [10] sun.net.www.ParseUtil.encodePath
  [11] sun.misc.URLClassPath$FileLoader.getResource
  [12] sun.misc.URLClassPath.getResource
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] java.lang.Class.getDeclaredConstructors0
  [21] java.lang.Class.privateGetDeclaredConstructors
  [22] java.lang.Class.getConstructors
  [23] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.expression
  [24] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [25] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [26] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [27] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [28] scala.Option.getOrElse
  [29] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [30] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [31] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [32] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [33] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [34] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [35] scala.Option.getOrElse
  [36] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [37] org.apache.spark.sql.SparkSession.sessionState
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158147035600 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [11] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [13] org.apache.spark.sql.types.FloatType.<init>
  [14] org.apache.spark.sql.types.FloatType$.<init>
  [15] org.apache.spark.sql.types.FloatType$.<clinit>
  [16] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [17] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [19] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [22] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [23] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [24] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [25] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [26] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [27] scala.Option.getOrElse
  [28] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [29] org.apache.spark.sql.SparkSession.sessionState
  [30] sun.reflect.NativeMethodAccessorImpl.invoke0
  [31] sun.reflect.NativeMethodAccessorImpl.invoke
  [32] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [33] java.lang.reflect.Method.invoke
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.AbstractCommand.invokeMethod
  [38] py4j.commands.CallCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16146]

--- 1551158147135336 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.types.DecimalType.<init>
  [13] org.apache.spark.sql.types.DecimalType$.<init>
  [14] org.apache.spark.sql.types.DecimalType$.<clinit>
  [15] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<init>
  [16] org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.<clinit>
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [18] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$functionRegistry$2.apply
  [19] scala.Option.getOrElse
  [20] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry$lzycompute
  [21] org.apache.spark.sql.internal.BaseSessionStateBuilder.functionRegistry
  [22] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [23] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [24] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [25] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [26] scala.Option.getOrElse
  [27] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [28] org.apache.spark.sql.SparkSession.sessionState
  [29] sun.reflect.NativeMethodAccessorImpl.invoke0
  [30] sun.reflect.NativeMethodAccessorImpl.invoke
  [31] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [32] java.lang.reflect.Method.invoke
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.AbstractCommand.invokeMethod
  [37] py4j.commands.CallCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16146]

--- 1551158147251568 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] org.apache.spark.sql.execution.SparkSqlParser.<init>
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser$lzycompute
  [14] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [16] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [17] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [18] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [19] scala.Option.getOrElse
  [20] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [21] org.apache.spark.sql.SparkSession.sessionState
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16146]

--- 1551158147394016 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.sql.execution.SparkSqlParser.<init>
  [11] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser$lzycompute
  [12] org.apache.spark.sql.internal.BaseSessionStateBuilder.sqlParser
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [14] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [15] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [16] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [19] org.apache.spark.sql.SparkSession.sessionState
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158147510980 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$listenerManager$2.apply
  [10] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$listenerManager$2.apply
  [11] scala.Option.getOrElse
  [12] org.apache.spark.sql.internal.BaseSessionStateBuilder.listenerManager
  [13] org.apache.spark.sql.internal.BaseSessionStateBuilder.build
  [14] org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState
  [15] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [16] org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply
  [17] scala.Option.getOrElse
  [18] org.apache.spark.sql.SparkSession.sessionState$lzycompute
  [19] org.apache.spark.sql.SparkSession.sessionState
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158147616282 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.apply
  [10] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$._descriptorFor
  [11] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.fieldName
  [12] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.findImplicitPropertyName
  [13] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.findImplicitPropertyName
  [14] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addFields
  [15] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [16] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [17] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [18] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [19] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [20] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [21] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [22] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [23] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [24] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [25] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [26] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [27] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [28] org.apache.spark.rdd.RDDOperationScope.toJson
  [29] org.apache.spark.rdd.RDDOperationScope$.withScope
  [30] org.apache.spark.rdd.RDDOperationScope$.withScope
  [31] org.apache.spark.SparkContext.withScope
  [32] org.apache.spark.SparkContext.textFile
  [33] org.apache.spark.api.java.JavaSparkContext.textFile
  [34] sun.reflect.NativeMethodAccessorImpl.invoke0
  [35] sun.reflect.NativeMethodAccessorImpl.invoke
  [36] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [37] java.lang.reflect.Method.invoke
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.AbstractCommand.invokeMethod
  [42] py4j.commands.CallCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16146]

--- 1551158147716528 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.collection.SeqLike$class.view
  [10] scala.collection.AbstractSeq.view
  [11] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.hasCreatorAnnotation
  [12] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.hasCreatorAnnotation
  [13] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreatorParam
  [14] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreators
  [15] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [16] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [17] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [18] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [19] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [20] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [21] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [22] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [23] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [24] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [25] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [26] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [27] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [28] org.apache.spark.rdd.RDDOperationScope.toJson
  [29] org.apache.spark.rdd.RDDOperationScope$.withScope
  [30] org.apache.spark.rdd.RDDOperationScope$.withScope
  [31] org.apache.spark.SparkContext.withScope
  [32] org.apache.spark.SparkContext.textFile
  [33] org.apache.spark.api.java.JavaSparkContext.textFile
  [34] sun.reflect.NativeMethodAccessorImpl.invoke0
  [35] sun.reflect.NativeMethodAccessorImpl.invoke
  [36] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [37] java.lang.reflect.Method.invoke
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.AbstractCommand.invokeMethod
  [42] py4j.commands.CallCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16146]

--- 1551158147816276 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.collection.SeqViewLike$$anon$4.<init>
  [13] scala.collection.SeqViewLike$class.newFlatMapped
  [14] scala.collection.SeqLike$$anon$2.newFlatMapped
  [15] scala.collection.SeqLike$$anon$2.newFlatMapped
  [16] scala.collection.TraversableViewLike$class.flatMap
  [17] scala.collection.SeqLike$$anon$2.flatMap
  [18] com.fasterxml.jackson.module.scala.introspect.ScalaAnnotationIntrospector$.hasCreatorAnnotation
  [19] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.hasCreatorAnnotation
  [20] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreatorParam
  [21] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._addCreators
  [22] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll
  [23] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod
  [24] com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod
  [25] com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations
  [26] com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2
  [27] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [28] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [29] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [30] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [31] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [32] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [33] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [34] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [35] org.apache.spark.rdd.RDDOperationScope.toJson
  [36] org.apache.spark.rdd.RDDOperationScope$.withScope
  [37] org.apache.spark.rdd.RDDOperationScope$.withScope
  [38] org.apache.spark.SparkContext.withScope
  [39] org.apache.spark.SparkContext.textFile
  [40] org.apache.spark.api.java.JavaSparkContext.textFile
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16146]

--- 1551158147916703 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] com.fasterxml.jackson.core.JsonFactory._createParser
  [11] com.fasterxml.jackson.core.JsonFactory.createParser
  [12] com.fasterxml.jackson.databind.ObjectMapper.readValue
  [13] org.apache.spark.rdd.RDDOperationScope$.fromJson
  [14] org.apache.spark.rdd.RDDOperationScope$$anonfun$5.apply
  [15] org.apache.spark.rdd.RDDOperationScope$$anonfun$5.apply
  [16] scala.Option.map
  [17] org.apache.spark.rdd.RDDOperationScope$.withScope
  [18] org.apache.spark.rdd.RDDOperationScope$.withScope
  [19] org.apache.spark.SparkContext.withScope
  [20] org.apache.spark.SparkContext.hadoopFile
  [21] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [22] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [23] org.apache.spark.rdd.RDDOperationScope$.withScope
  [24] org.apache.spark.rdd.RDDOperationScope$.withScope
  [25] org.apache.spark.SparkContext.withScope
  [26] org.apache.spark.SparkContext.textFile
  [27] org.apache.spark.api.java.JavaSparkContext.textFile
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16146]

--- 1551158148016446 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.SparkContext.broadcast
  [10] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [11] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [12] org.apache.spark.rdd.RDDOperationScope$.withScope
  [13] org.apache.spark.rdd.RDDOperationScope$.withScope
  [14] org.apache.spark.SparkContext.withScope
  [15] org.apache.spark.SparkContext.hadoopFile
  [16] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [17] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [18] org.apache.spark.rdd.RDDOperationScope$.withScope
  [19] org.apache.spark.rdd.RDDOperationScope$.withScope
  [20] org.apache.spark.SparkContext.withScope
  [21] org.apache.spark.SparkContext.textFile
  [22] org.apache.spark.api.java.JavaSparkContext.textFile
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16146]

--- 1551158148119384 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.collection.mutable.MutableList.<init>
  [10] scala.collection.mutable.Queue.<init>
  [11] org.apache.spark.util.collection.SizeTracker$class.$init$
  [12] org.apache.spark.util.collection.SizeTrackingVector.<init>
  [13] org.apache.spark.storage.memory.DeserializedValuesHolder.<init>
  [14] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [15] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [16] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [17] org.apache.spark.storage.BlockManager.doPut
  [18] org.apache.spark.storage.BlockManager.doPutIterator
  [19] org.apache.spark.storage.BlockManager.putIterator
  [20] org.apache.spark.storage.BlockManager.putSingle
  [21] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [22] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [23] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [24] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [25] org.apache.spark.SparkContext.broadcast
  [26] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [27] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [28] org.apache.spark.rdd.RDDOperationScope$.withScope
  [29] org.apache.spark.rdd.RDDOperationScope$.withScope
  [30] org.apache.spark.SparkContext.withScope
  [31] org.apache.spark.SparkContext.hadoopFile
  [32] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [33] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [34] org.apache.spark.rdd.RDDOperationScope$.withScope
  [35] org.apache.spark.rdd.RDDOperationScope$.withScope
  [36] org.apache.spark.SparkContext.withScope
  [37] org.apache.spark.SparkContext.textFile
  [38] org.apache.spark.api.java.JavaSparkContext.textFile
  [39] sun.reflect.NativeMethodAccessorImpl.invoke0
  [40] sun.reflect.NativeMethodAccessorImpl.invoke
  [41] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [42] java.lang.reflect.Method.invoke
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.AbstractCommand.invokeMethod
  [47] py4j.commands.CallCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16146]

--- 1551158148220590 us
  [ 0] ArgumentSizeComputer::set(int, BasicType)
  [ 1] Method::compute_size_of_parameters(Thread*)
  [ 2] ClassFileParser::parse_method(bool, AccessFlags*, Thread*)
  [ 3] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 4] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 5] ClassLoader::load_classfile(Symbol*, Thread*)
  [ 6] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 7] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 8] SystemDictionary::resolve_or_fail(Symbol*, Handle, Handle, bool, Thread*)
  [ 9] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] sun.management.ManagementFactoryHelper.getDiagnosticCommandMBean
  [12] sun.management.ManagementFactoryHelper.getPlatformDynamicMBeans
  [13] java.lang.management.ManagementFactory.getPlatformMBeanServer
  [14] org.apache.spark.util.SizeEstimator$.getIsCompressedOops
  [15] org.apache.spark.util.SizeEstimator$.initialize
  [16] org.apache.spark.util.SizeEstimator$.<init>
  [17] org.apache.spark.util.SizeEstimator$.<clinit>
  [18] org.apache.spark.util.collection.SizeTracker$class.takeSample
  [19] org.apache.spark.util.collection.SizeTracker$class.resetSamples
  [20] org.apache.spark.util.collection.SizeTrackingVector.resetSamples
  [21] org.apache.spark.util.collection.SizeTracker$class.$init$
  [22] org.apache.spark.util.collection.SizeTrackingVector.<init>
  [23] org.apache.spark.storage.memory.DeserializedValuesHolder.<init>
  [24] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [25] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [26] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [27] org.apache.spark.storage.BlockManager.doPut
  [28] org.apache.spark.storage.BlockManager.doPutIterator
  [29] org.apache.spark.storage.BlockManager.putIterator
  [30] org.apache.spark.storage.BlockManager.putSingle
  [31] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [32] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [33] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [34] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [35] org.apache.spark.SparkContext.broadcast
  [36] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [37] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [38] org.apache.spark.rdd.RDDOperationScope$.withScope
  [39] org.apache.spark.rdd.RDDOperationScope$.withScope
  [40] org.apache.spark.SparkContext.withScope
  [41] org.apache.spark.SparkContext.hadoopFile
  [42] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [43] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [44] org.apache.spark.rdd.RDDOperationScope$.withScope
  [45] org.apache.spark.rdd.RDDOperationScope$.withScope
  [46] org.apache.spark.SparkContext.withScope
  [47] org.apache.spark.SparkContext.textFile
  [48] org.apache.spark.api.java.JavaSparkContext.textFile
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16146]

--- 1551158148329342 us
  [ 0] org.apache.spark.util.SizeEstimator$SearchState.size_$eq
  [ 1] org.apache.spark.util.SizeEstimator$.visitSingleObject
  [ 2] org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate
  [ 3] org.apache.spark.util.SizeEstimator$$anonfun$sampleArray$1.apply$mcVI$sp
  [ 4] scala.collection.immutable.Range.foreach$mVc$sp
  [ 5] org.apache.spark.util.SizeEstimator$.sampleArray
  [ 6] org.apache.spark.util.SizeEstimator$.visitArray
  [ 7] org.apache.spark.util.SizeEstimator$.visitSingleObject
  [ 8] org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate
  [ 9] org.apache.spark.util.SizeEstimator$.estimate
  [10] org.apache.spark.util.collection.SizeTracker$class.takeSample
  [11] org.apache.spark.util.collection.SizeTracker$class.afterUpdate
  [12] org.apache.spark.util.collection.SizeTrackingVector.$plus$eq
  [13] org.apache.spark.storage.memory.DeserializedValuesHolder.storeValue
  [14] org.apache.spark.storage.memory.MemoryStore.putIterator
  [15] org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues
  [16] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [17] org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply
  [18] org.apache.spark.storage.BlockManager.doPut
  [19] org.apache.spark.storage.BlockManager.doPutIterator
  [20] org.apache.spark.storage.BlockManager.putIterator
  [21] org.apache.spark.storage.BlockManager.putSingle
  [22] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [23] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [24] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [25] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [26] org.apache.spark.SparkContext.broadcast
  [27] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [28] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [29] org.apache.spark.rdd.RDDOperationScope$.withScope
  [30] org.apache.spark.rdd.RDDOperationScope$.withScope
  [31] org.apache.spark.SparkContext.withScope
  [32] org.apache.spark.SparkContext.hadoopFile
  [33] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [34] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [35] org.apache.spark.rdd.RDDOperationScope$.withScope
  [36] org.apache.spark.rdd.RDDOperationScope$.withScope
  [37] org.apache.spark.SparkContext.withScope
  [38] org.apache.spark.SparkContext.textFile
  [39] org.apache.spark.api.java.JavaSparkContext.textFile
  [40] sun.reflect.NativeMethodAccessorImpl.invoke0
  [41] sun.reflect.NativeMethodAccessorImpl.invoke
  [42] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [43] java.lang.reflect.Method.invoke
  [44] py4j.reflection.MethodInvoker.invoke
  [45] py4j.reflection.ReflectionEngine.invoke
  [46] py4j.Gateway.invoke
  [47] py4j.commands.AbstractCommand.invokeMethod
  [48] py4j.commands.CallCommand.execute
  [49] py4j.GatewayConnection.run
  [50] java.lang.Thread.run
  [51] [tid=16146]

--- 1551158148435265 us
  [ 0] org.apache.hadoop.conf.Configuration.write
  [ 1] org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply$mcV$sp
  [ 2] org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply
  [ 3] org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply
  [ 4] org.apache.spark.util.Utils$.tryOrIOException
  [ 5] org.apache.spark.util.SerializableConfiguration.writeObject
  [ 6] sun.reflect.NativeMethodAccessorImpl.invoke0
  [ 7] sun.reflect.NativeMethodAccessorImpl.invoke
  [ 8] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [ 9] java.lang.reflect.Method.invoke
  [10] java.io.ObjectStreamClass.invokeWriteObject
  [11] java.io.ObjectOutputStream.writeSerialData
  [12] java.io.ObjectOutputStream.writeOrdinaryObject
  [13] java.io.ObjectOutputStream.writeObject0
  [14] java.io.ObjectOutputStream.writeObject
  [15] org.apache.spark.serializer.JavaSerializationStream.writeObject
  [16] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply
  [17] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply
  [18] org.apache.spark.util.Utils$.tryWithSafeFinally
  [19] org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject
  [20] org.apache.spark.broadcast.TorrentBroadcast.writeBlocks
  [21] org.apache.spark.broadcast.TorrentBroadcast.<init>
  [22] org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast
  [23] org.apache.spark.broadcast.BroadcastManager.newBroadcast
  [24] org.apache.spark.SparkContext.broadcast
  [25] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [26] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [27] org.apache.spark.rdd.RDDOperationScope$.withScope
  [28] org.apache.spark.rdd.RDDOperationScope$.withScope
  [29] org.apache.spark.SparkContext.withScope
  [30] org.apache.spark.SparkContext.hadoopFile
  [31] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [32] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [33] org.apache.spark.rdd.RDDOperationScope$.withScope
  [34] org.apache.spark.rdd.RDDOperationScope$.withScope
  [35] org.apache.spark.SparkContext.withScope
  [36] org.apache.spark.SparkContext.textFile
  [37] org.apache.spark.api.java.JavaSparkContext.textFile
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158148543981 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [11] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [12] org.apache.spark.rdd.RDDOperationScope$.withScope
  [13] org.apache.spark.rdd.RDDOperationScope$.withScope
  [14] org.apache.spark.SparkContext.withScope
  [15] org.apache.spark.SparkContext.hadoopFile
  [16] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [17] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [18] org.apache.spark.rdd.RDDOperationScope$.withScope
  [19] org.apache.spark.rdd.RDDOperationScope$.withScope
  [20] org.apache.spark.SparkContext.withScope
  [21] org.apache.spark.SparkContext.textFile
  [22] org.apache.spark.api.java.JavaSparkContext.textFile
  [23] sun.reflect.NativeMethodAccessorImpl.invoke0
  [24] sun.reflect.NativeMethodAccessorImpl.invoke
  [25] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [26] java.lang.reflect.Method.invoke
  [27] py4j.reflection.MethodInvoker.invoke
  [28] py4j.reflection.ReflectionEngine.invoke
  [29] py4j.Gateway.invoke
  [30] py4j.commands.AbstractCommand.invokeMethod
  [31] py4j.commands.CallCommand.execute
  [32] py4j.GatewayConnection.run
  [33] java.lang.Thread.run
  [34] [tid=16146]

--- 1551158148648441 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean
  [16] org.apache.spark.util.ClosureCleaner$.clean
  [17] org.apache.spark.SparkContext.clean
  [18] org.apache.spark.rdd.HadoopRDD.<init>
  [19] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [20] org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply
  [21] org.apache.spark.rdd.RDDOperationScope$.withScope
  [22] org.apache.spark.rdd.RDDOperationScope$.withScope
  [23] org.apache.spark.SparkContext.withScope
  [24] org.apache.spark.SparkContext.hadoopFile
  [25] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [26] org.apache.spark.SparkContext$$anonfun$textFile$1.apply
  [27] org.apache.spark.rdd.RDDOperationScope$.withScope
  [28] org.apache.spark.rdd.RDDOperationScope$.withScope
  [29] org.apache.spark.SparkContext.withScope
  [30] org.apache.spark.SparkContext.textFile
  [31] org.apache.spark.api.java.JavaSparkContext.textFile
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158148756827 us
  [ 0] inflate_fast
  [ 1] inflate
  [ 2] Java_java_util_zip_Inflater_inflateBytes
  [ 3] java.util.zip.Inflater.inflateBytes
  [ 4] java.util.zip.Inflater.inflate
  [ 5] java.util.zip.InflaterInputStream.read
  [ 6] sun.misc.Resource.getBytes
  [ 7] java.net.URLClassLoader.defineClass
  [ 8] java.net.URLClassLoader.access$100
  [ 9] java.net.URLClassLoader$1.run
  [10] java.net.URLClassLoader$1.run
  [11] java.security.AccessController.doPrivileged
  [12] java.net.URLClassLoader.findClass
  [13] java.lang.ClassLoader.loadClass
  [14] sun.misc.Launcher$AppClassLoader.loadClass
  [15] java.lang.ClassLoader.loadClass
  [16] org.apache.spark.rdd.RDD.partitions
  [17] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [18] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.AbstractCommand.invokeMethod
  [27] py4j.commands.CallCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158148867436 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] alluxio.hadoop.HadoopUtils.addSwiftCredentials
  [12] alluxio.hadoop.AbstractFileSystem.initialize
  [13] alluxio.hadoop.FileSystem.initialize
  [14] org.apache.hadoop.fs.FileSystem.createFileSystem
  [15] org.apache.hadoop.fs.FileSystem.access$200
  [16] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [17] org.apache.hadoop.fs.FileSystem$Cache.get
  [18] org.apache.hadoop.fs.FileSystem.get
  [19] org.apache.hadoop.fs.Path.getFileSystem
  [20] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [21] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [22] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [23] org.apache.spark.rdd.HadoopRDD.getPartitions
  [24] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [25] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [26] scala.Option.getOrElse
  [27] org.apache.spark.rdd.RDD.partitions
  [28] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [29] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [30] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [31] scala.Option.getOrElse
  [32] org.apache.spark.rdd.RDD.partitions
  [33] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [34] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16146]

--- 1551158148980966 us
  [ 0] arrayof_jint_fill
  [ 1] java.util.regex.Matcher.reset
  [ 2] java.util.regex.Matcher.<init>
  [ 3] java.util.regex.Pattern.matcher
  [ 4] alluxio.PropertyKey$Template.matches
  [ 5] alluxio.PropertyKey.isValid
  [ 6] alluxio.conf.AlluxioProperties.merge
  [ 7] alluxio.Configuration.reset
  [ 8] alluxio.Configuration.<clinit>
  [ 9] alluxio.hadoop.AbstractFileSystem.initializeInternal
  [10] alluxio.hadoop.AbstractFileSystem.initialize
  [11] alluxio.hadoop.FileSystem.initialize
  [12] org.apache.hadoop.fs.FileSystem.createFileSystem
  [13] org.apache.hadoop.fs.FileSystem.access$200
  [14] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [15] org.apache.hadoop.fs.FileSystem$Cache.get
  [16] org.apache.hadoop.fs.FileSystem.get
  [17] org.apache.hadoop.fs.Path.getFileSystem
  [18] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [19] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [20] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [21] org.apache.spark.rdd.HadoopRDD.getPartitions
  [22] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [23] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.rdd.RDD.partitions
  [26] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [27] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [28] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [29] scala.Option.getOrElse
  [30] org.apache.spark.rdd.RDD.partitions
  [31] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [32] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [33] sun.reflect.NativeMethodAccessorImpl.invoke0
  [34] sun.reflect.NativeMethodAccessorImpl.invoke
  [35] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [36] java.lang.reflect.Method.invoke
  [37] py4j.reflection.MethodInvoker.invoke
  [38] py4j.reflection.ReflectionEngine.invoke
  [39] py4j.Gateway.invoke
  [40] py4j.commands.AbstractCommand.invokeMethod
  [41] py4j.commands.CallCommand.execute
  [42] py4j.GatewayConnection.run
  [43] java.lang.Thread.run
  [44] [tid=16146]

--- 1551158149104962 us
  [ 0] java.util.zip.Inflater.ended
  [ 1] java.util.zip.ZipFile.releaseInflater
  [ 2] java.util.zip.ZipFile.access$100
  [ 3] java.util.zip.ZipFile$ZipFileInflaterInputStream.close
  [ 4] sun.misc.Resource.getBytes
  [ 5] java.net.URLClassLoader.defineClass
  [ 6] java.net.URLClassLoader.access$100
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] alluxio.core.client.runtime.com.codahale.metrics.jvm.MemoryUsageGaugeSet.getMetrics
  [15] alluxio.core.client.runtime.com.codahale.metrics.MetricRegistry.registerAll
  [16] alluxio.core.client.runtime.com.codahale.metrics.MetricRegistry.registerAll
  [17] alluxio.metrics.MetricsSystem.<clinit>
  [18] alluxio.client.file.FileSystemContext.<clinit>
  [19] alluxio.hadoop.AbstractFileSystem.initializeInternal
  [20] alluxio.hadoop.AbstractFileSystem.initialize
  [21] alluxio.hadoop.FileSystem.initialize
  [22] org.apache.hadoop.fs.FileSystem.createFileSystem
  [23] org.apache.hadoop.fs.FileSystem.access$200
  [24] org.apache.hadoop.fs.FileSystem$Cache.getInternal
  [25] org.apache.hadoop.fs.FileSystem$Cache.get
  [26] org.apache.hadoop.fs.FileSystem.get
  [27] org.apache.hadoop.fs.Path.getFileSystem
  [28] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [29] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [30] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [31] org.apache.spark.rdd.HadoopRDD.getPartitions
  [32] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [33] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [34] scala.Option.getOrElse
  [35] org.apache.spark.rdd.RDD.partitions
  [36] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [37] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [38] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [39] scala.Option.getOrElse
  [40] org.apache.spark.rdd.RDD.partitions
  [41] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [42] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [43] sun.reflect.NativeMethodAccessorImpl.invoke0
  [44] sun.reflect.NativeMethodAccessorImpl.invoke
  [45] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [46] java.lang.reflect.Method.invoke
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.AbstractCommand.invokeMethod
  [51] py4j.commands.CallCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16146]

--- 1551158149324054 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.Configuration.loadClusterDefault
  [10] alluxio.AbstractClient.beforeConnect
  [11] alluxio.AbstractClient.connect
  [12] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [13] alluxio.client.metrics.ClientMasterSync.heartbeat
  [14] alluxio.heartbeat.HeartbeatThread.run
  [15] java.util.concurrent.Executors$RunnableAdapter.call
  [16] java.util.concurrent.FutureTask.run
  [17] java.util.concurrent.ThreadPoolExecutor.runWorker
  [18] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [19] java.lang.Thread.run
  [20] [tid=16276]

--- 1551158149431429 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] sun.security.util.DerInputStream.init
  [10] sun.security.util.DerInputStream.<init>
  [11] sun.security.pkcs.PKCS7.<init>
  [12] sun.security.util.SignatureFileVerifier.<init>
  [13] java.util.jar.JarVerifier.processEntry
  [14] java.util.jar.JarVerifier.update
  [15] java.util.jar.JarFile.initializeVerifier
  [16] java.util.jar.JarFile.ensureInitialization
  [17] java.util.jar.JavaUtilJarAccessImpl.ensureInitialization
  [18] sun.misc.URLClassPath$JarLoader$2.getManifest
  [19] java.net.URLClassLoader.defineClass
  [20] java.net.URLClassLoader.access$100
  [21] java.net.URLClassLoader$1.run
  [22] java.net.URLClassLoader$1.run
  [23] java.security.AccessController.doPrivileged
  [24] java.net.URLClassLoader.findClass
  [25] java.lang.ClassLoader.loadClass
  [26] java.lang.ClassLoader.loadClass
  [27] sun.misc.Launcher$AppClassLoader.loadClass
  [28] java.lang.ClassLoader.loadClass
  [29] sun.security.jca.ProviderConfig$2.run
  [30] sun.security.jca.ProviderConfig$2.run
  [31] java.security.AccessController.doPrivileged
  [32] sun.security.jca.ProviderConfig.doLoadProvider
  [33] sun.security.jca.ProviderConfig.getProvider
  [34] sun.security.jca.ProviderList.loadAll
  [35] sun.security.jca.ProviderList.removeInvalid
  [36] sun.security.jca.Providers.getFullProviderList
  [37] java.security.Security.insertProviderAt
  [38] java.security.Security.addProvider
  [39] alluxio.security.authentication.PlainSaslTransportProvider.<clinit>
  [40] alluxio.security.authentication.TransportProvider$Factory.create
  [41] alluxio.AbstractClient.connect
  [42] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [43] alluxio.client.metrics.ClientMasterSync.heartbeat
  [44] alluxio.heartbeat.HeartbeatThread.run
  [45] java.util.concurrent.Executors$RunnableAdapter.call
  [46] java.util.concurrent.FutureTask.run
  [47] java.util.concurrent.ThreadPoolExecutor.runWorker
  [48] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [49] java.lang.Thread.run
  [50] [tid=16276]

--- 1551158149532501 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] alluxio.security.authentication.PlainSaslTransportProvider.getClientTransport
  [11] alluxio.security.authentication.PlainSaslTransportProvider.getClientTransport
  [12] alluxio.AbstractClient.connect
  [13] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [14] alluxio.client.metrics.ClientMasterSync.heartbeat
  [15] alluxio.heartbeat.HeartbeatThread.run
  [16] java.util.concurrent.Executors$RunnableAdapter.call
  [17] java.util.concurrent.FutureTask.run
  [18] java.util.concurrent.ThreadPoolExecutor.runWorker
  [19] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [20] java.lang.Thread.run
  [21] [tid=16276]

--- 1551158149640499 us
  [ 0] SignatureVerifier::is_valid_method_signature(Symbol*)
  [ 1] ClassVerifier::verify_class(Thread*)
  [ 2] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 3] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 4] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 5] InstanceKlass::initialize(Thread*)
  [ 6] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 7] alluxio.thrift.MetricsMasterClientService$metricsHeartbeat_result$metricsHeartbeat_resultStandardSchemeFactory.getScheme
  [ 8] alluxio.thrift.MetricsMasterClientService$metricsHeartbeat_result$metricsHeartbeat_resultStandardSchemeFactory.getScheme
  [ 9] alluxio.thrift.MetricsMasterClientService$metricsHeartbeat_result.read
  [10] alluxio.core.client.runtime.org.apache.thrift.TServiceClient.receiveBase
  [11] alluxio.thrift.MetricsMasterClientService$Client.recv_metricsHeartbeat
  [12] alluxio.thrift.MetricsMasterClientService$Client.metricsHeartbeat
  [13] alluxio.client.metrics.MetricsMasterClient.heartbeat
  [14] alluxio.client.metrics.ClientMasterSync.heartbeat
  [15] alluxio.heartbeat.HeartbeatThread.run
  [16] java.util.concurrent.Executors$RunnableAdapter.call
  [17] java.util.concurrent.FutureTask.run
  [18] java.util.concurrent.ThreadPoolExecutor.runWorker
  [19] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [20] java.lang.Thread.run
  [21] [tid=16276]

--- 1551158149744455 us
  [ 0] Method::get_c2i_entry()
  [ 1] [tid=16146]

--- 1551158149848767 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.retry.ExponentialTimeBoundedRetry.builder
  [10] alluxio.retry.RetryUtils.defaultClientRetry
  [11] alluxio.AbstractClient$$Lambda$40.1543935188.get
  [12] alluxio.AbstractClient.retryRPCInternal
  [13] alluxio.AbstractClient.retryRPC
  [14] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [15] alluxio.client.file.BaseFileSystem.getStatus
  [16] alluxio.client.file.BaseFileSystem.getStatus
  [17] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [18] alluxio.hadoop.FileSystem.getFileStatus
  [19] org.apache.hadoop.fs.Globber.getFileStatus
  [20] org.apache.hadoop.fs.Globber.glob
  [21] org.apache.hadoop.fs.FileSystem.globStatus
  [22] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [23] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [24] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [25] org.apache.spark.rdd.HadoopRDD.getPartitions
  [26] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [27] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [28] scala.Option.getOrElse
  [29] org.apache.spark.rdd.RDD.partitions
  [30] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [31] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [32] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [33] scala.Option.getOrElse
  [34] org.apache.spark.rdd.RDD.partitions
  [35] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [36] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [37] sun.reflect.NativeMethodAccessorImpl.invoke0
  [38] sun.reflect.NativeMethodAccessorImpl.invoke
  [39] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [40] java.lang.reflect.Method.invoke
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.AbstractCommand.invokeMethod
  [45] py4j.commands.CallCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16146]

--- 1551158149890996 us
  [ 0] MetadataOnStackMark::~MetadataOnStackMark()
  [ 1] ClassLoaderDataGraph::do_unloading(BoolObjectClosure*, bool)
  [ 2] SystemDictionary::do_unloading(BoolObjectClosure*, bool)
  [ 3] PSParallelCompact::marking_phase(ParCompactionManager*, bool, ParallelOldTracer*)
  [ 4] PSParallelCompact::invoke_no_policy(bool) [clone .part.118]
  [ 5] PSParallelCompact::invoke(bool)
  [ 6] CollectedHeap::collect_as_vm_thread(GCCause::Cause)
  [ 7] VM_CollectForMetadataAllocation::doit()
  [ 8] VM_Operation::evaluate()
  [ 9] VMThread::evaluate_operation(VM_Operation*) [clone .constprop.44]
  [10] VMThread::loop()
  [11] VMThread::run()
  [12] java_start(Thread*)
  [13] start_thread
  [14] [tid=16093]

--- 1551158149951595 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16072]

--- 1551158149953780 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16078]

--- 1551158149957026 us
  [ 0] ParallelTaskTerminator::offer_termination(TerminatorTerminator*)
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16088]

--- 1551158149957762 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16076]

--- 1551158149959024 us
  [ 0] __do_page_fault_[k]
  [ 1] page_fault_[k]
  [ 2] MoveAndUpdateClosure::copy_partial_obj()
  [ 3] PSParallelCompact::fill_region(ParCompactionManager*, unsigned long)
  [ 4] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 5] GCTaskThread::run()
  [ 6] java_start(Thread*)
  [ 7] start_thread
  [ 8] [tid=16066]

--- 1551158149959069 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16090]

--- 1551158149962215 us
  [ 0] SpinPause
  [ 1] StealRegionCompactionTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16068]

--- 1551158150069331 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [10] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [11] alluxio.thrift.GetStatusTResponse.read
  [12] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [13] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [14] alluxio.thrift.FileSystemMasterClientService$getStatus_result.read
  [15] alluxio.core.client.runtime.org.apache.thrift.TServiceClient.receiveBase
  [16] alluxio.thrift.FileSystemMasterClientService$Client.recv_getStatus
  [17] alluxio.thrift.FileSystemMasterClientService$Client.getStatus
  [18] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [19] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.295290579.call
  [20] alluxio.AbstractClient.retryRPCInternal
  [21] alluxio.AbstractClient.retryRPC
  [22] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [23] alluxio.client.file.BaseFileSystem.getStatus
  [24] alluxio.client.file.BaseFileSystem.getStatus
  [25] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [26] alluxio.hadoop.FileSystem.getFileStatus
  [27] org.apache.hadoop.fs.Globber.getFileStatus
  [28] org.apache.hadoop.fs.Globber.glob
  [29] org.apache.hadoop.fs.FileSystem.globStatus
  [30] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [31] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [32] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [33] org.apache.spark.rdd.HadoopRDD.getPartitions
  [34] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [35] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [36] scala.Option.getOrElse
  [37] org.apache.spark.rdd.RDD.partitions
  [38] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [39] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [40] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [41] scala.Option.getOrElse
  [42] org.apache.spark.rdd.RDD.partitions
  [43] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [44] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [45] sun.reflect.NativeMethodAccessorImpl.invoke0
  [46] sun.reflect.NativeMethodAccessorImpl.invoke
  [47] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [48] java.lang.reflect.Method.invoke
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.AbstractCommand.invokeMethod
  [53] py4j.commands.CallCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16146]

--- 1551158150169072 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] alluxio.thrift.BlockLocation$BlockLocationStandardScheme.read
  [10] alluxio.thrift.BlockLocation$BlockLocationStandardScheme.read
  [11] alluxio.thrift.BlockLocation.read
  [12] alluxio.thrift.BlockInfo$BlockInfoStandardScheme.read
  [13] alluxio.thrift.BlockInfo$BlockInfoStandardScheme.read
  [14] alluxio.thrift.BlockInfo.read
  [15] alluxio.thrift.FileBlockInfo$FileBlockInfoStandardScheme.read
  [16] alluxio.thrift.FileBlockInfo$FileBlockInfoStandardScheme.read
  [17] alluxio.thrift.FileBlockInfo.read
  [18] alluxio.thrift.FileInfo$FileInfoStandardScheme.read
  [19] alluxio.thrift.FileInfo$FileInfoStandardScheme.read
  [20] alluxio.thrift.FileInfo.read
  [21] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [22] alluxio.thrift.GetStatusTResponse$GetStatusTResponseStandardScheme.read
  [23] alluxio.thrift.GetStatusTResponse.read
  [24] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [25] alluxio.thrift.FileSystemMasterClientService$getStatus_result$getStatus_resultStandardScheme.read
  [26] alluxio.thrift.FileSystemMasterClientService$getStatus_result.read
  [27] alluxio.core.client.runtime.org.apache.thrift.TServiceClient.receiveBase
  [28] alluxio.thrift.FileSystemMasterClientService$Client.recv_getStatus
  [29] alluxio.thrift.FileSystemMasterClientService$Client.getStatus
  [30] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [31] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.295290579.call
  [32] alluxio.AbstractClient.retryRPCInternal
  [33] alluxio.AbstractClient.retryRPC
  [34] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [35] alluxio.client.file.BaseFileSystem.getStatus
  [36] alluxio.client.file.BaseFileSystem.getStatus
  [37] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [38] alluxio.hadoop.FileSystem.getFileStatus
  [39] org.apache.hadoop.fs.Globber.getFileStatus
  [40] org.apache.hadoop.fs.Globber.glob
  [41] org.apache.hadoop.fs.FileSystem.globStatus
  [42] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [43] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [44] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [45] org.apache.spark.rdd.HadoopRDD.getPartitions
  [46] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [47] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [48] scala.Option.getOrElse
  [49] org.apache.spark.rdd.RDD.partitions
  [50] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [51] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [52] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [53] scala.Option.getOrElse
  [54] org.apache.spark.rdd.RDD.partitions
  [55] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [56] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [57] sun.reflect.NativeMethodAccessorImpl.invoke0
  [58] sun.reflect.NativeMethodAccessorImpl.invoke
  [59] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [60] java.lang.reflect.Method.invoke
  [61] py4j.reflection.MethodInvoker.invoke
  [62] py4j.reflection.ReflectionEngine.invoke
  [63] py4j.Gateway.invoke
  [64] py4j.commands.AbstractCommand.invokeMethod
  [65] py4j.commands.CallCommand.execute
  [66] py4j.GatewayConnection.run
  [67] java.lang.Thread.run
  [68] [tid=16146]

--- 1551158150273599 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] alluxio.core.client.runtime.com.google.common.collect.ImmutableList.construct
  [13] alluxio.core.client.runtime.com.google.common.collect.ImmutableList.asImmutableList
  [14] alluxio.core.client.runtime.com.google.common.collect.ImmutableList.copyFromCollection
  [15] alluxio.core.client.runtime.com.google.common.collect.ImmutableList.copyOf
  [16] alluxio.wire.TieredIdentity.<init>
  [17] alluxio.wire.TieredIdentity.fromThrift
  [18] alluxio.wire.WorkerNetAddress.fromThrift
  [19] alluxio.wire.BlockLocation.fromThrift
  [20] alluxio.wire.BlockInfo$$Lambda$44.890978347.apply
  [21] java.util.stream.ReferencePipeline$3$1.accept
  [22] java.util.ArrayList$ArrayListSpliterator.forEachRemaining
  [23] java.util.stream.AbstractPipeline.copyInto
  [24] java.util.stream.AbstractPipeline.wrapAndCopyInto
  [25] java.util.stream.ReduceOps$ReduceOp.evaluateSequential
  [26] java.util.stream.AbstractPipeline.evaluate
  [27] java.util.stream.ReferencePipeline.collect
  [28] alluxio.util.StreamUtils.map
  [29] alluxio.wire.BlockInfo.fromThrift
  [30] alluxio.wire.FileBlockInfo.fromThrift
  [31] alluxio.wire.FileInfo$$Lambda$43.1047639181.apply
  [32] java.util.stream.ReferencePipeline$3$1.accept
  [33] java.util.ArrayList$ArrayListSpliterator.forEachRemaining
  [34] java.util.stream.AbstractPipeline.copyInto
  [35] java.util.stream.AbstractPipeline.wrapAndCopyInto
  [36] java.util.stream.ReduceOps$ReduceOp.evaluateSequential
  [37] java.util.stream.AbstractPipeline.evaluate
  [38] java.util.stream.ReferencePipeline.collect
  [39] alluxio.util.StreamUtils.map
  [40] alluxio.wire.FileInfo.fromThrift
  [41] alluxio.client.file.RetryHandlingFileSystemMasterClient.lambda$getStatus$6
  [42] alluxio.client.file.RetryHandlingFileSystemMasterClient$$Lambda$41.295290579.call
  [43] alluxio.AbstractClient.retryRPCInternal
  [44] alluxio.AbstractClient.retryRPC
  [45] alluxio.client.file.RetryHandlingFileSystemMasterClient.getStatus
  [46] alluxio.client.file.BaseFileSystem.getStatus
  [47] alluxio.client.file.BaseFileSystem.getStatus
  [48] alluxio.hadoop.AbstractFileSystem.getFileStatus
  [49] alluxio.hadoop.FileSystem.getFileStatus
  [50] org.apache.hadoop.fs.Globber.getFileStatus
  [51] org.apache.hadoop.fs.Globber.glob
  [52] org.apache.hadoop.fs.FileSystem.globStatus
  [53] org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus
  [54] org.apache.hadoop.mapred.FileInputFormat.listStatus
  [55] org.apache.hadoop.mapred.FileInputFormat.getSplits
  [56] org.apache.spark.rdd.HadoopRDD.getPartitions
  [57] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [58] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [59] scala.Option.getOrElse
  [60] org.apache.spark.rdd.RDD.partitions
  [61] org.apache.spark.rdd.MapPartitionsRDD.getPartitions
  [62] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [63] org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply
  [64] scala.Option.getOrElse
  [65] org.apache.spark.rdd.RDD.partitions
  [66] org.apache.spark.api.java.JavaRDDLike$class.partitions
  [67] org.apache.spark.api.java.AbstractJavaRDDLike.partitions
  [68] sun.reflect.NativeMethodAccessorImpl.invoke0
  [69] sun.reflect.NativeMethodAccessorImpl.invoke
  [70] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [71] java.lang.reflect.Method.invoke
  [72] py4j.reflection.MethodInvoker.invoke
  [73] py4j.reflection.ReflectionEngine.invoke
  [74] py4j.Gateway.invoke
  [75] py4j.commands.AbstractCommand.invokeMethod
  [76] py4j.commands.CallCommand.execute
  [77] py4j.GatewayConnection.run
  [78] java.lang.Thread.run
  [79] [tid=16146]

--- 1551158150390717 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.SparkContext.runJob
  [10] org.apache.spark.SparkContext.runJob
  [11] org.apache.spark.api.python.PythonRDD$.runJob
  [12] org.apache.spark.api.python.PythonRDD.runJob
  [13] sun.reflect.NativeMethodAccessorImpl.invoke0
  [14] sun.reflect.NativeMethodAccessorImpl.invoke
  [15] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [16] java.lang.reflect.Method.invoke
  [17] py4j.reflection.MethodInvoker.invoke
  [18] py4j.reflection.ReflectionEngine.invoke
  [19] py4j.Gateway.invoke
  [20] py4j.commands.AbstractCommand.invokeMethod
  [21] py4j.commands.CallCommand.execute
  [22] py4j.GatewayConnection.run
  [23] java.lang.Thread.run
  [24] [tid=16146]

--- 1551158150485475 us
  [ 0] resource_allocate_bytes(unsigned long, AllocFailStrategy::AllocFailEnum)
  [ 1] SignatureStream::SignatureStream(Symbol*, bool)
  [ 2] AdapterHandlerLibrary::get_adapter(methodHandle)
  [ 3] Method::link_method(methodHandle, Thread*) [clone .part.92]
  [ 4] InstanceKlass::link_methods(Thread*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.math.Ordering$class.on
  [10] scala.math.Ordering$Int$.on
  [11] scala.collection.SeqLike$class.sortBy
  [12] scala.collection.AbstractSeq.sortBy
  [13] org.apache.spark.status.AppStatusListener.onJobStart
  [14] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [15] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [16] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [17] org.apache.spark.util.ListenerBus$class.postToAll
  [18] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [19] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [20] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [21] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [22] scala.util.DynamicVariable.withValue
  [23] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [24] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [25] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [26] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [27] [tid=16260]

--- 1551158150555847 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.util.JsonProtocol$.stageInfoToJson
  [10] org.apache.spark.util.JsonProtocol$$anonfun$jobStartToJson$4.apply
  [11] org.apache.spark.util.JsonProtocol$$anonfun$jobStartToJson$4.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.IndexedSeqOptimized$class.foreach
  [15] scala.collection.mutable.WrappedArray.foreach
  [16] scala.collection.TraversableLike$class.map
  [17] scala.collection.AbstractTraversable.map
  [18] org.apache.spark.util.JsonProtocol$.jobStartToJson
  [19] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [20] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [21] org.apache.spark.scheduler.EventLoggingListener.onJobStart
  [22] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [23] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [24] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [25] org.apache.spark.util.ListenerBus$class.postToAll
  [26] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [27] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [28] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [29] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [30] scala.util.DynamicVariable.withValue
  [31] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [32] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [33] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [34] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [35] [tid=16262]

--- 1551158150557406 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1$$anonfun$apply$5.apply
  [10] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1$$anonfun$apply$5.apply
  [11] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [12] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [13] scala.collection.IndexedSeqOptimized$class.foreach
  [14] scala.collection.mutable.ArrayOps$ofRef.foreach
  [15] scala.collection.TraversableLike$class.flatMap
  [16] scala.collection.mutable.ArrayOps$ofRef.flatMap
  [17] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1.apply
  [18] org.apache.spark.rdd.HadoopRDD$$anonfun$convertSplitLocationInfo$1.apply
  [19] scala.Option.map
  [20] org.apache.spark.rdd.HadoopRDD$.convertSplitLocationInfo
  [21] org.apache.spark.rdd.HadoopRDD.getPreferredLocations
  [22] org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply
  [23] org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply
  [24] scala.Option.getOrElse
  [25] org.apache.spark.rdd.RDD.preferredLocations
  [26] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [27] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply$mcVI$sp
  [28] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [29] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [30] scala.collection.immutable.List.foreach
  [31] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [32] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [33] scala.collection.immutable.List.foreach
  [34] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [35] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply$mcVI$sp
  [36] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [37] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$1.apply
  [38] scala.collection.immutable.List.foreach
  [39] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [40] org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply
  [41] scala.collection.immutable.List.foreach
  [42] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal
  [43] org.apache.spark.scheduler.DAGScheduler.getPreferredLocs
  [44] org.apache.spark.scheduler.DAGScheduler$$anonfun$17.apply
  [45] org.apache.spark.scheduler.DAGScheduler$$anonfun$17.apply
  [46] scala.collection.TraversableLike$$anonfun$map$1.apply
  [47] scala.collection.TraversableLike$$anonfun$map$1.apply
  [48] scala.collection.Iterator$class.foreach
  [49] scala.collection.AbstractIterator.foreach
  [50] scala.collection.IterableLike$class.foreach
  [51] scala.collection.AbstractIterable.foreach
  [52] scala.collection.TraversableLike$class.map
  [53] scala.collection.AbstractTraversable.map
  [54] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [55] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [56] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [57] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [58] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [59] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [60] org.apache.spark.util.EventLoop$$anon$1.run
  [61] [tid=16222]

--- 1551158150718646 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$makeOperationGraph$2$$anonfun$apply$2.apply
  [10] org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$makeOperationGraph$2$$anonfun$apply$2.apply
  [11] scala.collection.TraversableLike$$anonfun$map$1.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.immutable.List.foreach
  [14] scala.collection.TraversableLike$class.map
  [15] scala.collection.immutable.List.map
  [16] org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$makeOperationGraph$2.apply
  [17] org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$makeOperationGraph$2.apply
  [18] scala.collection.immutable.List.foreach
  [19] org.apache.spark.ui.scope.RDDOperationGraph$.makeOperationGraph
  [20] org.apache.spark.status.AppStatusListener$$anonfun$onJobStart$2.apply
  [21] org.apache.spark.status.AppStatusListener$$anonfun$onJobStart$2.apply
  [22] scala.collection.IndexedSeqOptimized$class.foreach
  [23] scala.collection.mutable.WrappedArray.foreach
  [24] org.apache.spark.status.AppStatusListener.onJobStart
  [25] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [26] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [27] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [28] org.apache.spark.util.ListenerBus$class.postToAll
  [29] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [30] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [31] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [32] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [33] scala.util.DynamicVariable.withValue
  [34] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [36] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [37] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [38] [tid=16260]

--- 1551158150762580 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [10] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [11] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [13] com.google.protobuf.DescriptorProtos$FileDescriptorProto.initFields
  [14] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<clinit>
  [15] com.google.protobuf.Descriptors$FileDescriptor.internalBuildGeneratedFileFrom
  [16] org.apache.hadoop.security.proto.SecurityProtos.<clinit>
  [17] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [18] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [19] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [20] com.google.protobuf.GeneratedMessage.getAllFields
  [21] com.google.protobuf.TextFormat$Printer.print
  [22] com.google.protobuf.TextFormat$Printer.access$400
  [23] com.google.protobuf.TextFormat.shortDebugString
  [24] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [25] java.lang.String.valueOf
  [26] java.lang.StringBuilder.append
  [27] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [28] [tid=16273]

--- 1551158150782726 us
  [ 0] ConstantPool::impl_klass_ref_index_at(int, bool)
  [ 1] ClassFileParser::parse_constant_pool(Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 4] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 5] JVM_DefineClass
  [ 6] Unsafe_DefineClass_impl(JNIEnv_*, _jstring*, _jbyteArray*, int, int, _jobject*, _jobject*)
  [ 7] Unsafe_DefineClass
  [ 8] sun.misc.Unsafe.defineClass
  [ 9] sun.reflect.ClassDefiner.defineClass
  [10] sun.reflect.MethodAccessorGenerator$1.run
  [11] sun.reflect.MethodAccessorGenerator$1.run
  [12] java.security.AccessController.doPrivileged
  [13] sun.reflect.MethodAccessorGenerator.generate
  [14] sun.reflect.MethodAccessorGenerator.generateSerializationConstructor
  [15] sun.reflect.ReflectionFactory.generateConstructor
  [16] sun.reflect.ReflectionFactory.newConstructorForSerialization
  [17] java.io.ObjectStreamClass.getSerializableConstructor
  [18] java.io.ObjectStreamClass.access$1500
  [19] java.io.ObjectStreamClass$3.run
  [20] java.io.ObjectStreamClass$3.run
  [21] java.security.AccessController.doPrivileged
  [22] java.io.ObjectStreamClass.<init>
  [23] java.io.ObjectStreamClass.lookup
  [24] java.io.ObjectOutputStream.writeObject0
  [25] java.io.ObjectOutputStream.defaultWriteFields
  [26] java.io.ObjectOutputStream.writeSerialData
  [27] java.io.ObjectOutputStream.writeOrdinaryObject
  [28] java.io.ObjectOutputStream.writeObject0
  [29] java.io.ObjectOutputStream.defaultWriteFields
  [30] java.io.ObjectOutputStream.writeSerialData
  [31] java.io.ObjectOutputStream.writeOrdinaryObject
  [32] java.io.ObjectOutputStream.writeObject0
  [33] java.io.ObjectOutputStream.defaultWriteFields
  [34] java.io.ObjectOutputStream.writeSerialData
  [35] java.io.ObjectOutputStream.writeOrdinaryObject
  [36] java.io.ObjectOutputStream.writeObject0
  [37] java.io.ObjectOutputStream.defaultWriteFields
  [38] java.io.ObjectOutputStream.writeSerialData
  [39] java.io.ObjectOutputStream.writeOrdinaryObject
  [40] java.io.ObjectOutputStream.writeObject0
  [41] java.io.ObjectOutputStream.writeObject
  [42] org.apache.spark.serializer.JavaSerializationStream.writeObject
  [43] org.apache.spark.serializer.JavaSerializerInstance.serialize
  [44] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [45] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [46] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [47] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [48] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [49] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [50] org.apache.spark.util.EventLoop$$anon$1.run
  [51] [tid=16222]

--- 1551158150882388 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] com.google.protobuf.DescriptorProtos$FieldDescriptorProto.initFields
  [16] com.google.protobuf.DescriptorProtos$FieldDescriptorProto.<clinit>
  [17] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [18] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [19] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [20] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [21] com.google.protobuf.CodedInputStream.readMessage
  [22] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [23] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [24] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [25] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [26] com.google.protobuf.AbstractParser.parsePartialFrom
  [27] com.google.protobuf.AbstractParser.parseFrom
  [28] com.google.protobuf.AbstractParser.parseFrom
  [29] com.google.protobuf.AbstractParser.parseFrom
  [30] com.google.protobuf.AbstractParser.parseFrom
  [31] com.google.protobuf.DescriptorProtos$FileDescriptorProto.parseFrom
  [32] com.google.protobuf.Descriptors$FileDescriptor.internalBuildGeneratedFileFrom
  [33] org.apache.hadoop.security.proto.SecurityProtos.<clinit>
  [34] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [35] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [36] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [37] com.google.protobuf.GeneratedMessage.getAllFields
  [38] com.google.protobuf.TextFormat$Printer.print
  [39] com.google.protobuf.TextFormat$Printer.access$400
  [40] com.google.protobuf.TextFormat.shortDebugString
  [41] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [42] java.lang.String.valueOf
  [43] java.lang.StringBuilder.append
  [44] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [45] [tid=16273]

--- 1551158150982136 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.math.Ordering$DoubleOrdering$class.reverse
  [10] scala.math.Ordering$Double$.reverse
  [11] org.apache.spark.util.collection.MedianHeap.<init>
  [12] org.apache.spark.scheduler.TaskSetManager.<init>
  [13] org.apache.spark.scheduler.TaskSchedulerImpl.createTaskSetManager
  [14] org.apache.spark.scheduler.TaskSchedulerImpl.submitTasks
  [15] org.apache.spark.scheduler.DAGScheduler.submitMissingTasks
  [16] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage
  [17] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [18] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [19] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [20] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [21] org.apache.spark.util.EventLoop$$anon$1.run
  [22] [tid=16222]

--- 1551158151049537 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [13] com.google.protobuf.DescriptorProtos$DescriptorProto.<init>
  [14] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [15] com.google.protobuf.DescriptorProtos$DescriptorProto$1.parsePartialFrom
  [16] com.google.protobuf.CodedInputStream.readMessage
  [17] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [18] com.google.protobuf.DescriptorProtos$FileDescriptorProto.<init>
  [19] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [20] com.google.protobuf.DescriptorProtos$FileDescriptorProto$1.parsePartialFrom
  [21] com.google.protobuf.AbstractParser.parsePartialFrom
  [22] com.google.protobuf.AbstractParser.parseFrom
  [23] com.google.protobuf.AbstractParser.parseFrom
  [24] com.google.protobuf.AbstractParser.parseFrom
  [25] com.google.protobuf.AbstractParser.parseFrom
  [26] com.google.protobuf.DescriptorProtos$FileDescriptorProto.parseFrom
  [27] com.google.protobuf.Descriptors$FileDescriptor.internalBuildGeneratedFileFrom
  [28] org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.<clinit>
  [29] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.<clinit>
  [30] org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.internalGetFieldAccessorTable
  [31] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [32] com.google.protobuf.GeneratedMessage.getAllFields
  [33] com.google.protobuf.TextFormat$Printer.print
  [34] com.google.protobuf.TextFormat$Printer.access$400
  [35] com.google.protobuf.TextFormat.shortDebugString
  [36] org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.toString
  [37] java.lang.String.valueOf
  [38] java.lang.StringBuilder.append
  [39] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [40] [tid=16273]

--- 1551158156475582 us
  [ 0] [tid=16146]

--- 1551158156575233 us
  [ 0] methodHandle::remove()
  [ 1] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [ 2] org.apache.spark.sql.types.StructType$.apply
  [ 3] org.apache.spark.sql.types.DataType$.parseDataType
  [ 4] org.apache.spark.sql.types.DataType$.fromJson
  [ 5] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [ 6] sun.reflect.NativeMethodAccessorImpl.invoke0
  [ 7] sun.reflect.NativeMethodAccessorImpl.invoke
  [ 8] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [ 9] java.lang.reflect.Method.invoke
  [10] py4j.reflection.MethodInvoker.invoke
  [11] py4j.reflection.ReflectionEngine.invoke
  [12] py4j.Gateway.invoke
  [13] py4j.commands.AbstractCommand.invokeMethod
  [14] py4j.commands.CallCommand.execute
  [15] py4j.GatewayConnection.run
  [16] java.lang.Thread.run
  [17] [tid=16146]

--- 1551158156687688 us
  [ 0] java.util.Arrays.copyOfRange
  [ 1] java.lang.String.<init>
  [ 2] java.lang.String.substring
  [ 3] java.io.File.getName
  [ 4] java.io.UnixFileSystem.getBooleanAttributes
  [ 5] java.io.File.exists
  [ 6] sun.misc.URLClassPath$FileLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.sql.internal.BaseSessionStateBuilder.analyzer
  [16] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply
  [17] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply
  [18] org.apache.spark.sql.internal.SessionState.analyzer$lzycompute
  [19] org.apache.spark.sql.internal.SessionState.analyzer
  [20] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [21] org.apache.spark.sql.execution.QueryExecution.analyzed
  [22] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158156809613 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [10] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [11] org.apache.spark.sql.execution.QueryExecution.analyzed
  [12] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [13] org.apache.spark.sql.Dataset$.ofRows
  [14] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [15] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [16] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16146]

--- 1551158156928499 us
  [ 0] Monitor::jvm_raw_unlock()
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] org.apache.spark.sql.catalyst.analysis.ResolveTimeZone.<init>
  [16] org.apache.spark.sql.catalyst.analysis.Analyzer.batches$lzycompute
  [17] org.apache.spark.sql.catalyst.analysis.Analyzer.batches
  [18] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [19] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [20] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [21] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [22] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [23] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [24] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [25] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [26] org.apache.spark.sql.execution.QueryExecution.analyzed
  [27] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [28] org.apache.spark.sql.Dataset$.ofRows
  [29] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [30] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [31] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158157065189 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown
  [10] org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions
  [11] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse
  [12] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse
  [13] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply
  [14] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply
  [15] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [16] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [19] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown
  [20] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown
  [21] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators
  [22] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators
  [23] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveExpressions
  [24] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressions
  [25] org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply
  [26] org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply
  [27] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [28] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [29] scala.collection.IndexedSeqOptimized$class.foldl
  [30] scala.collection.IndexedSeqOptimized$class.foldLeft
  [31] scala.collection.mutable.WrappedArray.foldLeft
  [32] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [33] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [34] scala.collection.immutable.List.foreach
  [35] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [36] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [37] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [38] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [39] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [41] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [42] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [43] org.apache.spark.sql.execution.QueryExecution.analyzed
  [44] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [45] org.apache.spark.sql.Dataset$.ofRows
  [46] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [47] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [48] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16146]

--- 1551158157170586 us
  [ 0] java.util.zip.ZipFile.getInputStream
  [ 1] java.util.jar.JarFile.getInputStream
  [ 2] sun.misc.URLClassPath$JarLoader$2.getInputStream
  [ 3] sun.misc.Resource.cachedInputStream
  [ 4] sun.misc.Resource.getByteBuffer
  [ 5] java.net.URLClassLoader.defineClass
  [ 6] java.net.URLClassLoader.access$100
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse
  [15] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse
  [16] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [18] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [19] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [21] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [22] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [23] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [24] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$.apply
  [25] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$.apply
  [26] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [27] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [28] scala.collection.LinearSeqOptimized$class.foldLeft
  [29] scala.collection.immutable.List.foldLeft
  [30] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [31] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [32] scala.collection.immutable.List.foreach
  [33] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [34] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [35] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [36] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [37] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [38] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [39] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [40] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [41] org.apache.spark.sql.execution.QueryExecution.analyzed
  [42] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [43] org.apache.spark.sql.Dataset$.ofRows
  [44] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [45] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [46] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [47] sun.reflect.NativeMethodAccessorImpl.invoke0
  [48] sun.reflect.NativeMethodAccessorImpl.invoke
  [49] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [50] java.lang.reflect.Method.invoke
  [51] py4j.reflection.MethodInvoker.invoke
  [52] py4j.reflection.ReflectionEngine.invoke
  [53] py4j.Gateway.invoke
  [54] py4j.commands.AbstractCommand.invokeMethod
  [55] py4j.commands.CallCommand.execute
  [56] py4j.GatewayConnection.run
  [57] java.lang.Thread.run
  [58] [tid=16146]

--- 1551158157284680 us
  [ 0] Symbol::equals(char const*, int) const
  [ 1] SignatureStream::as_symbol(Thread*)
  [ 2] ClassVerifier::change_sig_to_verificationType(SignatureStream*, VerificationType*, Thread*)
  [ 3] ClassVerifier::verify_invoke_instructions(RawBytecodeStream*, unsigned int, StackMapFrame*, bool, bool*, VerificationType, constantPoolHandle, StackMapTable*, Thread*)
  [ 4] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 5] ClassVerifier::verify_class(Thread*)
  [ 6] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] org.apache.spark.sql.catalyst.expressions.AttributeMap$.apply
  [12] org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability$$anonfun$apply$25.applyOrElse
  [13] org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability$$anonfun$apply$25.applyOrElse
  [14] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [15] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [16] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [19] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [21] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [22] org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability$.apply
  [23] org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability$.apply
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [25] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [26] scala.collection.IndexedSeqOptimized$class.foldl
  [27] scala.collection.IndexedSeqOptimized$class.foldLeft
  [28] scala.collection.mutable.WrappedArray.foldLeft
  [29] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [30] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [31] scala.collection.immutable.List.foreach
  [32] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [33] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [34] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [35] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [36] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [37] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [38] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [39] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [40] org.apache.spark.sql.execution.QueryExecution.analyzed
  [41] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [42] org.apache.spark.sql.Dataset$.ofRows
  [43] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [46] sun.reflect.NativeMethodAccessorImpl.invoke0
  [47] sun.reflect.NativeMethodAccessorImpl.invoke
  [48] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [49] java.lang.reflect.Method.invoke
  [50] py4j.reflection.MethodInvoker.invoke
  [51] py4j.reflection.ReflectionEngine.invoke
  [52] py4j.Gateway.invoke
  [53] py4j.commands.AbstractCommand.invokeMethod
  [54] py4j.commands.CallCommand.execute
  [55] py4j.GatewayConnection.run
  [56] java.lang.Thread.run
  [57] [tid=16146]

--- 1551158157391230 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.Dataset$.ofRows
  [10] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [11] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [12] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [13] sun.reflect.NativeMethodAccessorImpl.invoke0
  [14] sun.reflect.NativeMethodAccessorImpl.invoke
  [15] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [16] java.lang.reflect.Method.invoke
  [17] py4j.reflection.MethodInvoker.invoke
  [18] py4j.reflection.ReflectionEngine.invoke
  [19] py4j.Gateway.invoke
  [20] py4j.commands.AbstractCommand.invokeMethod
  [21] py4j.commands.CallCommand.execute
  [22] py4j.GatewayConnection.run
  [23] java.lang.Thread.run
  [24] [tid=16146]

--- 1551158157560142 us
  [ 0] java.util.concurrent.ConcurrentHashMap.putVal
  [ 1] java.lang.String.lastIndexOf
  [ 2] java.lang.String.lastIndexOf
  [ 3] sun.misc.Launcher$AppClassLoader.loadClass
  [ 4] java.lang.ClassLoader.loadClass
  [ 5] sun.misc.Launcher$AppClassLoader.loadClass
  [ 6] java.lang.ClassLoader.loadClass
  [ 7] sun.misc.Launcher$AppClassLoader.loadClass
  [ 8] java.lang.ClassLoader.loadClass
  [ 9] java.lang.ClassLoader.defineClass1
  [10] java.lang.ClassLoader.defineClass
  [11] java.security.SecureClassLoader.defineClass
  [12] java.net.URLClassLoader.defineClass
  [13] java.net.URLClassLoader.access$100
  [14] java.net.URLClassLoader$1.run
  [15] java.net.URLClassLoader$1.run
  [16] java.security.AccessController.doPrivileged
  [17] java.net.URLClassLoader.findClass
  [18] java.lang.ClassLoader.loadClass
  [19] sun.misc.Launcher$AppClassLoader.loadClass
  [20] java.lang.ClassLoader.loadClass
  [21] java.lang.ClassLoader.defineClass1
  [22] java.lang.ClassLoader.defineClass
  [23] java.security.SecureClassLoader.defineClass
  [24] java.net.URLClassLoader.defineClass
  [25] java.net.URLClassLoader.access$100
  [26] java.net.URLClassLoader$1.run
  [27] java.net.URLClassLoader$1.run
  [28] java.security.AccessController.doPrivileged
  [29] java.net.URLClassLoader.findClass
  [30] java.lang.ClassLoader.loadClass
  [31] sun.misc.Launcher$AppClassLoader.loadClass
  [32] java.lang.ClassLoader.loadClass
  [33] scala.reflect.runtime.package$.universe$lzycompute
  [34] scala.reflect.runtime.package$.universe
  [35] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [36] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [37] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [38] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [39] org.apache.spark.sql.Dataset$.ofRows
  [40] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [41] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [42] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [43] sun.reflect.NativeMethodAccessorImpl.invoke0
  [44] sun.reflect.NativeMethodAccessorImpl.invoke
  [45] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [46] java.lang.reflect.Method.invoke
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.AbstractCommand.invokeMethod
  [51] py4j.commands.CallCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16146]

--- 1551158157664902 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] scala.reflect.runtime.package$.universe$lzycompute
  [13] scala.reflect.runtime.package$.universe
  [14] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [15] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [16] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [17] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [18] org.apache.spark.sql.Dataset$.ofRows
  [19] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16146]

--- 1551158157764632 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.package$.universe$lzycompute
  [12] scala.reflect.runtime.package$.universe
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [14] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [16] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [17] org.apache.spark.sql.Dataset$.ofRows
  [18] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.AbstractCommand.invokeMethod
  [29] py4j.commands.CallCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158157907677 us
  [ 0] Rewriter::compute_index_maps()
  [ 1] Rewriter::rewrite_bytecodes(Thread*)
  [ 2] Rewriter::Rewriter(instanceKlassHandle, constantPoolHandle, Array<Method*>*, Thread*)
  [ 3] Rewriter::rewrite(instanceKlassHandle, Thread*)
  [ 4] InstanceKlass::rewrite_class(Thread*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.package$.universe$lzycompute
  [11] scala.reflect.runtime.package$.universe
  [12] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [14] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158158007411 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.package$.universe$lzycompute
  [11] scala.reflect.runtime.package$.universe
  [12] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [14] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158158107127 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.package$.universe$lzycompute
  [11] scala.reflect.runtime.package$.universe
  [12] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [14] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158158213793 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.JavaUniverse.newLazyTreeCopier
  [12] scala.reflect.runtime.JavaUniverse.newLazyTreeCopier
  [13] scala.reflect.api.Trees$class.$init$
  [14] scala.reflect.api.Universe.<init>
  [15] scala.reflect.macros.Universe.<init>
  [16] scala.reflect.internal.SymbolTable.<init>
  [17] scala.reflect.runtime.JavaUniverse.<init>
  [18] scala.reflect.runtime.package$.universe$lzycompute
  [19] scala.reflect.runtime.package$.universe
  [20] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [21] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [22] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [23] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [24] org.apache.spark.sql.Dataset$.ofRows
  [25] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16146]

--- 1551158158316089 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] scala.reflect.internal.Symbols$class.$init$
  [12] scala.reflect.internal.SymbolTable.<init>
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158158423574 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.internal.SymbolTable.<init>
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158158530194 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.nme$lzycompute
  [11] scala.reflect.internal.SymbolTable.nme
  [12] scala.reflect.internal.StdNames$class.$init$
  [13] scala.reflect.internal.SymbolTable.<init>
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158158629931 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.Names$class.body$1
  [11] scala.reflect.internal.Names$class.newTermName
  [12] scala.reflect.internal.SymbolTable.newTermName
  [13] scala.reflect.internal.Names$class.newTermNameCached
  [14] scala.reflect.internal.SymbolTable.newTermNameCached
  [15] scala.reflect.internal.StdNames$KeywordSetBuilder.apply
  [16] scala.reflect.internal.StdNames$Keywords.<init>
  [17] scala.reflect.internal.StdNames$TermNames.<init>
  [18] scala.reflect.internal.StdNames$nme$.<init>
  [19] scala.reflect.internal.SymbolTable.nme$lzycompute
  [20] scala.reflect.internal.SymbolTable.nme
  [21] scala.reflect.internal.StdNames$class.$init$
  [22] scala.reflect.internal.SymbolTable.<init>
  [23] scala.reflect.runtime.JavaUniverse.<init>
  [24] scala.reflect.runtime.package$.universe$lzycompute
  [25] scala.reflect.runtime.package$.universe
  [26] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [28] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [30] org.apache.spark.sql.Dataset$.ofRows
  [31] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [32] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] sun.reflect.NativeMethodAccessorImpl.invoke0
  [35] sun.reflect.NativeMethodAccessorImpl.invoke
  [36] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [37] java.lang.reflect.Method.invoke
  [38] py4j.reflection.MethodInvoker.invoke
  [39] py4j.reflection.ReflectionEngine.invoke
  [40] py4j.Gateway.invoke
  [41] py4j.commands.AbstractCommand.invokeMethod
  [42] py4j.commands.CallCommand.execute
  [43] py4j.GatewayConnection.run
  [44] java.lang.Thread.run
  [45] [tid=16146]

--- 1551158158729777 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [12] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [13] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [14] scala.reflect.internal.Positions$class.$init$
  [15] scala.reflect.internal.SymbolTable.<init>
  [16] scala.reflect.runtime.JavaUniverse.<init>
  [17] scala.reflect.runtime.package$.universe$lzycompute
  [18] scala.reflect.runtime.package$.universe
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [20] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [22] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158158831594 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.ReificationSupport$class.$init$
  [10] scala.reflect.internal.SymbolTable.<init>
  [11] scala.reflect.runtime.JavaUniverse.<init>
  [12] scala.reflect.runtime.package$.universe$lzycompute
  [13] scala.reflect.runtime.package$.universe
  [14] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [15] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [16] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [17] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [18] org.apache.spark.sql.Dataset$.ofRows
  [19] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16146]

--- 1551158158931374 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [10] InstanceKlass::initialize(Thread*)
  [11] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [12] scala.reflect.internal.SymbolTable.definitions$lzycompute
  [13] scala.reflect.internal.SymbolTable.definitions
  [14] scala.reflect.runtime.JavaUniverse.init
  [15] scala.reflect.runtime.JavaUniverse.<init>
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16146]

--- 1551158159031131 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.definitions$lzycompute
  [11] scala.reflect.internal.SymbolTable.definitions
  [12] scala.reflect.runtime.JavaUniverse.init
  [13] scala.reflect.runtime.JavaUniverse.<init>
  [14] scala.reflect.runtime.package$.universe$lzycompute
  [15] scala.reflect.runtime.package$.universe
  [16] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158159130906 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.Names$TermName_S.createCompanionName
  [11] scala.reflect.internal.Names$TermName.body$2
  [12] scala.reflect.internal.Names$TermName.toTypeName
  [13] scala.reflect.internal.Names$class.newTypeName
  [14] scala.reflect.internal.SymbolTable.newTypeName
  [15] scala.reflect.internal.Names$class.newTypeNameCached
  [16] scala.reflect.internal.SymbolTable.newTypeNameCached
  [17] scala.reflect.internal.StdNames$TypeNames.createNameType
  [18] scala.reflect.internal.StdNames$TypeNames.createNameType
  [19] scala.reflect.internal.StdNames$CommonNames.<init>
  [20] scala.reflect.internal.StdNames$Keywords.<init>
  [21] scala.reflect.internal.StdNames$TypeNames.<init>
  [22] scala.reflect.internal.StdNames$tpnme$.<init>
  [23] scala.reflect.internal.SymbolTable.tpnme$lzycompute
  [24] scala.reflect.internal.SymbolTable.tpnme
  [25] scala.reflect.internal.Definitions$ValueClassDefinitions$class.$init$
  [26] scala.reflect.internal.Definitions$DefinitionsClass.<init>
  [27] scala.reflect.internal.Definitions$definitions$.<init>
  [28] scala.reflect.internal.SymbolTable.definitions$lzycompute
  [29] scala.reflect.internal.SymbolTable.definitions
  [30] scala.reflect.runtime.JavaUniverse.init
  [31] scala.reflect.runtime.JavaUniverse.<init>
  [32] scala.reflect.runtime.package$.universe$lzycompute
  [33] scala.reflect.runtime.package$.universe
  [34] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [35] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [36] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [37] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [38] org.apache.spark.sql.Dataset$.ofRows
  [39] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [40] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [41] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [42] sun.reflect.NativeMethodAccessorImpl.invoke0
  [43] sun.reflect.NativeMethodAccessorImpl.invoke
  [44] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [45] java.lang.reflect.Method.invoke
  [46] py4j.reflection.MethodInvoker.invoke
  [47] py4j.reflection.ReflectionEngine.invoke
  [48] py4j.Gateway.invoke
  [49] py4j.commands.AbstractCommand.invokeMethod
  [50] py4j.commands.CallCommand.execute
  [51] py4j.GatewayConnection.run
  [52] java.lang.Thread.run
  [53] [tid=16146]

--- 1551158159230848 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [12] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [13] scala.reflect.internal.Symbols$class.NoSymbol
  [14] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [15] scala.reflect.internal.SymbolTable.NoSymbol
  [16] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [17] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [18] scala.reflect.runtime.JavaUniverse.rootMirror
  [19] scala.reflect.runtime.JavaUniverse.rootMirror
  [20] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [21] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [22] scala.reflect.internal.Definitions$DefinitionsClass.init
  [23] scala.reflect.runtime.JavaUniverse.init
  [24] scala.reflect.runtime.JavaUniverse.<init>
  [25] scala.reflect.runtime.package$.universe$lzycompute
  [26] scala.reflect.runtime.package$.universe
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [28] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [30] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [31] org.apache.spark.sql.Dataset$.ofRows
  [32] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16146]

--- 1551158159330606 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.internal.Symbols$Symbol.<init>
  [13] scala.reflect.internal.Symbols$NoSymbol.<init>
  [14] scala.reflect.runtime.SynchronizedSymbols$$anon$16.<init>
  [15] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [16] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [17] scala.reflect.internal.Symbols$class.NoSymbol
  [18] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [19] scala.reflect.internal.SymbolTable.NoSymbol
  [20] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [21] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [22] scala.reflect.runtime.JavaUniverse.rootMirror
  [23] scala.reflect.runtime.JavaUniverse.rootMirror
  [24] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [25] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [26] scala.reflect.internal.Definitions$DefinitionsClass.init
  [27] scala.reflect.runtime.JavaUniverse.init
  [28] scala.reflect.runtime.JavaUniverse.<init>
  [29] scala.reflect.runtime.package$.universe$lzycompute
  [30] scala.reflect.runtime.package$.universe
  [31] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [32] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [33] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [34] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [35] org.apache.spark.sql.Dataset$.ofRows
  [36] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [37] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [38] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [39] sun.reflect.NativeMethodAccessorImpl.invoke0
  [40] sun.reflect.NativeMethodAccessorImpl.invoke
  [41] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [42] java.lang.reflect.Method.invoke
  [43] py4j.reflection.MethodInvoker.invoke
  [44] py4j.reflection.ReflectionEngine.invoke
  [45] py4j.Gateway.invoke
  [46] py4j.commands.AbstractCommand.invokeMethod
  [47] py4j.commands.CallCommand.execute
  [48] py4j.GatewayConnection.run
  [49] java.lang.Thread.run
  [50] [tid=16146]

--- 1551158159430354 us
  [ 0] scala.collection.mutable.ArrayOps$ofLong.length
  [ 1] scala.collection.IndexedSeqOptimized$class.prefixLengthImpl
  [ 2] scala.collection.IndexedSeqOptimized$class.exists
  [ 3] scala.collection.mutable.ArrayOps$ofLong.exists
  [ 4] scala.collection.SeqLike$class.contains
  [ 5] scala.collection.mutable.ArrayOps$ofLong.contains
  [ 6] scala.reflect.internal.Flags$$anonfun$4.apply$mcZJ$sp
  [ 7] scala.reflect.internal.Flags$$anonfun$4.apply
  [ 8] scala.reflect.internal.Flags$$anonfun$4.apply
  [ 9] scala.collection.TraversableLike$$anonfun$filterImpl$1.apply
  [10] scala.collection.Iterator$class.foreach
  [11] scala.collection.AbstractIterator.foreach
  [12] scala.collection.IterableLike$class.foreach
  [13] scala.collection.AbstractIterable.foreach
  [14] scala.collection.TraversableLike$class.filterImpl
  [15] scala.collection.TraversableLike$class.filterNot
  [16] scala.collection.AbstractTraversable.filterNot
  [17] scala.reflect.internal.Flags.<init>
  [18] scala.reflect.internal.Flags$.<init>
  [19] scala.reflect.internal.Flags$.<clinit>
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.$init$
  [21] scala.reflect.runtime.SynchronizedSymbols$$anon$16.<init>
  [22] scala.reflect.runtime.SynchronizedSymbols$class.makeNoSymbol
  [23] scala.reflect.runtime.JavaUniverse.makeNoSymbol
  [24] scala.reflect.internal.Symbols$class.NoSymbol
  [25] scala.reflect.internal.SymbolTable.NoSymbol$lzycompute
  [26] scala.reflect.internal.SymbolTable.NoSymbol
  [27] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [28] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [29] scala.reflect.runtime.JavaUniverse.rootMirror
  [30] scala.reflect.runtime.JavaUniverse.rootMirror
  [31] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [32] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [33] scala.reflect.internal.Definitions$DefinitionsClass.init
  [34] scala.reflect.runtime.JavaUniverse.init
  [35] scala.reflect.runtime.JavaUniverse.<init>
  [36] scala.reflect.runtime.package$.universe$lzycompute
  [37] scala.reflect.runtime.package$.universe
  [38] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [39] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [40] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [41] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [42] org.apache.spark.sql.Dataset$.ofRows
  [43] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [46] sun.reflect.NativeMethodAccessorImpl.invoke0
  [47] sun.reflect.NativeMethodAccessorImpl.invoke
  [48] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [49] java.lang.reflect.Method.invoke
  [50] py4j.reflection.MethodInvoker.invoke
  [51] py4j.reflection.ReflectionEngine.invoke
  [52] py4j.Gateway.invoke
  [53] py4j.commands.AbstractCommand.invokeMethod
  [54] py4j.commands.CallCommand.execute
  [55] py4j.GatewayConnection.run
  [56] java.lang.Thread.run
  [57] [tid=16146]

--- 1551158159530090 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 9] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [10] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [11] InstanceKlass::initialize(Thread*)
  [12] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackageClass$lzycompute
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackageClass
  [15] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackageClass
  [16] scala.reflect.internal.Mirrors$RootsBase.init
  [17] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [18] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [19] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [20] scala.reflect.runtime.JavaUniverse.rootMirror
  [21] scala.reflect.runtime.JavaUniverse.rootMirror
  [22] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [23] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [24] scala.reflect.internal.Definitions$DefinitionsClass.init
  [25] scala.reflect.runtime.JavaUniverse.init
  [26] scala.reflect.runtime.JavaUniverse.<init>
  [27] scala.reflect.runtime.package$.universe$lzycompute
  [28] scala.reflect.runtime.package$.universe
  [29] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [30] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [31] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [32] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [33] org.apache.spark.sql.Dataset$.ofRows
  [34] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [35] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [36] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [37] sun.reflect.NativeMethodAccessorImpl.invoke0
  [38] sun.reflect.NativeMethodAccessorImpl.invoke
  [39] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [40] java.lang.reflect.Method.invoke
  [41] py4j.reflection.MethodInvoker.invoke
  [42] py4j.reflection.ReflectionEngine.invoke
  [43] py4j.Gateway.invoke
  [44] py4j.commands.AbstractCommand.invokeMethod
  [45] py4j.commands.CallCommand.execute
  [46] py4j.GatewayConnection.run
  [47] java.lang.Thread.run
  [48] [tid=16146]

--- 1551158159629827 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage$lzycompute
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage
  [11] scala.reflect.runtime.JavaMirrors$JavaMirror.EmptyPackage
  [12] scala.reflect.internal.Mirrors$RootsBase.init
  [13] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [14] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [15] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [16] scala.reflect.runtime.JavaUniverse.rootMirror
  [17] scala.reflect.runtime.JavaUniverse.rootMirror
  [18] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [19] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [20] scala.reflect.internal.Definitions$DefinitionsClass.init
  [21] scala.reflect.runtime.JavaUniverse.init
  [22] scala.reflect.runtime.JavaUniverse.<init>
  [23] scala.reflect.runtime.package$.universe$lzycompute
  [24] scala.reflect.runtime.package$.universe
  [25] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [26] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [27] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [28] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [29] org.apache.spark.sql.Dataset$.ofRows
  [30] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [31] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [32] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [33] sun.reflect.NativeMethodAccessorImpl.invoke0
  [34] sun.reflect.NativeMethodAccessorImpl.invoke
  [35] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [36] java.lang.reflect.Method.invoke
  [37] py4j.reflection.MethodInvoker.invoke
  [38] py4j.reflection.ReflectionEngine.invoke
  [39] py4j.Gateway.invoke
  [40] py4j.commands.AbstractCommand.invokeMethod
  [41] py4j.commands.CallCommand.execute
  [42] py4j.GatewayConnection.run
  [43] java.lang.Thread.run
  [44] [tid=16146]

--- 1551158159729561 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [11] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [12] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [13] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [14] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [15] scala.reflect.internal.Symbols$Symbol.info
  [16] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [19] scala.reflect.runtime.Gil$class.gilSynchronized
  [20] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [22] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.gilSynchronizedIfNotThreadsafe
  [23] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [24] scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.info
  [25] scala.reflect.internal.Mirrors$RootsBase.init
  [26] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [27] scala.reflect.runtime.JavaMirrors$class.rootMirror
  [28] scala.reflect.runtime.JavaUniverse.rootMirror$lzycompute
  [29] scala.reflect.runtime.JavaUniverse.rootMirror
  [30] scala.reflect.runtime.JavaUniverse.rootMirror
  [31] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [32] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [33] scala.reflect.internal.Definitions$DefinitionsClass.init
  [34] scala.reflect.runtime.JavaUniverse.init
  [35] scala.reflect.runtime.JavaUniverse.<init>
  [36] scala.reflect.runtime.package$.universe$lzycompute
  [37] scala.reflect.runtime.package$.universe
  [38] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [39] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [40] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [41] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [42] org.apache.spark.sql.Dataset$.ofRows
  [43] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [46] sun.reflect.NativeMethodAccessorImpl.invoke0
  [47] sun.reflect.NativeMethodAccessorImpl.invoke
  [48] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [49] java.lang.reflect.Method.invoke
  [50] py4j.reflection.MethodInvoker.invoke
  [51] py4j.reflection.ReflectionEngine.invoke
  [52] py4j.Gateway.invoke
  [53] py4j.commands.AbstractCommand.invokeMethod
  [54] py4j.commands.CallCommand.execute
  [55] py4j.GatewayConnection.run
  [56] java.lang.Thread.run
  [57] [tid=16146]

--- 1551158159829316 us
  [ 0] __vdso_clock_gettime
  [ 1] __GI___clock_gettime
  [ 2] [unknown]
  [ 3] scala.reflect.internal.Mirrors$RootsBase.missingHook
  [ 4] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [ 5] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [ 6] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [ 7] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [ 8] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [ 9] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [10] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [11] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [12] scala.reflect.internal.Definitions$DefinitionsClass.init
  [13] scala.reflect.runtime.JavaUniverse.init
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158159929065 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.Predef$.byteArrayOps
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [11] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [12] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [13] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [14] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [15] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [16] scala.reflect.internal.Symbols$Symbol.info
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [19] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [20] scala.reflect.runtime.Gil$class.gilSynchronized
  [21] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [22] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [23] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [24] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [26] scala.reflect.internal.SymbolTable.openPackageModule
  [27] scala.reflect.internal.SymbolTable.openPackageModule
  [28] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [29] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [30] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [31] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [32] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [33] scala.reflect.internal.Symbols$Symbol.info
  [34] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [37] scala.reflect.runtime.Gil$class.gilSynchronized
  [38] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [39] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [40] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [41] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [42] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [43] scala.reflect.internal.Types$TypeRef.thisInfo
  [44] scala.reflect.internal.Types$TypeRef.baseClasses
  [45] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [46] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [47] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [48] scala.reflect.internal.Types$Type.findMember
  [49] scala.reflect.internal.Types$Type.memberBasedOnName
  [50] scala.reflect.internal.Types$Type.member
  [51] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [52] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [53] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [54] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [55] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [56] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [57] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [58] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [59] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [60] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [61] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [62] scala.reflect.internal.Mirrors$RootsBase.init
  [63] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [64] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [65] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [66] scala.reflect.runtime.Gil$class.gilSynchronized
  [67] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [68] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [69] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [70] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [71] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [72] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [73] scala.reflect.runtime.Gil$class.gilSynchronized
  [74] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [75] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [76] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [77] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [78] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [79] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [80] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [81] scala.reflect.internal.Types$Type.findMember
  [82] scala.reflect.internal.Types$Type.memberBasedOnName
  [83] scala.reflect.internal.Types$Type.member
  [84] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [85] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [86] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [87] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [88] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [89] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [90] scala.reflect.internal.Definitions$DefinitionsClass.init
  [91] scala.reflect.runtime.JavaUniverse.init
  [92] scala.reflect.runtime.JavaUniverse.<init>
  [93] scala.reflect.runtime.package$.universe$lzycompute
  [94] scala.reflect.runtime.package$.universe
  [95] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [96] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [97] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [98] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [99] org.apache.spark.sql.Dataset$.ofRows
  [100] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [101] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [102] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [103] sun.reflect.NativeMethodAccessorImpl.invoke0
  [104] sun.reflect.NativeMethodAccessorImpl.invoke
  [105] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [106] java.lang.reflect.Method.invoke
  [107] py4j.reflection.MethodInvoker.invoke
  [108] py4j.reflection.ReflectionEngine.invoke
  [109] py4j.Gateway.invoke
  [110] py4j.commands.AbstractCommand.invokeMethod
  [111] py4j.commands.CallCommand.execute
  [112] py4j.GatewayConnection.run
  [113] java.lang.Thread.run
  [114] [tid=16146]

--- 1551158160029046 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createMethodSymbol
  [16] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.createMethodSymbol
  [17] scala.reflect.internal.Symbols$Symbol.newTermSymbol
  [18] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [19] scala.reflect.internal.pickling.UnPickler$Scan.run
  [20] scala.reflect.internal.pickling.UnPickler.unpickle
  [21] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [22] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [23] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [24] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [25] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [26] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [27] scala.reflect.internal.Symbols$Symbol.info
  [28] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [29] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [30] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [31] scala.reflect.runtime.Gil$class.gilSynchronized
  [32] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [33] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [34] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [35] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [37] scala.reflect.internal.SymbolTable.openPackageModule
  [38] scala.reflect.internal.SymbolTable.openPackageModule
  [39] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [40] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [41] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [42] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [43] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [44] scala.reflect.internal.Symbols$Symbol.info
  [45] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [46] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [47] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [48] scala.reflect.runtime.Gil$class.gilSynchronized
  [49] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [50] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [51] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [52] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [53] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [54] scala.reflect.internal.Types$TypeRef.thisInfo
  [55] scala.reflect.internal.Types$TypeRef.baseClasses
  [56] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [57] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [58] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [59] scala.reflect.internal.Types$Type.findMember
  [60] scala.reflect.internal.Types$Type.memberBasedOnName
  [61] scala.reflect.internal.Types$Type.member
  [62] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [63] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [64] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [65] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [66] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [67] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [68] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [69] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [70] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [71] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [72] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [73] scala.reflect.internal.Mirrors$RootsBase.init
  [74] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [75] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [76] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [77] scala.reflect.runtime.Gil$class.gilSynchronized
  [78] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [79] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [80] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [81] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [82] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [83] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [84] scala.reflect.runtime.Gil$class.gilSynchronized
  [85] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [86] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [87] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [88] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [89] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [90] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [91] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [92] scala.reflect.internal.Types$Type.findMember
  [93] scala.reflect.internal.Types$Type.memberBasedOnName
  [94] scala.reflect.internal.Types$Type.member
  [95] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [96] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [97] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [98] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [99] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [100] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [101] scala.reflect.internal.Definitions$DefinitionsClass.init
  [102] scala.reflect.runtime.JavaUniverse.init
  [103] scala.reflect.runtime.JavaUniverse.<init>
  [104] scala.reflect.runtime.package$.universe$lzycompute
  [105] scala.reflect.runtime.package$.universe
  [106] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [107] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [108] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [109] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [110] org.apache.spark.sql.Dataset$.ofRows
  [111] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [112] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [113] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [114] sun.reflect.NativeMethodAccessorImpl.invoke0
  [115] sun.reflect.NativeMethodAccessorImpl.invoke
  [116] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [117] java.lang.reflect.Method.invoke
  [118] py4j.reflection.MethodInvoker.invoke
  [119] py4j.reflection.ReflectionEngine.invoke
  [120] py4j.Gateway.invoke
  [121] py4j.commands.AbstractCommand.invokeMethod
  [122] py4j.commands.CallCommand.execute
  [123] py4j.GatewayConnection.run
  [124] java.lang.Thread.run
  [125] [tid=16146]

--- 1551158160031765 us
  [ 0] ParallelTaskTerminator::offer_termination(TerminatorTerminator*)
  [ 1] StealTask::do_it(GCTaskManager*, unsigned int)
  [ 2] GCTaskThread::run()
  [ 3] java_start(Thread*)
  [ 4] start_thread
  [ 5] [tid=16074]

--- 1551158160154414 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createClassSymbol
  [10] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.createClassSymbol
  [11] scala.reflect.internal.Symbols$Symbol.newClassSymbol
  [12] scala.reflect.runtime.SymbolLoaders$class.initAndEnterClassAndModule
  [13] scala.reflect.runtime.JavaUniverse.initAndEnterClassAndModule
  [14] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.Gil$class.gilSynchronized
  [17] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [18] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [20] scala.reflect.internal.Types$Type.findDecl
  [21] scala.reflect.internal.Types$Type.decl
  [22] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [23] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [24] scala.reflect.runtime.Gil$class.gilSynchronized
  [25] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [26] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [27] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [28] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [29] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [30] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [31] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [32] scala.reflect.internal.Types$Type.findMember
  [33] scala.reflect.internal.Types$Type.memberBasedOnName
  [34] scala.reflect.internal.Types$Type.member
  [35] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [36] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [37] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [38] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [39] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [40] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [41] scala.reflect.internal.Definitions$DefinitionsClass.ObjectTpe$lzycompute
  [42] scala.reflect.internal.Definitions$DefinitionsClass.ObjectTpe
  [43] scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass$lzycompute
  [44] scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass
  [45] scala.reflect.runtime.JavaMirrors$class.missingHook
  [46] scala.reflect.runtime.JavaUniverse.missingHook
  [47] scala.reflect.internal.Mirrors$RootsBase.universeMissingHook
  [48] scala.reflect.internal.Mirrors$RootsBase.missingHook
  [49] scala.reflect.internal.pickling.UnPickler$Scan.readExtSymbol$1
  [50] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [51] scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef
  [52] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [53] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [54] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [55] scala.reflect.internal.pickling.UnPickler$Scan.at
  [56] scala.reflect.internal.pickling.UnPickler$Scan.readTypeRef
  [57] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypes$1$1.apply
  [58] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypes$1$1.apply
  [59] scala.reflect.internal.pickling.PickleBuffer.until
  [60] scala.reflect.internal.pickling.UnPickler$Scan.readTypes$1
  [61] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [62] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [63] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [64] scala.reflect.internal.pickling.UnPickler$Scan.at
  [65] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.completeInternal
  [66] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete
  [67] scala.reflect.internal.Symbols$Symbol.info
  [68] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [69] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [70] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [71] scala.reflect.runtime.Gil$class.gilSynchronized
  [72] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [73] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [74] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.gilSynchronizedIfNotThreadsafe
  [75] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [76] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$3.info
  [77] scala.reflect.internal.Types$TypeRef.decls
  [78] scala.reflect.internal.SymbolTable.openPackageModule
  [79] scala.reflect.internal.SymbolTable.openPackageModule
  [80] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [81] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [82] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [83] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [84] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [85] scala.reflect.internal.Symbols$Symbol.info
  [86] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [87] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [88] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [89] scala.reflect.runtime.Gil$class.gilSynchronized
  [90] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [91] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [92] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [93] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [94] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [95] scala.reflect.internal.Types$TypeRef.thisInfo
  [96] scala.reflect.internal.Types$TypeRef.baseClasses
  [97] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [98] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [99] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [100] scala.reflect.internal.Types$Type.findMember
  [101] scala.reflect.internal.Types$Type.memberBasedOnName
  [102] scala.reflect.internal.Types$Type.member
  [103] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [104] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [105] scala.reflect.internal.Mirrors$RootsBase.getPackage
  [106] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage$lzycompute
  [107] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackage
  [108] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass$lzycompute
  [109] scala.reflect.internal.Definitions$DefinitionsClass.RuntimePackageClass
  [110] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr$lzycompute
  [111] scala.reflect.internal.Definitions$DefinitionsClass.AnnotationDefaultAttr
  [112] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [113] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [114] scala.reflect.internal.Mirrors$RootsBase.init
  [115] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [116] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [117] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [118] scala.reflect.runtime.Gil$class.gilSynchronized
  [119] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [120] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [121] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [122] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [123] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [124] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [125] scala.reflect.runtime.Gil$class.gilSynchronized
  [126] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [127] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [128] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [129] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [130] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [131] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [132] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [133] scala.reflect.internal.Types$Type.findMember
  [134] scala.reflect.internal.Types$Type.memberBasedOnName
  [135] scala.reflect.internal.Types$Type.member
  [136] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [137] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [138] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [139] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [140] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [141] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [142] scala.reflect.internal.Definitions$DefinitionsClass.init
  [143] scala.reflect.runtime.JavaUniverse.init
  [144] scala.reflect.runtime.JavaUniverse.<init>
  [145] scala.reflect.runtime.package$.universe$lzycompute
  [146] scala.reflect.runtime.package$.universe
  [147] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [148] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [149] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [150] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [151] org.apache.spark.sql.Dataset$.ofRows
  [152] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [153] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [154] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [155] sun.reflect.NativeMethodAccessorImpl.invoke0
  [156] sun.reflect.NativeMethodAccessorImpl.invoke
  [157] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [158] java.lang.reflect.Method.invoke
  [159] py4j.reflection.MethodInvoker.invoke
  [160] py4j.reflection.ReflectionEngine.invoke
  [161] py4j.Gateway.invoke
  [162] py4j.commands.AbstractCommand.invokeMethod
  [163] py4j.commands.CallCommand.execute
  [164] py4j.GatewayConnection.run
  [165] java.lang.Thread.run
  [166] [tid=16146]

--- 1551158160256169 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.Types$TypeRef$.apply
  [11] scala.reflect.internal.Types$class.copyTypeRef
  [12] scala.reflect.internal.SymbolTable.copyTypeRef
  [13] scala.reflect.internal.Types$class.appliedType
  [14] scala.reflect.internal.SymbolTable.appliedType
  [15] scala.reflect.internal.Types$class.appliedType
  [16] scala.reflect.internal.SymbolTable.appliedType
  [17] scala.reflect.internal.Definitions$DefinitionsClass.seqType
  [18] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$RepeatedParamClass$1.apply
  [19] scala.reflect.internal.Definitions$DefinitionsClass$$anonfun$RepeatedParamClass$1.apply
  [20] scala.reflect.internal.Definitions$DefinitionsClass.specialPolyClass
  [21] scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass$lzycompute
  [22] scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass
  [23] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute
  [24] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses
  [25] scala.reflect.internal.Mirrors$RootsBase.init
  [26] scala.reflect.runtime.JavaMirrors$class.scala$reflect$runtime$JavaMirrors$$createMirror
  [27] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [28] scala.reflect.runtime.JavaMirrors$$anonfun$runtimeMirror$1.apply
  [29] scala.reflect.runtime.Gil$class.gilSynchronized
  [30] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [31] scala.reflect.runtime.JavaMirrors$class.runtimeMirror
  [32] scala.reflect.runtime.JavaUniverse.runtimeMirror
  [33] scala.reflect.runtime.JavaMirrors$JavaMirror.mirrorDefining
  [34] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [35] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [36] scala.reflect.runtime.Gil$class.gilSynchronized
  [37] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [38] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [39] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [40] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [41] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [42] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [43] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [44] scala.reflect.internal.Types$Type.findMember
  [45] scala.reflect.internal.Types$Type.memberBasedOnName
  [46] scala.reflect.internal.Types$Type.member
  [47] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [48] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [49] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [50] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [51] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute
  [52] scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass
  [53] scala.reflect.internal.Definitions$DefinitionsClass.init
  [54] scala.reflect.runtime.JavaUniverse.init
  [55] scala.reflect.runtime.JavaUniverse.<init>
  [56] scala.reflect.runtime.package$.universe$lzycompute
  [57] scala.reflect.runtime.package$.universe
  [58] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [59] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [60] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [61] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [62] org.apache.spark.sql.Dataset$.ofRows
  [63] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [64] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [65] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [66] sun.reflect.NativeMethodAccessorImpl.invoke0
  [67] sun.reflect.NativeMethodAccessorImpl.invoke
  [68] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [69] java.lang.reflect.Method.invoke
  [70] py4j.reflection.MethodInvoker.invoke
  [71] py4j.reflection.ReflectionEngine.invoke
  [72] py4j.Gateway.invoke
  [73] py4j.commands.AbstractCommand.invokeMethod
  [74] py4j.commands.CallCommand.execute
  [75] py4j.GatewayConnection.run
  [76] java.lang.Thread.run
  [77] [tid=16146]

--- 1551158160356450 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.AnnotationInfos$Annotatable$class.addThrowsAnnotation
  [11] scala.reflect.internal.Symbols$Symbol.addThrowsAnnotation
  [12] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$copyAnnotations
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [15] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [16] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [17] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [18] scala.reflect.runtime.Gil$class.gilSynchronized
  [19] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [20] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [21] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [22] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [23] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [24] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [25] scala.collection.IndexedSeqOptimized$class.foreach
  [26] scala.collection.mutable.ArrayOps$ofRef.foreach
  [27] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [28] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [29] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [30] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [31] scala.reflect.runtime.Gil$class.gilSynchronized
  [32] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [33] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [34] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.complete
  [35] scala.reflect.internal.Symbols$Symbol.info
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [37] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [38] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [39] scala.reflect.runtime.Gil$class.gilSynchronized
  [40] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [41] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [42] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [43] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [44] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [45] scala.reflect.internal.Symbols$Symbol.initialize
  [46] scala.reflect.internal.Definitions$DefinitionsClass.init
  [47] scala.reflect.runtime.JavaUniverse.init
  [48] scala.reflect.runtime.JavaUniverse.<init>
  [49] scala.reflect.runtime.package$.universe$lzycompute
  [50] scala.reflect.runtime.package$.universe
  [51] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [52] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [53] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [54] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [55] org.apache.spark.sql.Dataset$.ofRows
  [56] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [57] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [58] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [59] sun.reflect.NativeMethodAccessorImpl.invoke0
  [60] sun.reflect.NativeMethodAccessorImpl.invoke
  [61] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [62] java.lang.reflect.Method.invoke
  [63] py4j.reflection.MethodInvoker.invoke
  [64] py4j.reflection.ReflectionEngine.invoke
  [65] py4j.Gateway.invoke
  [66] py4j.commands.AbstractCommand.invokeMethod
  [67] py4j.commands.CallCommand.execute
  [68] py4j.GatewayConnection.run
  [69] java.lang.Thread.run
  [70] [tid=16146]

--- 1551158160456713 us
  [ 0] __lock_text_start_[k]
  [ 1] try_to_wake_up_[k]
  [ 2] wake_up_q_[k]
  [ 3] futex_wake_[k]
  [ 4] do_futex_[k]
  [ 5] sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __pthread_cond_signal
  [ 9] scala.reflect.internal.pickling.UnPickler$Scan.scala$reflect$internal$pickling$UnPickler$Scan$$$outer
  [10] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [11] scala.reflect.internal.pickling.UnPickler$Scan.run
  [12] scala.reflect.internal.pickling.UnPickler.unpickle
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [14] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [15] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [17] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [18] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [19] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.load
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [22] scala.reflect.runtime.Gil$class.gilSynchronized
  [23] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [24] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [26] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.typeParams
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.typeParams
  [28] scala.reflect.internal.Types$class.isRawIfWithoutArgs
  [29] scala.reflect.internal.SymbolTable.isRawIfWithoutArgs
  [30] scala.reflect.internal.tpe.TypeMaps$$anon$1.apply
  [31] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [32] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [33] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$22.apply
  [34] scala.collection.immutable.List.map
  [35] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [37] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [38] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [39] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [40] scala.reflect.runtime.Gil$class.gilSynchronized
  [41] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [42] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [43] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [44] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [45] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [46] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [47] scala.collection.IndexedSeqOptimized$class.foreach
  [48] scala.collection.mutable.ArrayOps$ofRef.foreach
  [49] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [50] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [51] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [52] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [53] scala.reflect.runtime.Gil$class.gilSynchronized
  [54] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [55] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [56] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.complete
  [57] scala.reflect.internal.Symbols$Symbol.info
  [58] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [59] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [60] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [61] scala.reflect.runtime.Gil$class.gilSynchronized
  [62] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [63] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [64] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [65] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [66] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [67] scala.reflect.internal.Symbols$Symbol.initialize
  [68] scala.reflect.internal.Definitions$DefinitionsClass.init
  [69] scala.reflect.runtime.JavaUniverse.init
  [70] scala.reflect.runtime.JavaUniverse.<init>
  [71] scala.reflect.runtime.package$.universe$lzycompute
  [72] scala.reflect.runtime.package$.universe
  [73] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [74] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [75] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [76] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [77] org.apache.spark.sql.Dataset$.ofRows
  [78] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [79] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [80] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [81] sun.reflect.NativeMethodAccessorImpl.invoke0
  [82] sun.reflect.NativeMethodAccessorImpl.invoke
  [83] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [84] java.lang.reflect.Method.invoke
  [85] py4j.reflection.MethodInvoker.invoke
  [86] py4j.reflection.ReflectionEngine.invoke
  [87] py4j.Gateway.invoke
  [88] py4j.commands.AbstractCommand.invokeMethod
  [89] py4j.commands.CallCommand.execute
  [90] py4j.GatewayConnection.run
  [91] java.lang.Thread.run
  [92] [tid=16146]

--- 1551158160558081 us
  [ 0] scala.reflect.internal.pickling.UnPickler$Scan.run
  [ 1] scala.reflect.internal.pickling.UnPickler.unpickle
  [ 2] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [ 3] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [ 4] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [ 5] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [ 6] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [ 7] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [ 8] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.load
  [ 9] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [10] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$typeParams$1.apply
  [11] scala.reflect.runtime.Gil$class.gilSynchronized
  [12] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [13] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [14] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.typeParams
  [16] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.typeParams
  [17] scala.reflect.internal.Types$class.isRawIfWithoutArgs
  [18] scala.reflect.internal.SymbolTable.isRawIfWithoutArgs
  [19] scala.reflect.internal.tpe.TypeMaps$$anon$1.apply
  [20] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [21] scala.reflect.runtime.JavaMirrors$JavaMirror.typeToScala
  [22] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala1
  [23] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala$1.apply
  [24] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala$1.apply
  [25] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [26] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [27] scala.reflect.runtime.Gil$class.gilSynchronized
  [28] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [29] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [30] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [31] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jfieldAsScala
  [32] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$3.apply
  [33] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$3.apply
  [34] scala.collection.IndexedSeqOptimized$class.foreach
  [35] scala.collection.mutable.ArrayOps$ofRef.foreach
  [36] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [37] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [38] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [39] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [40] scala.reflect.runtime.Gil$class.gilSynchronized
  [41] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [42] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [43] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$LazyPolyType.complete
  [44] scala.reflect.internal.Symbols$Symbol.info
  [45] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [46] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [47] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [48] scala.reflect.runtime.Gil$class.gilSynchronized
  [49] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [50] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [51] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [52] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [53] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [54] scala.reflect.internal.Definitions$class.scala$reflect$internal$Definitions$$enterNewMethod
  [55] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus$lzycompute
  [56] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus
  [57] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods$lzycompute
  [58] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods
  [59] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode$lzycompute
  [60] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode
  [61] scala.reflect.internal.Definitions$DefinitionsClass.init
  [62] scala.reflect.runtime.JavaUniverse.init
  [63] scala.reflect.runtime.JavaUniverse.<init>
  [64] scala.reflect.runtime.package$.universe$lzycompute
  [65] scala.reflect.runtime.package$.universe
  [66] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [67] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [68] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [69] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [70] org.apache.spark.sql.Dataset$.ofRows
  [71] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [72] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [73] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [74] sun.reflect.NativeMethodAccessorImpl.invoke0
  [75] sun.reflect.NativeMethodAccessorImpl.invoke
  [76] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [77] java.lang.reflect.Method.invoke
  [78] py4j.reflection.MethodInvoker.invoke
  [79] py4j.reflection.ReflectionEngine.invoke
  [80] py4j.Gateway.invoke
  [81] py4j.commands.AbstractCommand.invokeMethod
  [82] py4j.commands.CallCommand.execute
  [83] py4j.GatewayConnection.run
  [84] java.lang.Thread.run
  [85] [tid=16146]

--- 1551158160658703 us
  [ 0] binary_search(Array<Method*>*, Symbol*)
  [ 1] InstanceKlass::find_method_index(Array<Method*>*, Symbol*, Symbol*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.290]
  [ 2] InstanceKlass::uncached_lookup_method(Symbol*, Symbol*, Klass::OverpassLookupMode) const
  [ 3] LinkResolver::lookup_instance_method_in_klasses(methodHandle&, KlassHandle, Symbol*, Symbol*, Thread*)
  [ 4] LinkResolver::runtime_resolve_interface_method(CallInfo&, methodHandle, KlassHandle, Handle, KlassHandle, bool, Thread*)
  [ 5] LinkResolver::resolve_invokeinterface(CallInfo&, Handle, constantPoolHandle, int, Thread*)
  [ 6] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 7] SharedRuntime::find_callee_info_helper(JavaThread*, vframeStream&, Bytecodes::Code&, CallInfo&, Thread*)
  [ 8] SharedRuntime::resolve_sub_helper(JavaThread*, bool, bool, Thread*)
  [ 9] SharedRuntime::resolve_helper(JavaThread*, bool, bool, Thread*)
  [10] SharedRuntime::resolve_virtual_call_C(JavaThread*)
  [11] scala.collection.IndexedSeqOptimized$class.foreach
  [12] scala.collection.mutable.WrappedArray.foreach
  [13] scala.collection.generic.Growable$class.$plus$plus$eq
  [14] scala.collection.mutable.ListBuffer.$plus$plus$eq
  [15] scala.collection.mutable.ListBuffer.$plus$plus$eq
  [16] scala.collection.TraversableLike$class.to
  [17] scala.collection.AbstractTraversable.to
  [18] scala.collection.TraversableOnce$class.toList
  [19] scala.collection.AbstractTraversable.toList
  [20] scala.collection.immutable.List$.apply
  [21] scala.reflect.internal.Symbols$$anonfun$relevantSymbols$1.apply
  [22] scala.reflect.internal.Symbols$$anonfun$relevantSymbols$1.apply
  [23] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [24] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [25] scala.collection.IndexedSeqOptimized$class.foreach
  [26] scala.collection.mutable.WrappedArray.foreach
  [27] scala.collection.TraversableLike$class.flatMap
  [28] scala.collection.AbstractTraversable.flatMap
  [29] scala.reflect.internal.Symbols$class.relevantSymbols
  [30] scala.reflect.internal.Symbols$class.markFlagsCompleted
  [31] scala.reflect.internal.SymbolTable.markFlagsCompleted
  [32] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.<init>
  [33] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1$$anonfun$1.apply
  [34] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1$$anonfun$1.apply
  [35] scala.reflect.runtime.SymbolLoaders$class.initAndEnterClassAndModule
  [36] scala.reflect.runtime.JavaUniverse.initAndEnterClassAndModule
  [37] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [38] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [39] scala.reflect.runtime.Gil$class.gilSynchronized
  [40] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [41] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [42] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [43] scala.reflect.internal.Types$Type.findDecl
  [44] scala.reflect.internal.Types$Type.decl
  [45] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$coreLookup$1
  [46] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$lookupClass$1
  [47] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1
  [48] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [49] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply
  [50] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [51] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [52] scala.reflect.runtime.Gil$class.gilSynchronized
  [53] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [54] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [55] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [56] scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala
  [57] scala.reflect.runtime.JavaMirrors$JavaMirror$JavaAnnotationProxy.<init>
  [58] scala.reflect.runtime.JavaMirrors$JavaMirror$JavaAnnotationProxy$.apply
  [59] scala.reflect.runtime.JavaMirrors$JavaMirror$JavaAnnotationProxy$.apply
  [60] scala.collection.TraversableLike$$anonfun$map$1.apply
  [61] scala.collection.TraversableLike$$anonfun$map$1.apply
  [62] scala.collection.IndexedSeqOptimized$class.foreach
  [63] scala.collection.mutable.ArrayOps$ofRef.foreach
  [64] scala.collection.TraversableLike$class.map
  [65] scala.collection.mutable.ArrayOps$ofRef.map
  [66] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$copyAnnotations
  [67] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala1
  [68] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [69] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala$1.apply
  [70] scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply
  [71] scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply
  [72] scala.reflect.runtime.Gil$class.gilSynchronized
  [73] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [74] scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala
  [75] scala.reflect.runtime.JavaMirrors$JavaMirror.toScala
  [76] scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$jmethodAsScala
  [77] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [78] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$4.apply
  [79] scala.collection.IndexedSeqOptimized$class.foreach
  [80] scala.collection.mutable.ArrayOps$ofRef.foreach
  [81] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [82] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply$mcV$sp
  [83] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [84] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$$anonfun$completeRest$1.apply
  [85] scala.reflect.runtime.Gil$class.gilSynchronized
  [86] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [87] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter.completeRest
  [88] scala.reflect.runtime.JavaMirrors$JavaMirror$FromJavaClassCompleter$LazyPolyType.complete
  [89] scala.reflect.internal.Symbols$Symbol.info
  [90] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [91] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [92] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [93] scala.reflect.runtime.Gil$class.gilSynchronized
  [94] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [95] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [96] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [97] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [98] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [99] scala.reflect.internal.Definitions$class.scala$reflect$internal$Definitions$$enterNewMethod
  [100] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus$lzycompute
  [101] scala.reflect.internal.Definitions$DefinitionsClass.String_$plus
  [102] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods$lzycompute
  [103] scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreMethods
  [104] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode$lzycompute
  [105] scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode
  [106] scala.reflect.internal.Definitions$DefinitionsClass.init
  [107] scala.reflect.runtime.JavaUniverse.init
  [108] scala.reflect.runtime.JavaUniverse.<init>
  [109] scala.reflect.runtime.package$.universe$lzycompute
  [110] scala.reflect.runtime.package$.universe
  [111] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [112] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [113] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [114] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [115] org.apache.spark.sql.Dataset$.ofRows
  [116] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [117] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [118] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [119] sun.reflect.NativeMethodAccessorImpl.invoke0
  [120] sun.reflect.NativeMethodAccessorImpl.invoke
  [121] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [122] java.lang.reflect.Method.invoke
  [123] py4j.reflection.MethodInvoker.invoke
  [124] py4j.reflection.ReflectionEngine.invoke
  [125] py4j.Gateway.invoke
  [126] py4j.commands.AbstractCommand.invokeMethod
  [127] py4j.commands.CallCommand.execute
  [128] py4j.GatewayConnection.run
  [129] java.lang.Thread.run
  [130] [tid=16146]

--- 1551158160759942 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.reflect.api.TypeTags$TypeTag$.<init>
  [16] scala.reflect.api.Universe.TypeTag$lzycompute
  [17] scala.reflect.api.Universe.TypeTag
  [18] scala.reflect.api.TypeTags$WeakTypeTag$.<init>
  [19] scala.reflect.api.Universe.WeakTypeTag$lzycompute
  [20] scala.reflect.api.Universe.WeakTypeTag
  [21] scala.reflect.runtime.JavaUniverseForce$class.force
  [22] scala.reflect.runtime.JavaUniverse.force
  [23] scala.reflect.runtime.JavaUniverse.init
  [24] scala.reflect.runtime.JavaUniverse.<init>
  [25] scala.reflect.runtime.package$.universe$lzycompute
  [26] scala.reflect.runtime.package$.universe
  [27] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [28] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [29] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [30] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [31] org.apache.spark.sql.Dataset$.ofRows
  [32] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [33] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [34] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [35] sun.reflect.NativeMethodAccessorImpl.invoke0
  [36] sun.reflect.NativeMethodAccessorImpl.invoke
  [37] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [38] java.lang.reflect.Method.invoke
  [39] py4j.reflection.MethodInvoker.invoke
  [40] py4j.reflection.ReflectionEngine.invoke
  [41] py4j.Gateway.invoke
  [42] py4j.commands.AbstractCommand.invokeMethod
  [43] py4j.commands.CallCommand.execute
  [44] py4j.GatewayConnection.run
  [45] java.lang.Thread.run
  [46] [tid=16146]

--- 1551158160859683 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] scala.reflect.runtime.JavaUniverse$$anon$1.<init>
  [13] scala.reflect.runtime.JavaUniverse.internal$lzycompute
  [14] scala.reflect.runtime.JavaUniverse.internal
  [15] scala.reflect.runtime.JavaUniverseForce$class.force
  [16] scala.reflect.runtime.JavaUniverse.force
  [17] scala.reflect.runtime.JavaUniverse.init
  [18] scala.reflect.runtime.JavaUniverse.<init>
  [19] scala.reflect.runtime.package$.universe$lzycompute
  [20] scala.reflect.runtime.package$.universe
  [21] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [22] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [23] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [24] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [25] org.apache.spark.sql.Dataset$.ofRows
  [26] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [27] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [28] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [29] sun.reflect.NativeMethodAccessorImpl.invoke0
  [30] sun.reflect.NativeMethodAccessorImpl.invoke
  [31] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [32] java.lang.reflect.Method.invoke
  [33] py4j.reflection.MethodInvoker.invoke
  [34] py4j.reflection.ReflectionEngine.invoke
  [35] py4j.Gateway.invoke
  [36] py4j.commands.AbstractCommand.invokeMethod
  [37] py4j.commands.CallCommand.execute
  [38] py4j.GatewayConnection.run
  [39] java.lang.Thread.run
  [40] [tid=16146]

--- 1551158160959417 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.SymbolTable.CompoundTypeTreeOriginalAttachment$lzycompute
  [10] scala.reflect.internal.SymbolTable.CompoundTypeTreeOriginalAttachment
  [11] scala.reflect.runtime.JavaUniverseForce$class.force
  [12] scala.reflect.runtime.JavaUniverse.force
  [13] scala.reflect.runtime.JavaUniverse.init
  [14] scala.reflect.runtime.JavaUniverse.<init>
  [15] scala.reflect.runtime.package$.universe$lzycompute
  [16] scala.reflect.runtime.package$.universe
  [17] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [21] org.apache.spark.sql.Dataset$.ofRows
  [22] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158161059170 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] scala.reflect.internal.SymbolTable.Apply$lzycompute
  [11] scala.reflect.internal.SymbolTable.Apply
  [12] scala.reflect.runtime.JavaUniverseForce$class.force
  [13] scala.reflect.runtime.JavaUniverse.force
  [14] scala.reflect.runtime.JavaUniverse.init
  [15] scala.reflect.runtime.JavaUniverse.<init>
  [16] scala.reflect.runtime.package$.universe$lzycompute
  [17] scala.reflect.runtime.package$.universe
  [18] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [20] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [24] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] sun.reflect.NativeMethodAccessorImpl.invoke0
  [27] sun.reflect.NativeMethodAccessorImpl.invoke
  [28] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [29] java.lang.reflect.Method.invoke
  [30] py4j.reflection.MethodInvoker.invoke
  [31] py4j.reflection.ReflectionEngine.invoke
  [32] py4j.Gateway.invoke
  [33] py4j.commands.AbstractCommand.invokeMethod
  [34] py4j.commands.CallCommand.execute
  [35] py4j.GatewayConnection.run
  [36] java.lang.Thread.run
  [37] [tid=16146]

--- 1551158161158926 us
  [ 0] ZIP_GetEntry2
  [ 1] Java_java_util_zip_ZipFile_getEntry
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.zip.ZipFile.getEntry
  [ 4] java.util.jar.JarFile.getEntry
  [ 5] java.util.jar.JarFile.getJarEntry
  [ 6] sun.misc.URLClassPath$JarLoader.getResource
  [ 7] sun.misc.URLClassPath.getResource
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.net.URLClassLoader$1.run
  [10] java.security.AccessController.doPrivileged
  [11] java.net.URLClassLoader.findClass
  [12] java.lang.ClassLoader.loadClass
  [13] sun.misc.Launcher$AppClassLoader.loadClass
  [14] java.lang.ClassLoader.loadClass
  [15] scala.reflect.internal.Trees$class.EmptyTreeTypeSubstituter
  [16] scala.reflect.internal.SymbolTable.EmptyTreeTypeSubstituter$lzycompute
  [17] scala.reflect.internal.SymbolTable.EmptyTreeTypeSubstituter
  [18] scala.reflect.runtime.JavaUniverseForce$class.force
  [19] scala.reflect.runtime.JavaUniverse.force
  [20] scala.reflect.runtime.JavaUniverse.init
  [21] scala.reflect.runtime.JavaUniverse.<init>
  [22] scala.reflect.runtime.package$.universe$lzycompute
  [23] scala.reflect.runtime.package$.universe
  [24] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [25] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [26] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [27] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [28] org.apache.spark.sql.Dataset$.ofRows
  [29] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [30] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [31] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158161259414 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] scala.reflect.internal.SymbolTable.unwrapToClass$lzycompute
  [12] scala.reflect.internal.SymbolTable.unwrapToClass
  [13] scala.reflect.runtime.JavaUniverseForce$class.force
  [14] scala.reflect.runtime.JavaUniverse.force
  [15] scala.reflect.runtime.JavaUniverse.init
  [16] scala.reflect.runtime.JavaUniverse.<init>
  [17] scala.reflect.runtime.package$.universe$lzycompute
  [18] scala.reflect.runtime.package$.universe
  [19] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [20] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [21] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [22] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [23] org.apache.spark.sql.Dataset$.ofRows
  [24] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [25] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [26] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [27] sun.reflect.NativeMethodAccessorImpl.invoke0
  [28] sun.reflect.NativeMethodAccessorImpl.invoke
  [29] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [30] java.lang.reflect.Method.invoke
  [31] py4j.reflection.MethodInvoker.invoke
  [32] py4j.reflection.ReflectionEngine.invoke
  [33] py4j.Gateway.invoke
  [34] py4j.commands.AbstractCommand.invokeMethod
  [35] py4j.commands.CallCommand.execute
  [36] py4j.GatewayConnection.run
  [37] java.lang.Thread.run
  [38] [tid=16146]

--- 1551158161359699 us
  [ 0] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 1] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 2] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 3] InstanceKlass::initialize(Thread*)
  [ 4] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 5] JVM_FindClassFromCaller
  [ 6] Java_java_lang_Class_forName0
  [ 7] java.lang.Class.forName0
  [ 8] java.lang.Class.forName
  [ 9] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [10] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [11] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [12] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [13] scala.reflect.runtime.Gil$class.gilSynchronized
  [14] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [17] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [18] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [19] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [20] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [21] scala.reflect.internal.Types$Type.findMember
  [22] scala.reflect.internal.Types$Type.memberBasedOnName
  [23] scala.reflect.internal.Types$Type.member
  [24] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [25] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [26] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [27] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [28] scala.reflect.internal.Mirrors$RootsBase.requiredClass
  [29] scala.reflect.internal.Definitions$DefinitionsClass.UninitializedErrorClass$lzycompute
  [30] scala.reflect.internal.Definitions$DefinitionsClass.UninitializedErrorClass
  [31] scala.reflect.runtime.JavaUniverseForce$class.force
  [32] scala.reflect.runtime.JavaUniverse.force
  [33] scala.reflect.runtime.JavaUniverse.init
  [34] scala.reflect.runtime.JavaUniverse.<init>
  [35] scala.reflect.runtime.package$.universe$lzycompute
  [36] scala.reflect.runtime.package$.universe
  [37] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [38] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [39] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [40] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [41] org.apache.spark.sql.Dataset$.ofRows
  [42] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [43] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [44] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [45] sun.reflect.NativeMethodAccessorImpl.invoke0
  [46] sun.reflect.NativeMethodAccessorImpl.invoke
  [47] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [48] java.lang.reflect.Method.invoke
  [49] py4j.reflection.MethodInvoker.invoke
  [50] py4j.reflection.ReflectionEngine.invoke
  [51] py4j.Gateway.invoke
  [52] py4j.commands.AbstractCommand.invokeMethod
  [53] py4j.commands.CallCommand.execute
  [54] py4j.GatewayConnection.run
  [55] java.lang.Thread.run
  [56] [tid=16146]

--- 1551158161461232 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] scala.reflect.internal.pickling.UnPickler$Scan.readSymbolAnnotation
  [10] scala.reflect.internal.pickling.UnPickler$Scan.run
  [11] scala.reflect.internal.pickling.UnPickler.unpickle
  [12] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [13] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [14] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [15] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [16] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [17] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [18] scala.reflect.internal.Symbols$Symbol.info
  [19] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [22] scala.reflect.runtime.Gil$class.gilSynchronized
  [23] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [24] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [25] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [26] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [27] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [28] scala.reflect.internal.SymbolTable.openPackageModule
  [29] scala.reflect.internal.SymbolTable.openPackageModule
  [30] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp
  [31] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [32] scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply
  [33] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [34] scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete
  [35] scala.reflect.internal.Symbols$Symbol.info
  [36] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [37] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [38] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [39] scala.reflect.runtime.Gil$class.gilSynchronized
  [40] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [41] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [42] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe
  [43] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [44] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info
  [45] scala.reflect.internal.Types$TypeRef.thisInfo
  [46] scala.reflect.internal.Types$TypeRef.baseClasses
  [47] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [48] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [49] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [50] scala.reflect.internal.Types$Type.findMember
  [51] scala.reflect.internal.Types$Type.memberBasedOnName
  [52] scala.reflect.internal.Types$Type.member
  [53] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [54] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [55] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [56] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [57] scala.reflect.internal.Mirrors$RootsBase.requiredClass
  [58] scala.reflect.internal.Definitions$DefinitionsClass.ScalaNumberClass$lzycompute
  [59] scala.reflect.internal.Definitions$DefinitionsClass.ScalaNumberClass
  [60] scala.reflect.runtime.JavaUniverseForce$class.force
  [61] scala.reflect.runtime.JavaUniverse.force
  [62] scala.reflect.runtime.JavaUniverse.init
  [63] scala.reflect.runtime.JavaUniverse.<init>
  [64] scala.reflect.runtime.package$.universe$lzycompute
  [65] scala.reflect.runtime.package$.universe
  [66] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [67] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [68] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [69] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [70] org.apache.spark.sql.Dataset$.ofRows
  [71] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [72] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [73] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [74] sun.reflect.NativeMethodAccessorImpl.invoke0
  [75] sun.reflect.NativeMethodAccessorImpl.invoke
  [76] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [77] java.lang.reflect.Method.invoke
  [78] py4j.reflection.MethodInvoker.invoke
  [79] py4j.reflection.ReflectionEngine.invoke
  [80] py4j.Gateway.invoke
  [81] py4j.commands.AbstractCommand.invokeMethod
  [82] py4j.commands.CallCommand.execute
  [83] py4j.GatewayConnection.run
  [84] java.lang.Thread.run
  [85] [tid=16146]

--- 1551158161561606 us
  [ 0] scala.collection.AbstractIterable.<init>
  [ 1] scala.collection.AbstractSeq.<init>
  [ 2] scala.collection.mutable.AbstractSeq.<init>
  [ 3] scala.collection.mutable.StringBuilder.<init>
  [ 4] scala.collection.mutable.StringBuilder.<init>
  [ 5] scala.collection.mutable.StringBuilder.<init>
  [ 6] scala.Predef$any2stringadd$.$plus$extension
  [ 7] scala.reflect.runtime.ReflectionUtils$.scalacShouldntLoadClass
  [ 8] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [ 9] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [10] scala.reflect.runtime.Gil$class.gilSynchronized
  [11] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [12] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [13] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [14] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [15] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [16] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [17] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [18] scala.reflect.internal.Types$Type.findMember
  [19] scala.reflect.internal.Types$Type.memberBasedOnName
  [20] scala.reflect.internal.Types$Type.member
  [21] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [22] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [23] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [24] scala.reflect.internal.Mirrors$RootsBase.getModuleByName
  [25] scala.reflect.internal.Mirrors$RootsBase.getRequiredModule
  [26] scala.reflect.internal.Mirrors$RootsBase.requiredModule
  [27] scala.reflect.internal.Definitions$DefinitionsClass.ReflectPackage$lzycompute
  [28] scala.reflect.internal.Definitions$DefinitionsClass.ReflectPackage
  [29] scala.reflect.runtime.JavaUniverseForce$class.force
  [30] scala.reflect.runtime.JavaUniverse.force
  [31] scala.reflect.runtime.JavaUniverse.init
  [32] scala.reflect.runtime.JavaUniverse.<init>
  [33] scala.reflect.runtime.package$.universe$lzycompute
  [34] scala.reflect.runtime.package$.universe
  [35] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [36] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [37] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [38] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [39] org.apache.spark.sql.Dataset$.ofRows
  [40] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [41] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [42] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [43] sun.reflect.NativeMethodAccessorImpl.invoke0
  [44] sun.reflect.NativeMethodAccessorImpl.invoke
  [45] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [46] java.lang.reflect.Method.invoke
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.AbstractCommand.invokeMethod
  [51] py4j.commands.CallCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16146]

--- 1551158161661698 us
  [ 0] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$15.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$_initialized_$eq
  [ 1] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.$init$
  [ 2] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$15.<init>
  [ 3] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.createValueMemberSymbol
  [ 4] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$2.createValueMemberSymbol
  [ 5] scala.reflect.internal.Symbols$Symbol.newTermSymbol
  [ 6] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [ 7] scala.reflect.internal.pickling.UnPickler$Scan.run
  [ 8] scala.reflect.internal.pickling.UnPickler.unpickle
  [ 9] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [10] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [11] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [12] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [13] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [14] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [15] scala.reflect.internal.Symbols$Symbol.info
  [16] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [17] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [19] scala.reflect.runtime.Gil$class.gilSynchronized
  [20] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [22] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe
  [23] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [24] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info
  [25] scala.reflect.internal.Symbols$Symbol.initialize
  [26] scala.reflect.internal.Symbols$Symbol.hasFlag
  [27] scala.reflect.internal.HasFlags$class.isMutable
  [28] scala.reflect.internal.Symbols$Symbol.isMutable
  [29] scala.reflect.internal.Symbols$Symbol.isStable
  [30] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readType$1.apply
  [31] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readType$1.apply
  [32] scala.reflect.internal.Symbols$Symbol.filter
  [33] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [34] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [35] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [36] scala.reflect.internal.pickling.UnPickler$Scan.at
  [37] scala.reflect.internal.pickling.UnPickler$Scan.readTypeRef
  [38] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [39] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [40] scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply
  [41] scala.reflect.internal.pickling.UnPickler$Scan.at
  [42] scala.reflect.internal.pickling.UnPickler$Scan.readTypeRef
  [43] scala.reflect.internal.pickling.UnPickler$Scan.readType
  [44] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [45] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply
  [46] scala.reflect.internal.pickling.UnPickler$Scan.at
  [47] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.completeInternal
  [48] scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete
  [49] scala.reflect.internal.Symbols$Symbol.info
  [50] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [51] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [52] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [53] scala.reflect.runtime.Gil$class.gilSynchronized
  [54] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [55] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [56] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.gilSynchronizedIfNotThreadsafe
  [57] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [58] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info
  [59] scala.reflect.internal.Symbols$Symbol.initialize
  [60] scala.reflect.internal.Symbols$Symbol.annotations
  [61] scala.reflect.internal.Symbols$Symbol.addAnnotation
  [62] scala.reflect.internal.pickling.UnPickler$Scan.readSymbolAnnotation
  [63] scala.reflect.internal.pickling.UnPickler$Scan.run
  [64] scala.reflect.internal.pickling.UnPickler.unpickle
  [65] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [66] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [67] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [68] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [69] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [70] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [71] scala.reflect.internal.Symbols$Symbol.info
  [72] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [73] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [74] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [75] scala.reflect.runtime.Gil$class.gilSynchronized
  [76] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [77] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [78] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [79] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [80] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [81] scala.reflect.internal.Types$TypeRef.thisInfo
  [82] scala.reflect.internal.Types$TypeRef.baseClasses
  [83] scala.reflect.internal.Types$class.computeBaseClasses
  [84] scala.reflect.internal.SymbolTable.computeBaseClasses
  [85] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [86] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [87] scala.reflect.internal.Types$CompoundType.updateCache$1
  [88] scala.reflect.internal.Types$CompoundType.memo
  [89] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [90] scala.reflect.internal.Types$class.define$1
  [91] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [92] scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$defineBaseClassesOfCompoundType
  [93] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [94] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [95] scala.reflect.runtime.Gil$class.gilSynchronized
  [96] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [97] scala.reflect.runtime.SynchronizedTypes$class.defineBaseClassesOfCompoundType
  [98] scala.reflect.runtime.JavaUniverse.defineBaseClassesOfCompoundType
  [99] scala.reflect.internal.Types$CompoundType.baseClasses
  [100] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [101] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [102] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [103] scala.reflect.internal.Types$Type.findMember
  [104] scala.reflect.internal.Types$Type.memberBasedOnName
  [105] scala.reflect.internal.Types$Type.nonPrivateMember
  [106] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [107] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass$lzycompute
  [108] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass
  [109] scala.reflect.runtime.JavaUniverseForce$class.force
  [110] scala.reflect.runtime.JavaUniverse.force
  [111] scala.reflect.runtime.JavaUniverse.init
  [112] scala.reflect.runtime.JavaUniverse.<init>
  [113] scala.reflect.runtime.package$.universe$lzycompute
  [114] scala.reflect.runtime.package$.universe
  [115] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [116] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [117] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [118] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [119] org.apache.spark.sql.Dataset$.ofRows
  [120] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [121] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [122] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [123] sun.reflect.NativeMethodAccessorImpl.invoke0
  [124] sun.reflect.NativeMethodAccessorImpl.invoke
  [125] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [126] java.lang.reflect.Method.invoke
  [127] py4j.reflection.MethodInvoker.invoke
  [128] py4j.reflection.ReflectionEngine.invoke
  [129] py4j.Gateway.invoke
  [130] py4j.commands.AbstractCommand.invokeMethod
  [131] py4j.commands.CallCommand.execute
  [132] py4j.GatewayConnection.run
  [133] java.lang.Thread.run
  [134] [tid=16146]

--- 1551158161762176 us
  [ 0] vtable stub
  [ 1] scala.reflect.internal.pickling.UnPickler$Scan.shouldEnterInOwnerScope$1
  [ 2] scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1
  [ 3] scala.reflect.internal.pickling.UnPickler$Scan.readSymbol
  [ 4] scala.reflect.internal.pickling.UnPickler$Scan.run
  [ 5] scala.reflect.internal.pickling.UnPickler.unpickle
  [ 6] scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass
  [ 7] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp
  [ 8] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [ 9] scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply
  [10] scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan
  [11] scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete
  [12] scala.reflect.internal.Symbols$Symbol.info
  [13] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info
  [14] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [15] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply
  [16] scala.reflect.runtime.Gil$class.gilSynchronized
  [17] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [18] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe
  [19] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.gilSynchronizedIfNotThreadsafe
  [20] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info
  [21] scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$8.info
  [22] scala.reflect.internal.Types$TypeRef.thisInfo
  [23] scala.reflect.internal.Types$TypeRef.baseClasses
  [24] scala.reflect.internal.Types$class.computeBaseClasses
  [25] scala.reflect.internal.SymbolTable.computeBaseClasses
  [26] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [27] scala.reflect.internal.Types$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [28] scala.reflect.internal.Types$CompoundType.updateCache$1
  [29] scala.reflect.internal.Types$CompoundType.memo
  [30] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [31] scala.reflect.internal.Types$class.define$1
  [32] scala.reflect.internal.Types$class.defineBaseClassesOfCompoundType
  [33] scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$defineBaseClassesOfCompoundType
  [34] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [35] scala.reflect.runtime.SynchronizedTypes$$anonfun$defineBaseClassesOfCompoundType$1.apply
  [36] scala.reflect.runtime.Gil$class.gilSynchronized
  [37] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [38] scala.reflect.runtime.SynchronizedTypes$class.defineBaseClassesOfCompoundType
  [39] scala.reflect.runtime.JavaUniverse.defineBaseClassesOfCompoundType
  [40] scala.reflect.internal.Types$CompoundType.baseClasses
  [41] scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>
  [42] scala.reflect.internal.tpe.FindMembers$FindMember.<init>
  [43] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [44] scala.reflect.internal.Types$Type.findMember
  [45] scala.reflect.internal.Types$Type.memberBasedOnName
  [46] scala.reflect.internal.Types$Type.nonPrivateMember
  [47] scala.reflect.internal.Definitions$DefinitionsClass.getMemberIfDefined
  [48] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass$lzycompute
  [49] scala.reflect.internal.Definitions$DefinitionsClass.QuasiquoteClass
  [50] scala.reflect.runtime.JavaUniverseForce$class.force
  [51] scala.reflect.runtime.JavaUniverse.force
  [52] scala.reflect.runtime.JavaUniverse.init
  [53] scala.reflect.runtime.JavaUniverse.<init>
  [54] scala.reflect.runtime.package$.universe$lzycompute
  [55] scala.reflect.runtime.package$.universe
  [56] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [57] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [58] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [59] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [60] org.apache.spark.sql.Dataset$.ofRows
  [61] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [62] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [63] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [64] sun.reflect.NativeMethodAccessorImpl.invoke0
  [65] sun.reflect.NativeMethodAccessorImpl.invoke
  [66] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [67] java.lang.reflect.Method.invoke
  [68] py4j.reflection.MethodInvoker.invoke
  [69] py4j.reflection.ReflectionEngine.invoke
  [70] py4j.Gateway.invoke
  [71] py4j.commands.AbstractCommand.invokeMethod
  [72] py4j.commands.CallCommand.execute
  [73] py4j.GatewayConnection.run
  [74] java.lang.Thread.run
  [75] [tid=16146]

--- 1551158161863232 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.member
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [33] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [34] scala.collection.TraversableLike$$anonfun$map$1.apply
  [35] scala.collection.TraversableLike$$anonfun$map$1.apply
  [36] scala.collection.immutable.Range.foreach
  [37] scala.collection.TraversableLike$class.map
  [38] scala.collection.AbstractTraversable.map
  [39] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [40] scala.reflect.internal.Definitions$DefinitionsClass.ProductClass$lzycompute
  [41] scala.reflect.internal.Definitions$DefinitionsClass.ProductClass
  [42] scala.reflect.runtime.JavaUniverseForce$class.force
  [43] scala.reflect.runtime.JavaUniverse.force
  [44] scala.reflect.runtime.JavaUniverse.init
  [45] scala.reflect.runtime.JavaUniverse.<init>
  [46] scala.reflect.runtime.package$.universe$lzycompute
  [47] scala.reflect.runtime.package$.universe
  [48] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [49] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [50] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [51] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [52] org.apache.spark.sql.Dataset$.ofRows
  [53] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [54] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [55] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16146]

--- 1551158161962994 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.member
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [33] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [34] scala.collection.TraversableLike$$anonfun$map$1.apply
  [35] scala.collection.TraversableLike$$anonfun$map$1.apply
  [36] scala.collection.immutable.Range.foreach
  [37] scala.collection.TraversableLike$class.map
  [38] scala.collection.AbstractTraversable.map
  [39] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [40] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass$lzycompute
  [41] scala.reflect.internal.Definitions$DefinitionsClass.TupleClass
  [42] scala.reflect.runtime.JavaUniverseForce$class.force
  [43] scala.reflect.runtime.JavaUniverse.force
  [44] scala.reflect.runtime.JavaUniverse.init
  [45] scala.reflect.runtime.JavaUniverse.<init>
  [46] scala.reflect.runtime.package$.universe$lzycompute
  [47] scala.reflect.runtime.package$.universe
  [48] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [49] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [50] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [51] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [52] org.apache.spark.sql.Dataset$.ofRows
  [53] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [54] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [55] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16146]

--- 1551158162063127 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] find_class_from_class_loader(JNIEnv_*, Symbol*, unsigned char, Handle, Handle, unsigned char, Thread*)
  [ 9] JVM_FindClassFromCaller
  [10] Java_java_lang_Class_forName0
  [11] java.lang.Class.forName0
  [12] java.lang.Class.forName
  [13] scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass
  [14] scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass
  [15] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [16] scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply
  [17] scala.reflect.runtime.Gil$class.gilSynchronized
  [18] scala.reflect.runtime.JavaUniverse.gilSynchronized
  [19] scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized
  [20] scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry
  [21] scala.reflect.internal.tpe.FindMembers$FindMemberBase.walkBaseClasses
  [22] scala.reflect.internal.tpe.FindMembers$FindMemberBase.searchConcreteThenDeferred
  [23] scala.reflect.internal.tpe.FindMembers$FindMemberBase.apply
  [24] scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1
  [25] scala.reflect.internal.Types$Type.findMember
  [26] scala.reflect.internal.Types$Type.memberBasedOnName
  [27] scala.reflect.internal.Types$Type.member
  [28] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [29] scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass
  [30] scala.reflect.internal.Mirrors$RootsBase.getClassByName
  [31] scala.reflect.internal.Mirrors$RootsBase.getRequiredClass
  [32] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [33] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass$$anonfun$1.apply
  [34] scala.collection.TraversableLike$$anonfun$map$1.apply
  [35] scala.collection.TraversableLike$$anonfun$map$1.apply
  [36] scala.collection.immutable.Range.foreach
  [37] scala.collection.TraversableLike$class.map
  [38] scala.collection.AbstractTraversable.map
  [39] scala.reflect.internal.Definitions$DefinitionsClass$VarArityClass.<init>
  [40] scala.reflect.internal.Definitions$DefinitionsClass.FunctionClass$lzycompute
  [41] scala.reflect.internal.Definitions$DefinitionsClass.FunctionClass
  [42] scala.reflect.runtime.JavaUniverseForce$class.force
  [43] scala.reflect.runtime.JavaUniverse.force
  [44] scala.reflect.runtime.JavaUniverse.init
  [45] scala.reflect.runtime.JavaUniverse.<init>
  [46] scala.reflect.runtime.package$.universe$lzycompute
  [47] scala.reflect.runtime.package$.universe
  [48] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [49] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [50] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [51] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [52] org.apache.spark.sql.Dataset$.ofRows
  [53] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [54] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [55] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16146]

--- 1551158162163986 us
  [ 0] CodeCache::find_blob(void*)
  [ 1] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [ 2] scala.reflect.internal.Scopes$class.newScopeWith
  [ 3] scala.reflect.internal.SymbolTable.newScopeWith
  [ 4] scala.reflect.internal.Scopes$Scope.filter
  [ 5] scala.reflect.internal.Definitions$DefinitionsClass.metaAnnotations$lzycompute
  [ 6] scala.reflect.internal.Definitions$DefinitionsClass.metaAnnotations
  [ 7] scala.reflect.runtime.JavaUniverseForce$class.force
  [ 8] scala.reflect.runtime.JavaUniverse.force
  [ 9] scala.reflect.runtime.JavaUniverse.init
  [10] scala.reflect.runtime.JavaUniverse.<init>
  [11] scala.reflect.runtime.package$.universe$lzycompute
  [12] scala.reflect.runtime.package$.universe
  [13] org.apache.spark.sql.catalyst.ScalaReflection$.<init>
  [14] org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [16] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [17] org.apache.spark.sql.Dataset$.ofRows
  [18] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] py4j.reflection.MethodInvoker.invoke
  [26] py4j.reflection.ReflectionEngine.invoke
  [27] py4j.Gateway.invoke
  [28] py4j.commands.AbstractCommand.invokeMethod
  [29] py4j.commands.CallCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158162263733 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [10] org.apache.spark.sql.catalyst.encoders.RowEncoder$$anonfun$2.apply
  [11] org.apache.spark.sql.catalyst.encoders.RowEncoder$$anonfun$2.apply
  [12] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [13] scala.collection.TraversableLike$$anonfun$flatMap$1.apply
  [14] scala.collection.IndexedSeqOptimized$class.foreach
  [15] scala.collection.mutable.ArrayOps$ofRef.foreach
  [16] scala.collection.TraversableLike$class.flatMap
  [17] scala.collection.mutable.ArrayOps$ofRef.flatMap
  [18] org.apache.spark.sql.catalyst.encoders.RowEncoder$.org$apache$spark$sql$catalyst$encoders$RowEncoder$$serializerFor
  [19] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [20] org.apache.spark.sql.Dataset$.ofRows
  [21] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [22] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [23] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158162363580 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.Expression.references
  [10] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$$anonfun$9.apply
  [11] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$$anonfun$9.apply
  [12] scala.collection.LinearSeqOptimized$class.forall
  [13] scala.collection.immutable.List.forall
  [14] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.<init>
  [15] org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply
  [16] org.apache.spark.sql.Dataset$.ofRows
  [17] org.apache.spark.sql.SparkSession.internalCreateDataFrame
  [18] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [19] org.apache.spark.sql.SparkSession.applySchemaToPythonRDD
  [20] sun.reflect.NativeMethodAccessorImpl.invoke0
  [21] sun.reflect.NativeMethodAccessorImpl.invoke
  [22] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [23] java.lang.reflect.Method.invoke
  [24] py4j.reflection.MethodInvoker.invoke
  [25] py4j.reflection.ReflectionEngine.invoke
  [26] py4j.Gateway.invoke
  [27] py4j.commands.AbstractCommand.invokeMethod
  [28] py4j.commands.CallCommand.execute
  [29] py4j.GatewayConnection.run
  [30] java.lang.Thread.run
  [31] [tid=16146]

--- 1551158162629745 us
  [ 0] SymbolTable::lookup_only(char const*, int, unsigned int&)
  [ 1] ClassFileParser::parse_constant_pool_entries(int, Thread*)
  [ 2] ClassFileParser::parse_constant_pool(Thread*)
  [ 3] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 4] SystemDictionary::resolve_from_stream(Symbol*, Handle, Handle, ClassFileStream*, bool, Thread*)
  [ 5] jvm_define_class_common(JNIEnv_*, char const*, _jobject*, signed char const*, int, _jobject*, char const*, unsigned char, Thread*)
  [ 6] JVM_DefineClassWithSource
  [ 7] Java_java_lang_ClassLoader_defineClass1
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] java.lang.Class.getDeclaredMethods0
  [21] java.lang.Class.privateGetDeclaredMethods
  [22] java.lang.Class.privateGetPublicMethods
  [23] java.lang.Class.getMethods
  [24] py4j.reflection.ReflectionEngine.getMethodsByNameAndLength
  [25] py4j.reflection.ReflectionEngine.getMethod
  [26] py4j.reflection.ReflectionEngine.getMethod
  [27] py4j.Gateway.invoke
  [28] py4j.commands.AbstractCommand.invokeMethod
  [29] py4j.commands.CallCommand.execute
  [30] py4j.GatewayConnection.run
  [31] java.lang.Thread.run
  [32] [tid=16146]

--- 1551158162729517 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] Runtime1::new_instance(JavaThread*, Klass*)
  [ 9] org.antlr.v4.runtime.atn.ATNDeserializer.edgeFactory
  [10] org.antlr.v4.runtime.atn.ATNDeserializer.deserialize
  [11] org.apache.spark.sql.catalyst.parser.SqlBaseLexer.<clinit>
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [13] org.apache.spark.sql.execution.SparkSqlParser.parse
  [14] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [15] org.apache.spark.sql.Dataset.createTempViewCommand
  [16] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16146]

--- 1551158162830053 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [11] org.apache.spark.sql.execution.SparkSqlParser.parse
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [13] org.apache.spark.sql.Dataset.createTempViewCommand
  [14] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [15] sun.reflect.NativeMethodAccessorImpl.invoke0
  [16] sun.reflect.NativeMethodAccessorImpl.invoke
  [17] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [18] java.lang.reflect.Method.invoke
  [19] py4j.reflection.MethodInvoker.invoke
  [20] py4j.reflection.ReflectionEngine.invoke
  [21] py4j.Gateway.invoke
  [22] py4j.commands.AbstractCommand.invokeMethod
  [23] py4j.commands.CallCommand.execute
  [24] py4j.GatewayConnection.run
  [25] java.lang.Thread.run
  [26] [tid=16146]

--- 1551158162929866 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.catalyst.parser.SqlBaseParser.<clinit>
  [13] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [14] org.apache.spark.sql.execution.SparkSqlParser.parse
  [15] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [16] org.apache.spark.sql.Dataset.createTempViewCommand
  [17] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] py4j.reflection.MethodInvoker.invoke
  [23] py4j.reflection.ReflectionEngine.invoke
  [24] py4j.Gateway.invoke
  [25] py4j.commands.AbstractCommand.invokeMethod
  [26] py4j.commands.CallCommand.execute
  [27] py4j.GatewayConnection.run
  [28] java.lang.Thread.run
  [29] [tid=16146]

--- 1551158163029601 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [11] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [12] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [13] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [14] org.apache.spark.sql.execution.SparkSqlParser.parse
  [15] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [16] org.apache.spark.sql.Dataset.createTempViewCommand
  [17] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] py4j.reflection.MethodInvoker.invoke
  [23] py4j.reflection.ReflectionEngine.invoke
  [24] py4j.Gateway.invoke
  [25] py4j.commands.AbstractCommand.invokeMethod
  [26] py4j.commands.CallCommand.execute
  [27] py4j.GatewayConnection.run
  [28] java.lang.Thread.run
  [29] [tid=16146]

--- 1551158163129331 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [13] org.apache.spark.sql.execution.SparkSqlParser.parse
  [14] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [15] org.apache.spark.sql.Dataset.createTempViewCommand
  [16] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [17] sun.reflect.NativeMethodAccessorImpl.invoke0
  [18] sun.reflect.NativeMethodAccessorImpl.invoke
  [19] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [20] java.lang.reflect.Method.invoke
  [21] py4j.reflection.MethodInvoker.invoke
  [22] py4j.reflection.ReflectionEngine.invoke
  [23] py4j.Gateway.invoke
  [24] py4j.commands.AbstractCommand.invokeMethod
  [25] py4j.commands.CallCommand.execute
  [26] py4j.GatewayConnection.run
  [27] java.lang.Thread.run
  [28] [tid=16146]

--- 1551158163229980 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.SqlBaseParser.identifier
  [10] org.apache.spark.sql.catalyst.parser.SqlBaseParser.tableIdentifier
  [11] org.apache.spark.sql.catalyst.parser.SqlBaseParser.singleTableIdentifier
  [12] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseTableIdentifier$1.apply
  [13] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseTableIdentifier$1.apply
  [14] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [15] org.apache.spark.sql.execution.SparkSqlParser.parse
  [16] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableIdentifier
  [17] org.apache.spark.sql.Dataset.createTempViewCommand
  [18] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.AbstractCommand.invokeMethod
  [27] py4j.commands.CallCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158163330005 us
  [ 0] java.security.ProtectionDomain.getCodeSource
  [ 1] java.lang.ClassLoader.postDefineClass
  [ 2] java.lang.ClassLoader.defineClass
  [ 3] java.security.SecureClassLoader.defineClass
  [ 4] java.net.URLClassLoader.defineClass
  [ 5] java.net.URLClassLoader.access$100
  [ 6] java.net.URLClassLoader$1.run
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.security.AccessController.doPrivileged
  [ 9] java.net.URLClassLoader.findClass
  [10] java.lang.ClassLoader.loadClass
  [11] sun.misc.Launcher$AppClassLoader.loadClass
  [12] java.lang.ClassLoader.loadClass
  [13] org.apache.spark.sql.catalyst.optimizer.Optimizer.defaultBatches
  [14] org.apache.spark.sql.execution.SparkOptimizer.defaultBatches
  [15] org.apache.spark.sql.catalyst.optimizer.Optimizer.batches
  [16] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [17] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [18] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [19] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [20] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [21] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [22] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [23] org.apache.spark.sql.Dataset.withAction
  [24] org.apache.spark.sql.Dataset.<init>
  [25] org.apache.spark.sql.Dataset$.ofRows
  [26] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [27] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [28] sun.reflect.NativeMethodAccessorImpl.invoke0
  [29] sun.reflect.NativeMethodAccessorImpl.invoke
  [30] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [31] java.lang.reflect.Method.invoke
  [32] py4j.reflection.MethodInvoker.invoke
  [33] py4j.reflection.ReflectionEngine.invoke
  [34] py4j.Gateway.invoke
  [35] py4j.commands.AbstractCommand.invokeMethod
  [36] py4j.commands.CallCommand.execute
  [37] py4j.GatewayConnection.run
  [38] java.lang.Thread.run
  [39] [tid=16146]

--- 1551158163429758 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.execution.SparkOptimizer.defaultBatches
  [12] org.apache.spark.sql.catalyst.optimizer.Optimizer.batches
  [13] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [14] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [15] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [16] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [17] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [18] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [19] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [20] org.apache.spark.sql.Dataset.withAction
  [21] org.apache.spark.sql.Dataset.<init>
  [22] org.apache.spark.sql.Dataset$.ofRows
  [23] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [24] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [25] sun.reflect.NativeMethodAccessorImpl.invoke0
  [26] sun.reflect.NativeMethodAccessorImpl.invoke
  [27] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [28] java.lang.reflect.Method.invoke
  [29] py4j.reflection.MethodInvoker.invoke
  [30] py4j.reflection.ReflectionEngine.invoke
  [31] py4j.Gateway.invoke
  [32] py4j.commands.AbstractCommand.invokeMethod
  [33] py4j.commands.CallCommand.execute
  [34] py4j.GatewayConnection.run
  [35] java.lang.Thread.run
  [36] [tid=16146]

--- 1551158163531893 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.expressions.ExpressionSet$.apply
  [10] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.org$apache$spark$sql$catalyst$optimizer$ReorderAssociativeOperator$$collectGroupingExpressions
  [11] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$$anonfun$apply$3.applyOrElse
  [12] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$$anonfun$apply$3.applyOrElse
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [14] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [15] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [16] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [17] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [20] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [21] org.apache.spark.sql.catalyst.trees.TreeNode.transform
  [22] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.apply
  [23] org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator$.apply
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [25] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [26] scala.collection.LinearSeqOptimized$class.foldLeft
  [27] scala.collection.immutable.List.foldLeft
  [28] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [29] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [30] scala.collection.immutable.List.foreach
  [31] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [32] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [33] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [34] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [35] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [36] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [37] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [38] org.apache.spark.sql.Dataset.withAction
  [39] org.apache.spark.sql.Dataset.<init>
  [40] org.apache.spark.sql.Dataset$.ofRows
  [41] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [42] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [43] sun.reflect.NativeMethodAccessorImpl.invoke0
  [44] sun.reflect.NativeMethodAccessorImpl.invoke
  [45] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [46] java.lang.reflect.Method.invoke
  [47] py4j.reflection.MethodInvoker.invoke
  [48] py4j.reflection.ReflectionEngine.invoke
  [49] py4j.Gateway.invoke
  [50] py4j.commands.AbstractCommand.invokeMethod
  [51] py4j.commands.CallCommand.execute
  [52] py4j.GatewayConnection.run
  [53] java.lang.Thread.run
  [54] [tid=16146]

--- 1551158163631923 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.python.ExtractPythonUDFFromAggregate$.apply
  [10] org.apache.spark.sql.execution.python.ExtractPythonUDFFromAggregate$.apply
  [11] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [12] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [13] scala.collection.LinearSeqOptimized$class.foldLeft
  [14] scala.collection.immutable.List.foldLeft
  [15] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [16] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [17] scala.collection.immutable.List.foreach
  [18] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [19] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [20] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [21] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [22] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [23] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [24] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [25] org.apache.spark.sql.Dataset.withAction
  [26] org.apache.spark.sql.Dataset.<init>
  [27] org.apache.spark.sql.Dataset$.ofRows
  [28] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [29] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [30] sun.reflect.NativeMethodAccessorImpl.invoke0
  [31] sun.reflect.NativeMethodAccessorImpl.invoke
  [32] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [33] java.lang.reflect.Method.invoke
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.AbstractCommand.invokeMethod
  [38] py4j.commands.CallCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16146]

--- 1551158163773283 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.QueryExecution.preparations
  [10] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [11] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [12] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [13] org.apache.spark.sql.Dataset.withAction
  [14] org.apache.spark.sql.Dataset.<init>
  [15] org.apache.spark.sql.Dataset$.ofRows
  [16] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [17] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] py4j.reflection.MethodInvoker.invoke
  [23] py4j.reflection.ReflectionEngine.invoke
  [24] py4j.Gateway.invoke
  [25] py4j.commands.AbstractCommand.invokeMethod
  [26] py4j.commands.CallCommand.execute
  [27] py4j.GatewayConnection.run
  [28] java.lang.Thread.run
  [29] [tid=16146]

--- 1551158163904493 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.ui.SparkPlanGraph.<init>
  [10] org.apache.spark.sql.execution.ui.SparkPlanGraph$.apply
  [11] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onExecutionStart
  [12] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onOtherEvent
  [13] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [14] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [15] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [16] org.apache.spark.util.ListenerBus$class.postToAll
  [17] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [18] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [19] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [20] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [21] scala.util.DynamicVariable.withValue
  [22] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [23] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [24] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [25] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [26] [tid=16260]

--- 1551158163905837 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [10] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [11] org.apache.spark.sql.Dataset.withAction
  [12] org.apache.spark.sql.Dataset.<init>
  [13] org.apache.spark.sql.Dataset$.ofRows
  [14] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [15] org.apache.spark.sql.Dataset.createOrReplaceTempView
  [16] sun.reflect.NativeMethodAccessorImpl.invoke0
  [17] sun.reflect.NativeMethodAccessorImpl.invoke
  [18] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [19] java.lang.reflect.Method.invoke
  [20] py4j.reflection.MethodInvoker.invoke
  [21] py4j.reflection.ReflectionEngine.invoke
  [22] py4j.Gateway.invoke
  [23] py4j.commands.AbstractCommand.invokeMethod
  [24] py4j.commands.CallCommand.execute
  [25] py4j.GatewayConnection.run
  [26] java.lang.Thread.run
  [27] [tid=16146]

--- 1551158163908535 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredConstructors
  [ 9] java.lang.Class.getDeclaredConstructors0
  [10] java.lang.Class.privateGetDeclaredConstructors
  [11] java.lang.Class.getConstructor0
  [12] java.lang.Class.getConstructor
  [13] java.lang.reflect.Proxy.newProxyInstance
  [14] sun.reflect.annotation.AnnotationParser$1.run
  [15] sun.reflect.annotation.AnnotationParser$1.run
  [16] java.security.AccessController.doPrivileged
  [17] sun.reflect.annotation.AnnotationParser.annotationForMap
  [18] sun.reflect.annotation.AnnotationParser.parseAnnotation2
  [19] sun.reflect.annotation.AnnotationParser.parseAnnotations2
  [20] sun.reflect.annotation.AnnotationParser.parseAnnotations
  [21] java.lang.Class.createAnnotationData
  [22] java.lang.Class.annotationData
  [23] java.lang.Class.getDeclaredAnnotations
  [24] com.fasterxml.jackson.databind.introspect.AnnotatedClass.resolveClassAnnotations
  [25] com.fasterxml.jackson.databind.introspect.AnnotatedClass.getAnnotation
  [26] com.fasterxml.jackson.databind.AnnotationIntrospector._findAnnotation
  [27] com.fasterxml.jackson.databind.introspect.JacksonAnnotationIntrospector.findAutoDetectVisibility
  [28] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.findAutoDetectVisibility
  [29] com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.findAutoDetectVisibility
  [30] com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.<init>
  [31] com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.constructPropertyCollector
  [32] com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.collectProperties
  [33] com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.forSerialization
  [34] com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.forSerialization
  [35] com.fasterxml.jackson.databind.SerializationConfig.introspect
  [36] com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer
  [37] com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer
  [38] com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer
  [39] com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer
  [40] com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer
  [41] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [42] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [43] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [44] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [45] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [46] org.apache.spark.scheduler.EventLoggingListener.onOtherEvent
  [47] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [48] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [49] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [50] org.apache.spark.util.ListenerBus$class.postToAll
  [51] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [52] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [53] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [54] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [55] scala.util.DynamicVariable.withValue
  [56] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [57] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [58] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [59] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [60] [tid=16262]

--- 1551158164055052 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] com.fasterxml.jackson.module.scala.ser.IterableSerializer$class.withResolved
  [10] com.fasterxml.jackson.module.scala.ser.UnresolvedIterableSerializer.withResolved
  [11] com.fasterxml.jackson.module.scala.ser.UnresolvedIterableSerializer.withResolved
  [12] com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.createContextual
  [13] com.fasterxml.jackson.databind.SerializerProvider.handlePrimaryContextualization
  [14] com.fasterxml.jackson.databind.SerializerProvider.findPrimaryPropertySerializer
  [15] com.fasterxml.jackson.databind.ser.impl.PropertySerializerMap.findAndAddPrimarySerializer
  [16] com.fasterxml.jackson.databind.ser.BeanPropertyWriter._findAndAddDynamic
  [17] com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField
  [18] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields
  [19] com.fasterxml.jackson.databind.ser.BeanSerializer.serialize
  [20] com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField
  [21] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields
  [22] com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType
  [23] com.fasterxml.jackson.databind.ser.impl.TypeWrappedSerializer.serialize
  [24] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [25] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [26] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [27] org.apache.spark.util.JsonProtocol$.sparkEventToJson
  [28] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [29] org.apache.spark.scheduler.EventLoggingListener.onOtherEvent
  [30] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [31] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [32] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [33] org.apache.spark.util.ListenerBus$class.postToAll
  [34] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [36] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [37] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [38] scala.util.DynamicVariable.withValue
  [39] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [40] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [41] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [42] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [43] [tid=16262]

--- 1551158164073018 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.SqlBaseParser.relation
  [10] org.apache.spark.sql.catalyst.parser.SqlBaseParser.fromClause
  [11] org.apache.spark.sql.catalyst.parser.SqlBaseParser.querySpecification
  [12] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryPrimary
  [13] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryTerm
  [14] org.apache.spark.sql.catalyst.parser.SqlBaseParser.queryNoWith
  [15] org.apache.spark.sql.catalyst.parser.SqlBaseParser.query
  [16] org.apache.spark.sql.catalyst.parser.SqlBaseParser.statement
  [17] org.apache.spark.sql.catalyst.parser.SqlBaseParser.singleStatement
  [18] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [19] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [20] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [21] org.apache.spark.sql.execution.SparkSqlParser.parse
  [22] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan
  [23] org.apache.spark.sql.SparkSession.sql
  [24] sun.reflect.NativeMethodAccessorImpl.invoke0
  [25] sun.reflect.NativeMethodAccessorImpl.invoke
  [26] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [27] java.lang.reflect.Method.invoke
  [28] py4j.reflection.MethodInvoker.invoke
  [29] py4j.reflection.ReflectionEngine.invoke
  [30] py4j.Gateway.invoke
  [31] py4j.commands.AbstractCommand.invokeMethod
  [32] py4j.commands.CallCommand.execute
  [33] py4j.GatewayConnection.run
  [34] java.lang.Thread.run
  [35] [tid=16146]

--- 1551158164198242 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.org$apache$spark$sql$catalyst$parser$AstBuilder$$anonfun$$filter$1
  [10] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1$$anonfun$23.apply
  [11] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1$$anonfun$23.apply
  [12] org.apache.spark.sql.catalyst.parser.ParserUtils$EnhancedLogicalPlan$.optionalMap$extension
  [13] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.apply
  [14] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification$1.apply
  [15] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [16] org.apache.spark.sql.catalyst.parser.AstBuilder.org$apache$spark$sql$catalyst$parser$AstBuilder$$withQuerySpecification
  [17] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuerySpecification$1.apply
  [18] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuerySpecification$1.apply
  [19] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [20] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuerySpecification
  [21] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuerySpecification
  [22] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QuerySpecificationContext.accept
  [23] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [24] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault
  [25] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept
  [26] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [27] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault
  [28] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept
  [29] org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit
  [30] org.apache.spark.sql.catalyst.parser.AstBuilder.plan
  [31] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply
  [32] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply
  [33] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [34] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery
  [35] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery
  [36] org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleInsertQueryContext.accept
  [37] org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit
  [38] org.apache.spark.sql.catalyst.parser.AstBuilder.plan
  [39] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply
  [40] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply
  [41] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [42] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery
  [43] org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery
  [44] org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept
  [45] org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren
  [46] org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault
  [47] org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept
  [48] org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit
  [49] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply
  [50] org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply
  [51] org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin
  [52] org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement
  [53] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [54] org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply
  [55] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse
  [56] org.apache.spark.sql.execution.SparkSqlParser.parse
  [57] org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan
  [58] org.apache.spark.sql.SparkSession.sql
  [59] sun.reflect.NativeMethodAccessorImpl.invoke0
  [60] sun.reflect.NativeMethodAccessorImpl.invoke
  [61] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [62] java.lang.reflect.Method.invoke
  [63] py4j.reflection.MethodInvoker.invoke
  [64] py4j.reflection.ReflectionEngine.invoke
  [65] py4j.Gateway.invoke
  [66] py4j.commands.AbstractCommand.invokeMethod
  [67] py4j.commands.CallCommand.execute
  [68] py4j.GatewayConnection.run
  [69] java.lang.Thread.run
  [70] [tid=16146]

--- 1551158164299214 us
  [ 0] JVM_GetCallerClass
  [ 1] sun.reflect.Reflection.getCallerClass
  [ 2] java.lang.Class.getDeclaredConstructor
  [ 3] org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect
  [ 4] org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute
  [ 5] org.apache.spark.sql.internal.SharedState.externalCatalog
  [ 6] org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute
  [ 7] org.apache.spark.sql.internal.SharedState.globalTempViewManager
  [ 8] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$4.apply
  [ 9] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$4.apply
  [10] org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute
  [11] org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager
  [12] org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation
  [13] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog
  [14] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation
  [15] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse
  [16] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse
  [17] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [18] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [19] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [21] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [22] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [23] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [24] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [25] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [26] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [27] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [28] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [29] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [32] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [33] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [34] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [35] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [36] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [37] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [38] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [39] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [41] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [42] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [43] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [44] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [45] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [46] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [47] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [48] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [49] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [50] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [51] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [52] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [53] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [54] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [55] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [56] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [57] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [58] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [59] scala.collection.LinearSeqOptimized$class.foldLeft
  [60] scala.collection.immutable.List.foldLeft
  [61] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [62] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [63] scala.collection.immutable.List.foreach
  [64] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [65] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [66] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [67] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [68] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [69] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [70] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [71] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [72] org.apache.spark.sql.execution.QueryExecution.analyzed
  [73] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [74] org.apache.spark.sql.Dataset$.ofRows
  [75] org.apache.spark.sql.SparkSession.sql
  [76] sun.reflect.NativeMethodAccessorImpl.invoke0
  [77] sun.reflect.NativeMethodAccessorImpl.invoke
  [78] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [79] java.lang.reflect.Method.invoke
  [80] py4j.reflection.MethodInvoker.invoke
  [81] py4j.reflection.ReflectionEngine.invoke
  [82] py4j.Gateway.invoke
  [83] py4j.commands.AbstractCommand.invokeMethod
  [84] py4j.commands.CallCommand.execute
  [85] py4j.GatewayConnection.run
  [86] java.lang.Thread.run
  [87] [tid=16146]

--- 1551158164399049 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.commons.lang3.ClassUtils.isAssignable
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$6.apply
  [14] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$6.apply
  [15] scala.collection.IndexedSeqOptimized$$anonfun$1.apply
  [16] scala.collection.IndexedSeqOptimized$$anonfun$1.apply
  [17] scala.collection.IndexedSeqOptimized$class.segmentLength
  [18] scala.collection.mutable.ArrayOps$ofRef.segmentLength
  [19] scala.collection.GenSeqLike$class.prefixLength
  [20] scala.collection.mutable.ArrayOps$ofRef.prefixLength
  [21] scala.collection.IndexedSeqOptimized$class.find
  [22] scala.collection.mutable.ArrayOps$ofRef.find
  [23] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply
  [24] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply
  [25] org.apache.spark.sql.catalyst.errors.package$.attachTree
  [26] org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy
  [27] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [29] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [32] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [33] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [34] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [35] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [36] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [37] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [38] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [39] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [41] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [42] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [43] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [44] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply
  [45] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [46] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [47] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [48] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [49] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [50] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [51] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [52] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [53] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [54] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply
  [55] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [56] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [57] scala.collection.LinearSeqOptimized$class.foldLeft
  [58] scala.collection.immutable.List.foldLeft
  [59] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [60] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [61] scala.collection.immutable.List.foreach
  [62] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [63] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [64] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [65] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [66] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [67] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [68] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [69] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [70] org.apache.spark.sql.execution.QueryExecution.analyzed
  [71] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [72] org.apache.spark.sql.Dataset$.ofRows
  [73] org.apache.spark.sql.SparkSession.sql
  [74] sun.reflect.NativeMethodAccessorImpl.invoke0
  [75] sun.reflect.NativeMethodAccessorImpl.invoke
  [76] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [77] java.lang.reflect.Method.invoke
  [78] py4j.reflection.MethodInvoker.invoke
  [79] py4j.reflection.ReflectionEngine.invoke
  [80] py4j.Gateway.invoke
  [81] py4j.commands.AbstractCommand.invokeMethod
  [82] py4j.commands.CallCommand.execute
  [83] py4j.GatewayConnection.run
  [84] java.lang.Thread.run
  [85] [tid=16146]

--- 1551158164499140 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.expressions.BinaryComparison.checkInputDataTypes
  [12] org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute
  [13] org.apache.spark.sql.catalyst.expressions.Expression.resolved
  [14] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolved$1.apply
  [15] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolved$1.apply
  [16] scala.collection.LinearSeqOptimized$class.forall
  [17] scala.collection.immutable.Stream.forall
  [18] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolved$lzycompute
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolved
  [20] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$childrenResolved$1.apply
  [21] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$childrenResolved$1.apply
  [22] scala.collection.LinearSeqOptimized$class.forall
  [23] scala.collection.immutable.List.forall
  [24] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.childrenResolved
  [25] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse
  [26] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse
  [27] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$3.apply
  [28] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$3.apply
  [29] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [32] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [33] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [34] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [35] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply
  [36] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply
  [37] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [38] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [39] scala.collection.LinearSeqOptimized$class.foldLeft
  [40] scala.collection.immutable.List.foldLeft
  [41] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [42] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [43] scala.collection.immutable.List.foreach
  [44] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [45] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [46] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [47] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [48] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [49] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [50] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [51] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [52] org.apache.spark.sql.execution.QueryExecution.analyzed
  [53] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [54] org.apache.spark.sql.Dataset$.ofRows
  [55] org.apache.spark.sql.SparkSession.sql
  [56] sun.reflect.NativeMethodAccessorImpl.invoke0
  [57] sun.reflect.NativeMethodAccessorImpl.invoke
  [58] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [59] java.lang.reflect.Method.invoke
  [60] py4j.reflection.MethodInvoker.invoke
  [61] py4j.reflection.ReflectionEngine.invoke
  [62] py4j.Gateway.invoke
  [63] py4j.commands.AbstractCommand.invokeMethod
  [64] py4j.commands.CallCommand.execute
  [65] py4j.GatewayConnection.run
  [66] java.lang.Thread.run
  [67] [tid=16146]

--- 1551158164602462 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.Column$.apply
  [10] org.apache.spark.sql.Dataset$$anonfun$9.apply
  [11] org.apache.spark.sql.Dataset$$anonfun$9.apply
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.immutable.List.foreach
  [15] scala.collection.TraversableLike$class.map
  [16] scala.collection.immutable.List.map
  [17] org.apache.spark.sql.Dataset.getRows
  [18] org.apache.spark.sql.Dataset.showString
  [19] sun.reflect.NativeMethodAccessorImpl.invoke0
  [20] sun.reflect.NativeMethodAccessorImpl.invoke
  [21] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [22] java.lang.reflect.Method.invoke
  [23] py4j.reflection.MethodInvoker.invoke
  [24] py4j.reflection.ReflectionEngine.invoke
  [25] py4j.Gateway.invoke
  [26] py4j.commands.AbstractCommand.invokeMethod
  [27] py4j.commands.CallCommand.execute
  [28] py4j.GatewayConnection.run
  [29] java.lang.Thread.run
  [30] [tid=16146]

--- 1551158164703431 us
  [ 0] CodeHeap::find_start(void*) const
  [ 1] CodeCache::find_blob(void*)
  [ 2] InterpreterRuntime::cache_entry(JavaThread*)
  [ 3] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [ 4] scala.collection.immutable.StringLike$class.$times
  [ 5] scala.collection.immutable.StringOps.$times
  [ 6] org.apache.spark.sql.catalyst.util.package$$anonfun$sideBySide$1.apply
  [ 7] org.apache.spark.sql.catalyst.util.package$$anonfun$sideBySide$1.apply
  [ 8] scala.collection.TraversableLike$$anonfun$map$1.apply
  [ 9] scala.collection.TraversableLike$$anonfun$map$1.apply
  [10] scala.collection.mutable.ResizableArray$class.foreach
  [11] scala.collection.mutable.ArrayBuffer.foreach
  [12] scala.collection.TraversableLike$class.map
  [13] scala.collection.AbstractTraversable.map
  [14] org.apache.spark.sql.catalyst.util.package$.sideBySide
  [15] org.apache.spark.sql.catalyst.util.package$.sideBySide
  [16] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [17] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [18] org.apache.spark.internal.Logging$class.logTrace
  [19] org.apache.spark.sql.catalyst.rules.RuleExecutor.logTrace
  [20] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [21] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [22] scala.collection.LinearSeqOptimized$class.foldLeft
  [23] scala.collection.immutable.List.foldLeft
  [24] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [25] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [26] scala.collection.immutable.List.foreach
  [27] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [28] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [29] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [30] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [31] org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply
  [32] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer
  [33] org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck
  [34] org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute
  [35] org.apache.spark.sql.execution.QueryExecution.analyzed
  [36] org.apache.spark.sql.execution.QueryExecution.assertAnalyzed
  [37] org.apache.spark.sql.Dataset$.ofRows
  [38] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan
  [39] org.apache.spark.sql.Dataset.select
  [40] org.apache.spark.sql.Dataset.getRows
  [41] org.apache.spark.sql.Dataset.showString
  [42] sun.reflect.NativeMethodAccessorImpl.invoke0
  [43] sun.reflect.NativeMethodAccessorImpl.invoke
  [44] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [45] java.lang.reflect.Method.invoke
  [46] py4j.reflection.MethodInvoker.invoke
  [47] py4j.reflection.ReflectionEngine.invoke
  [48] py4j.Gateway.invoke
  [49] py4j.commands.AbstractCommand.invokeMethod
  [50] py4j.commands.CallCommand.execute
  [51] py4j.GatewayConnection.run
  [52] java.lang.Thread.run
  [53] [tid=16146]

--- 1551158164804857 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.plans.logical.Limit$.apply
  [10] org.apache.spark.sql.Dataset.limit
  [11] org.apache.spark.sql.Dataset.head
  [12] org.apache.spark.sql.Dataset.take
  [13] org.apache.spark.sql.Dataset.getRows
  [14] org.apache.spark.sql.Dataset.showString
  [15] sun.reflect.NativeMethodAccessorImpl.invoke0
  [16] sun.reflect.NativeMethodAccessorImpl.invoke
  [17] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [18] java.lang.reflect.Method.invoke
  [19] py4j.reflection.MethodInvoker.invoke
  [20] py4j.reflection.ReflectionEngine.invoke
  [21] py4j.Gateway.invoke
  [22] py4j.commands.AbstractCommand.invokeMethod
  [23] py4j.commands.CallCommand.execute
  [24] py4j.GatewayConnection.run
  [25] java.lang.Thread.run
  [26] [tid=16146]

--- 1551158164908280 us
  [ 0] org.apache.spark.sql.catalyst.expressions.ExprId.id
  [ 1] org.apache.spark.sql.catalyst.expressions.ExprId.hashCode
  [ 2] org.apache.spark.sql.catalyst.expressions.AttributeReference.hashCode
  [ 3] scala.runtime.ScalaRunTime$.hash
  [ 4] scala.collection.immutable.HashSet.elemHashCode
  [ 5] scala.collection.immutable.HashSet.computeHash
  [ 6] scala.collection.immutable.HashSet.$plus
  [ 7] scala.collection.immutable.HashSet.$plus
  [ 8] scala.collection.immutable.Set$Set4.$plus
  [ 9] scala.collection.immutable.Set$Set4.$plus
  [10] scala.collection.mutable.SetBuilder.$plus$eq
  [11] scala.collection.mutable.SetBuilder.$plus$eq
  [12] scala.collection.TraversableLike$$anonfun$map$1.apply
  [13] scala.collection.TraversableLike$$anonfun$map$1.apply
  [14] scala.collection.immutable.HashSet$HashSet1.foreach
  [15] scala.collection.immutable.HashSet$HashTrieSet.foreach
  [16] scala.collection.immutable.HashSet$HashTrieSet.foreach
  [17] scala.collection.TraversableLike$class.map
  [18] scala.collection.AbstractSet.scala$collection$SetLike$$super$map
  [19] scala.collection.SetLike$class.map
  [20] scala.collection.AbstractSet.map
  [21] org.apache.spark.sql.catalyst.expressions.AttributeSet.foreach
  [22] scala.collection.TraversableLike$class.map
  [23] org.apache.spark.sql.catalyst.expressions.AttributeSet.map
  [24] org.apache.spark.sql.catalyst.expressions.AttributeSet.$minus$minus
  [25] org.apache.spark.sql.catalyst.plans.QueryPlan.missingInput
  [26] org.apache.spark.sql.catalyst.plans.QueryPlan.statePrefix
  [27] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.statePrefix
  [28] org.apache.spark.sql.catalyst.plans.QueryPlan.simpleString
  [29] org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString
  [30] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [31] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [32] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [33] org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString
  [34] org.apache.spark.sql.catalyst.trees.TreeNode.treeString
  [35] org.apache.spark.sql.catalyst.trees.TreeNode.treeString
  [36] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [37] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$apply$2.apply
  [38] org.apache.spark.internal.Logging$class.logTrace
  [39] org.apache.spark.sql.catalyst.rules.RuleExecutor.logTrace
  [40] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [41] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [42] scala.collection.LinearSeqOptimized$class.foldLeft
  [43] scala.collection.immutable.List.foldLeft
  [44] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [45] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [46] scala.collection.immutable.List.foreach
  [47] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [48] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [49] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [50] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [51] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [52] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [53] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [54] org.apache.spark.sql.Dataset.withAction
  [55] org.apache.spark.sql.Dataset.head
  [56] org.apache.spark.sql.Dataset.take
  [57] org.apache.spark.sql.Dataset.getRows
  [58] org.apache.spark.sql.Dataset.showString
  [59] sun.reflect.NativeMethodAccessorImpl.invoke0
  [60] sun.reflect.NativeMethodAccessorImpl.invoke
  [61] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [62] java.lang.reflect.Method.invoke
  [63] py4j.reflection.MethodInvoker.invoke
  [64] py4j.reflection.ReflectionEngine.invoke
  [65] py4j.Gateway.invoke
  [66] py4j.commands.AbstractCommand.invokeMethod
  [67] py4j.commands.CallCommand.execute
  [68] py4j.GatewayConnection.run
  [69] java.lang.Thread.run
  [70] [tid=16146]

--- 1551158165027432 us
  [ 0] klassItable::initialize_itable(bool, Thread*)
  [ 1] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 2] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 3] InstanceKlass::initialize(Thread*)
  [ 4] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 5] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [ 6] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [ 7] org.apache.spark.sql.catalyst.plans.logical.ConstraintHelper$class.constructIsNotNullConstraints
  [ 8] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constructIsNotNullConstraints
  [ 9] org.apache.spark.sql.catalyst.plans.logical.QueryPlanConstraints$class.constraints
  [10] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints$lzycompute
  [11] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints
  [12] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.org$apache$spark$sql$catalyst$optimizer$InferFiltersFromConstraints$$getAllConstraints
  [13] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$inferFilters$1.applyOrElse
  [14] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$inferFilters$1.applyOrElse
  [15] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [16] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [17] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [18] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [19] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown
  [20] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [21] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [22] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [23] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply
  [24] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply
  [25] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [26] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [27] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [28] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [29] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [31] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [32] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [33] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply
  [34] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply
  [35] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [36] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [37] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [38] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [39] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown
  [40] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown
  [41] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [42] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown
  [43] org.apache.spark.sql.catalyst.trees.TreeNode.transform
  [44] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.inferFilters
  [45] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.apply
  [46] org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.apply
  [47] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [48] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [49] scala.collection.IndexedSeqOptimized$class.foldl
  [50] scala.collection.IndexedSeqOptimized$class.foldLeft
  [51] scala.collection.mutable.WrappedArray.foldLeft
  [52] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [53] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [54] scala.collection.immutable.List.foreach
  [55] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [56] org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute
  [57] org.apache.spark.sql.execution.QueryExecution.optimizedPlan
  [58] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [59] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [60] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [61] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [62] org.apache.spark.sql.Dataset.withAction
  [63] org.apache.spark.sql.Dataset.head
  [64] org.apache.spark.sql.Dataset.take
  [65] org.apache.spark.sql.Dataset.getRows
  [66] org.apache.spark.sql.Dataset.showString
  [67] sun.reflect.NativeMethodAccessorImpl.invoke0
  [68] sun.reflect.NativeMethodAccessorImpl.invoke
  [69] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [70] java.lang.reflect.Method.invoke
  [71] py4j.reflection.MethodInvoker.invoke
  [72] py4j.reflection.ReflectionEngine.invoke
  [73] py4j.Gateway.invoke
  [74] py4j.commands.AbstractCommand.invokeMethod
  [75] py4j.commands.CallCommand.execute
  [76] py4j.GatewayConnection.run
  [77] java.lang.Thread.run
  [78] [tid=16146]

--- 1551158165127656 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [12] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply
  [13] scala.Option.getOrElse
  [14] org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$class.stats
  [15] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats
  [16] org.apache.spark.sql.execution.SparkStrategies$JoinSelection$.canBroadcastByHints
  [17] org.apache.spark.sql.execution.SparkStrategies$JoinSelection$.apply
  [18] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply
  [19] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply
  [20] scala.collection.Iterator$$anon$12.nextCur
  [21] scala.collection.Iterator$$anon$12.hasNext
  [22] scala.collection.Iterator$$anon$12.hasNext
  [23] org.apache.spark.sql.catalyst.planning.QueryPlanner.plan
  [24] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply
  [25] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply
  [26] scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply
  [27] scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply
  [28] scala.collection.Iterator$class.foreach
  [29] scala.collection.AbstractIterator.foreach
  [30] scala.collection.TraversableOnce$class.foldLeft
  [31] scala.collection.AbstractIterator.foldLeft
  [32] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply
  [33] org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply
  [34] scala.collection.Iterator$$anon$12.nextCur
  [35] scala.collection.Iterator$$anon$12.hasNext
  [36] org.apache.spark.sql.catalyst.planning.QueryPlanner.plan
  [37] org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute
  [38] org.apache.spark.sql.execution.QueryExecution.sparkPlan
  [39] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [40] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [41] org.apache.spark.sql.Dataset.withAction
  [42] org.apache.spark.sql.Dataset.head
  [43] org.apache.spark.sql.Dataset.take
  [44] org.apache.spark.sql.Dataset.getRows
  [45] org.apache.spark.sql.Dataset.showString
  [46] sun.reflect.NativeMethodAccessorImpl.invoke0
  [47] sun.reflect.NativeMethodAccessorImpl.invoke
  [48] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [49] java.lang.reflect.Method.invoke
  [50] py4j.reflection.MethodInvoker.invoke
  [51] py4j.reflection.ReflectionEngine.invoke
  [52] py4j.Gateway.invoke
  [53] py4j.commands.AbstractCommand.invokeMethod
  [54] py4j.commands.CallCommand.execute
  [55] py4j.GatewayConnection.run
  [56] java.lang.Thread.run
  [57] [tid=16146]

--- 1551158165231611 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [10] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [12] org.apache.spark.sql.execution.SparkPlan$$anonfun$requiredChildDistribution$1.apply
  [13] org.apache.spark.sql.execution.SparkPlan$$anonfun$requiredChildDistribution$1.apply
  [14] scala.collection.generic.GenTraversableFactory.fill
  [15] org.apache.spark.sql.execution.SparkPlan.requiredChildDistribution
  [16] org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering
  [17] org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse
  [18] org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse
  [19] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [20] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply
  [21] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [22] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [23] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [24] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [25] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [26] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [27] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [28] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [29] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [30] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply
  [31] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply
  [32] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [33] org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren
  [34] org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
  [35] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [36] org.apache.spark.sql.execution.exchange.EnsureRequirements.apply
  [37] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [38] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [39] scala.collection.LinearSeqOptimized$class.foldLeft
  [40] scala.collection.immutable.List.foldLeft
  [41] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [42] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [43] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [44] org.apache.spark.sql.Dataset.withAction
  [45] org.apache.spark.sql.Dataset.head
  [46] org.apache.spark.sql.Dataset.take
  [47] org.apache.spark.sql.Dataset.getRows
  [48] org.apache.spark.sql.Dataset.showString
  [49] sun.reflect.NativeMethodAccessorImpl.invoke0
  [50] sun.reflect.NativeMethodAccessorImpl.invoke
  [51] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [52] java.lang.reflect.Method.invoke
  [53] py4j.reflection.MethodInvoker.invoke
  [54] py4j.reflection.ReflectionEngine.invoke
  [55] py4j.Gateway.invoke
  [56] py4j.commands.AbstractCommand.invokeMethod
  [57] py4j.commands.CallCommand.execute
  [58] py4j.GatewayConnection.run
  [59] java.lang.Thread.run
  [60] [tid=16146]

--- 1551158165340233 us
  [ 0] sun.nio.cs.UTF_8$Encoder.encode
  [ 1] java.util.zip.ZipCoder.getBytes
  [ 2] java.util.zip.ZipFile.getEntry
  [ 3] java.util.jar.JarFile.getEntry
  [ 4] java.util.jar.JarFile.getJarEntry
  [ 5] sun.misc.URLClassPath$JarLoader.getResource
  [ 6] sun.misc.URLClassPath.getResource
  [ 7] java.net.URLClassLoader$1.run
  [ 8] java.net.URLClassLoader$1.run
  [ 9] java.security.AccessController.doPrivileged
  [10] java.net.URLClassLoader.findClass
  [11] java.lang.ClassLoader.loadClass
  [12] sun.misc.Launcher$AppClassLoader.loadClass
  [13] java.lang.ClassLoader.loadClass
  [14] org.apache.spark.sql.execution.CollapseCodegenStages.supportCodegen
  [15] org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen
  [16] org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply
  [17] org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply
  [18] scala.collection.TraversableLike$$anonfun$map$1.apply
  [19] scala.collection.TraversableLike$$anonfun$map$1.apply
  [20] scala.collection.immutable.List.foreach
  [21] scala.collection.TraversableLike$class.map
  [22] scala.collection.immutable.List.map
  [23] org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen
  [24] org.apache.spark.sql.execution.CollapseCodegenStages.apply
  [25] org.apache.spark.sql.execution.CollapseCodegenStages.apply
  [26] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [27] org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply
  [28] scala.collection.LinearSeqOptimized$class.foldLeft
  [29] scala.collection.immutable.List.foldLeft
  [30] org.apache.spark.sql.execution.QueryExecution.prepareForExecution
  [31] org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute
  [32] org.apache.spark.sql.execution.QueryExecution.executedPlan
  [33] org.apache.spark.sql.Dataset.withAction
  [34] org.apache.spark.sql.Dataset.head
  [35] org.apache.spark.sql.Dataset.take
  [36] org.apache.spark.sql.Dataset.getRows
  [37] org.apache.spark.sql.Dataset.showString
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158165455580 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveDeserializer$$validateNestedTupleFields
  [10] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36$$anonfun$applyOrElse$10.applyOrElse
  [11] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36$$anonfun$applyOrElse$10.applyOrElse
  [12] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [13] org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply
  [14] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [15] org.apache.spark.sql.catalyst.trees.TreeNode.transformDown
  [16] org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply
  [17] org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply
  [18] org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply
  [19] org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply
  [20] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [21] org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1
  [22] org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1
  [23] org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply
  [24] org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator
  [25] org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions
  [26] org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown
  [27] org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions
  [28] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36.applyOrElse
  [29] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36.applyOrElse
  [30] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [31] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply
  [32] org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin
  [33] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [34] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply
  [35] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer
  [36] org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp
  [37] org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp
  [38] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply
  [39] org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply
  [40] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [41] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply
  [42] scala.collection.LinearSeqOptimized$class.foldLeft
  [43] scala.collection.immutable.List.foldLeft
  [44] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [45] org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply
  [46] scala.collection.immutable.List.foreach
  [47] org.apache.spark.sql.catalyst.rules.RuleExecutor.execute
  [48] org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext
  [49] org.apache.spark.sql.catalyst.analysis.Analyzer.execute
  [50] org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind
  [51] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer$lzycompute
  [52] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer
  [53] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [54] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [55] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [56] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [57] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [58] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [59] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [60] org.apache.spark.sql.Dataset.withAction
  [61] org.apache.spark.sql.Dataset.head
  [62] org.apache.spark.sql.Dataset.take
  [63] org.apache.spark.sql.Dataset.getRows
  [64] org.apache.spark.sql.Dataset.showString
  [65] sun.reflect.NativeMethodAccessorImpl.invoke0
  [66] sun.reflect.NativeMethodAccessorImpl.invoke
  [67] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [68] java.lang.reflect.Method.invoke
  [69] py4j.reflection.MethodInvoker.invoke
  [70] py4j.reflection.ReflectionEngine.invoke
  [71] py4j.Gateway.invoke
  [72] py4j.commands.AbstractCommand.invokeMethod
  [73] py4j.commands.CallCommand.execute
  [74] py4j.GatewayConnection.run
  [75] java.lang.Thread.run
  [76] [tid=16146]

--- 1551158165558151 us
  [ 0] java.math.BigInteger.multiplyToLen
  [ 1] java.math.BigInteger.implMontgomeryMultiply
  [ 2] java.math.BigInteger.montgomeryMultiply
  [ 3] java.math.BigInteger.oddModPow
  [ 4] java.math.BigInteger.modPow
  [ 5] sun.security.provider.DSA.generateV
  [ 6] sun.security.provider.DSA.engineVerify
  [ 7] sun.security.provider.DSA.engineVerify
  [ 8] java.security.Signature$Delegate.engineVerify
  [ 9] java.security.Signature.verify
  [10] sun.security.pkcs.SignerInfo.verify
  [11] sun.security.pkcs.PKCS7.verify
  [12] sun.security.pkcs.PKCS7.verify
  [13] sun.security.util.SignatureFileVerifier.processImpl
  [14] sun.security.util.SignatureFileVerifier.process
  [15] java.util.jar.JarVerifier.processEntry
  [16] java.util.jar.JarVerifier.update
  [17] java.util.jar.JarFile.initializeVerifier
  [18] java.util.jar.JarFile.ensureInitialization
  [19] java.util.jar.JavaUtilJarAccessImpl.ensureInitialization
  [20] sun.misc.URLClassPath$JarLoader$2.getManifest
  [21] java.net.URLClassLoader.defineClass
  [22] java.net.URLClassLoader.access$100
  [23] java.net.URLClassLoader$1.run
  [24] java.net.URLClassLoader$1.run
  [25] java.security.AccessController.doPrivileged
  [26] java.net.URLClassLoader.findClass
  [27] java.lang.ClassLoader.loadClass
  [28] sun.misc.Launcher$AppClassLoader.loadClass
  [29] java.lang.ClassLoader.loadClass
  [30] org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.variable
  [31] org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.isNullVariable
  [32] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [33] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [34] scala.Option.getOrElse
  [35] org.apache.spark.sql.catalyst.expressions.Expression.genCode
  [36] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [37] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [38] scala.collection.TraversableLike$$anonfun$map$1.apply
  [39] scala.collection.TraversableLike$$anonfun$map$1.apply
  [40] scala.collection.immutable.List.foreach
  [41] scala.collection.TraversableLike$class.map
  [42] scala.collection.immutable.List.map
  [43] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [44] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [46] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [47] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [48] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [49] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [50] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [51] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [52] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [53] org.apache.spark.sql.Dataset.withAction
  [54] org.apache.spark.sql.Dataset.head
  [55] org.apache.spark.sql.Dataset.take
  [56] org.apache.spark.sql.Dataset.getRows
  [57] org.apache.spark.sql.Dataset.showString
  [58] sun.reflect.NativeMethodAccessorImpl.invoke0
  [59] sun.reflect.NativeMethodAccessorImpl.invoke
  [60] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [61] java.lang.reflect.Method.invoke
  [62] py4j.reflection.MethodInvoker.invoke
  [63] py4j.reflection.ReflectionEngine.invoke
  [64] py4j.Gateway.invoke
  [65] py4j.commands.AbstractCommand.invokeMethod
  [66] py4j.commands.CallCommand.execute
  [67] py4j.GatewayConnection.run
  [68] java.lang.Thread.run
  [69] [tid=16146]

--- 1551158165659780 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_field(fieldDescriptor&, KlassHandle, Symbol*, Symbol*, KlassHandle, Bytecodes::Code, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$buildCodeBlocks$1.apply
  [12] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$buildCodeBlocks$1.apply
  [13] scala.collection.immutable.List.foreach
  [14] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.buildCodeBlocks
  [15] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressions
  [16] org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressionsWithCurrentInputs
  [17] org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow.doGenCode
  [18] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [19] org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply
  [20] scala.Option.getOrElse
  [21] org.apache.spark.sql.catalyst.expressions.Expression.genCode
  [22] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [23] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$$anonfun$3.apply
  [24] scala.collection.TraversableLike$$anonfun$map$1.apply
  [25] scala.collection.TraversableLike$$anonfun$map$1.apply
  [26] scala.collection.immutable.List.foreach
  [27] scala.collection.TraversableLike$class.map
  [28] scala.collection.immutable.List.map
  [29] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [30] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [31] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [32] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [33] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [34] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [35] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [36] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [37] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [38] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [39] org.apache.spark.sql.Dataset.withAction
  [40] org.apache.spark.sql.Dataset.head
  [41] org.apache.spark.sql.Dataset.take
  [42] org.apache.spark.sql.Dataset.getRows
  [43] org.apache.spark.sql.Dataset.showString
  [44] sun.reflect.NativeMethodAccessorImpl.invoke0
  [45] sun.reflect.NativeMethodAccessorImpl.invoke
  [46] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [47] java.lang.reflect.Method.invoke
  [48] py4j.reflection.MethodInvoker.invoke
  [49] py4j.reflection.ReflectionEngine.invoke
  [50] py4j.Gateway.invoke
  [51] py4j.commands.AbstractCommand.invokeMethod
  [52] py4j.commands.CallCommand.execute
  [53] py4j.GatewayConnection.run
  [54] java.lang.Thread.run
  [55] [tid=16146]

--- 1551158165764809 us
  [ 0] _copy_to_user_[k]
  [ 1] put_timespec64_[k]
  [ 2] sys_clock_gettime_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __vdso_clock_gettime
  [ 6] __GI___clock_gettime
  [ 7] [unknown]
  [ 8] java.lang.ClassLoader.defineClass1
  [ 9] java.lang.ClassLoader.defineClass
  [10] java.security.SecureClassLoader.defineClass
  [11] java.net.URLClassLoader.defineClass
  [12] java.net.URLClassLoader.access$100
  [13] java.net.URLClassLoader$1.run
  [14] java.net.URLClassLoader$1.run
  [15] java.security.AccessController.doPrivileged
  [16] java.net.URLClassLoader.findClass
  [17] java.lang.ClassLoader.loadClass
  [18] sun.misc.Launcher$AppClassLoader.loadClass
  [19] java.lang.ClassLoader.loadClass
  [20] java.lang.ClassLoader.defineClass1
  [21] java.lang.ClassLoader.defineClass
  [22] java.security.SecureClassLoader.defineClass
  [23] java.net.URLClassLoader.defineClass
  [24] java.net.URLClassLoader.access$100
  [25] java.net.URLClassLoader$1.run
  [26] java.net.URLClassLoader$1.run
  [27] java.security.AccessController.doPrivileged
  [28] java.net.URLClassLoader.findClass
  [29] java.lang.ClassLoader.loadClass
  [30] sun.misc.Launcher$AppClassLoader.loadClass
  [31] java.lang.ClassLoader.loadClass
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [34] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [35] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [36] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [37] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [38] org.spark_project.guava.cache.LocalCache$Segment.get
  [39] org.spark_project.guava.cache.LocalCache.get
  [40] org.spark_project.guava.cache.LocalCache.getOrLoad
  [41] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [42] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [43] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [44] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [46] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [47] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [48] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [49] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [50] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [51] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [52] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [53] org.apache.spark.sql.Dataset.withAction
  [54] org.apache.spark.sql.Dataset.head
  [55] org.apache.spark.sql.Dataset.take
  [56] org.apache.spark.sql.Dataset.getRows
  [57] org.apache.spark.sql.Dataset.showString
  [58] sun.reflect.NativeMethodAccessorImpl.invoke0
  [59] sun.reflect.NativeMethodAccessorImpl.invoke
  [60] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [61] java.lang.reflect.Method.invoke
  [62] py4j.reflection.MethodInvoker.invoke
  [63] py4j.reflection.ReflectionEngine.invoke
  [64] py4j.Gateway.invoke
  [65] py4j.commands.AbstractCommand.invokeMethod
  [66] py4j.commands.CallCommand.execute
  [67] py4j.GatewayConnection.run
  [68] java.lang.Thread.run
  [69] [tid=16146]

--- 1551158165867886 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.ClassBodyEvaluator.cook
  [10] org.codehaus.janino.SimpleCompiler.cook
  [11] org.codehaus.commons.compiler.Cookable.cook
  [12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [13] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [14] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [15] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [16] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [17] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [18] org.spark_project.guava.cache.LocalCache$Segment.get
  [19] org.spark_project.guava.cache.LocalCache.get
  [20] org.spark_project.guava.cache.LocalCache.getOrLoad
  [21] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [24] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [26] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [27] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [28] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [29] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [30] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [31] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [32] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [33] org.apache.spark.sql.Dataset.withAction
  [34] org.apache.spark.sql.Dataset.head
  [35] org.apache.spark.sql.Dataset.take
  [36] org.apache.spark.sql.Dataset.getRows
  [37] org.apache.spark.sql.Dataset.showString
  [38] sun.reflect.NativeMethodAccessorImpl.invoke0
  [39] sun.reflect.NativeMethodAccessorImpl.invoke
  [40] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [41] java.lang.reflect.Method.invoke
  [42] py4j.reflection.MethodInvoker.invoke
  [43] py4j.reflection.ReflectionEngine.invoke
  [44] py4j.Gateway.invoke
  [45] py4j.commands.AbstractCommand.invokeMethod
  [46] py4j.commands.CallCommand.execute
  [47] py4j.GatewayConnection.run
  [48] java.lang.Thread.run
  [49] [tid=16146]

--- 1551158165969287 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 8] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 9] InstanceKlass::initialize(Thread*)
  [10] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [11] org.codehaus.janino.Java$Rvalue.setEnclosingScope
  [12] org.codehaus.janino.Java$ReturnStatement.<init>
  [13] org.codehaus.janino.Parser.parseReturnStatement
  [14] org.codehaus.janino.Parser.parseStatement
  [15] org.codehaus.janino.Parser.parseBlockStatement
  [16] org.codehaus.janino.Parser.parseBlockStatements
  [17] org.codehaus.janino.Parser.parseMethodDeclarationRest
  [18] org.codehaus.janino.Parser.parseClassBodyDeclaration
  [19] org.codehaus.janino.ClassBodyEvaluator.cook
  [20] org.codehaus.janino.SimpleCompiler.cook
  [21] org.codehaus.commons.compiler.Cookable.cook
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [25] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [26] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [27] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [28] org.spark_project.guava.cache.LocalCache$Segment.get
  [29] org.spark_project.guava.cache.LocalCache.get
  [30] org.spark_project.guava.cache.LocalCache.getOrLoad
  [31] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [36] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [39] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [40] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [41] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [42] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [43] org.apache.spark.sql.Dataset.withAction
  [44] org.apache.spark.sql.Dataset.head
  [45] org.apache.spark.sql.Dataset.take
  [46] org.apache.spark.sql.Dataset.getRows
  [47] org.apache.spark.sql.Dataset.showString
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16146]

--- 1551158166070559 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.util.AbstractTraverser.<init>
  [10] org.codehaus.janino.Java$Rvalue$1.<init>
  [11] org.codehaus.janino.Java$Rvalue.setEnclosingScope
  [12] org.codehaus.janino.Java$ReturnStatement.<init>
  [13] org.codehaus.janino.Parser.parseReturnStatement
  [14] org.codehaus.janino.Parser.parseStatement
  [15] org.codehaus.janino.Parser.parseBlockStatement
  [16] org.codehaus.janino.Parser.parseBlockStatements
  [17] org.codehaus.janino.Parser.parseMethodDeclarationRest
  [18] org.codehaus.janino.Parser.parseClassBodyDeclaration
  [19] org.codehaus.janino.ClassBodyEvaluator.cook
  [20] org.codehaus.janino.SimpleCompiler.cook
  [21] org.codehaus.commons.compiler.Cookable.cook
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [25] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [26] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [27] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [28] org.spark_project.guava.cache.LocalCache$Segment.get
  [29] org.spark_project.guava.cache.LocalCache.get
  [30] org.spark_project.guava.cache.LocalCache.getOrLoad
  [31] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [36] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [39] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [40] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [41] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [42] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [43] org.apache.spark.sql.Dataset.withAction
  [44] org.apache.spark.sql.Dataset.head
  [45] org.apache.spark.sql.Dataset.take
  [46] org.apache.spark.sql.Dataset.getRows
  [47] org.apache.spark.sql.Dataset.showString
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16146]

--- 1551158166171300 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.codehaus.janino.IClassLoader.loadIClass
  [13] org.codehaus.janino.IClassLoader.requireType
  [14] org.codehaus.janino.IClassLoader.postConstruct
  [15] org.codehaus.janino.ClassLoaderIClassLoader.<init>
  [16] org.codehaus.janino.SimpleCompiler.cook
  [17] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [18] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [19] org.codehaus.janino.ClassBodyEvaluator.cook
  [20] org.codehaus.janino.SimpleCompiler.cook
  [21] org.codehaus.commons.compiler.Cookable.cook
  [22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [23] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [24] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [25] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [26] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [27] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [28] org.spark_project.guava.cache.LocalCache$Segment.get
  [29] org.spark_project.guava.cache.LocalCache.get
  [30] org.spark_project.guava.cache.LocalCache.getOrLoad
  [31] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [32] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [33] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [34] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [36] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [37] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [38] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [39] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [40] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [41] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [42] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [43] org.apache.spark.sql.Dataset.withAction
  [44] org.apache.spark.sql.Dataset.head
  [45] org.apache.spark.sql.Dataset.take
  [46] org.apache.spark.sql.Dataset.getRows
  [47] org.apache.spark.sql.Dataset.showString
  [48] sun.reflect.NativeMethodAccessorImpl.invoke0
  [49] sun.reflect.NativeMethodAccessorImpl.invoke
  [50] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [51] java.lang.reflect.Method.invoke
  [52] py4j.reflection.MethodInvoker.invoke
  [53] py4j.reflection.ReflectionEngine.invoke
  [54] py4j.Gateway.invoke
  [55] py4j.commands.AbstractCommand.invokeMethod
  [56] py4j.commands.CallCommand.execute
  [57] py4j.GatewayConnection.run
  [58] java.lang.Thread.run
  [59] [tid=16146]

--- 1551158166272835 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.SimpleCompiler.cook
  [10] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [11] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [12] org.codehaus.janino.ClassBodyEvaluator.cook
  [13] org.codehaus.janino.SimpleCompiler.cook
  [14] org.codehaus.commons.compiler.Cookable.cook
  [15] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [16] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [18] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [19] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [20] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [21] org.spark_project.guava.cache.LocalCache$Segment.get
  [22] org.spark_project.guava.cache.LocalCache.get
  [23] org.spark_project.guava.cache.LocalCache.getOrLoad
  [24] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [26] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [27] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [28] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [29] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [30] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [31] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [32] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [33] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [34] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [35] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [36] org.apache.spark.sql.Dataset.withAction
  [37] org.apache.spark.sql.Dataset.head
  [38] org.apache.spark.sql.Dataset.take
  [39] org.apache.spark.sql.Dataset.getRows
  [40] org.apache.spark.sql.Dataset.showString
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16146]

--- 1551158166372611 us
  [ 0] java.util.regex.Pattern$CharPropertyNames.<clinit>
  [ 1] java.util.regex.Pattern.charPropertyNodeFor
  [ 2] java.util.regex.Pattern.family
  [ 3] java.util.regex.Pattern.sequence
  [ 4] java.util.regex.Pattern.expr
  [ 5] java.util.regex.Pattern.compile
  [ 6] java.util.regex.Pattern.<init>
  [ 7] java.util.regex.Pattern.compile
  [ 8] org.codehaus.janino.UnitCompiler.<clinit>
  [ 9] org.codehaus.janino.SimpleCompiler.cook
  [10] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [11] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [12] org.codehaus.janino.ClassBodyEvaluator.cook
  [13] org.codehaus.janino.SimpleCompiler.cook
  [14] org.codehaus.commons.compiler.Cookable.cook
  [15] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [16] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [18] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [19] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [20] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [21] org.spark_project.guava.cache.LocalCache$Segment.get
  [22] org.spark_project.guava.cache.LocalCache.get
  [23] org.spark_project.guava.cache.LocalCache.getOrLoad
  [24] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [25] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [26] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [27] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [28] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [29] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [30] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [31] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [32] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [33] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [34] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [35] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [36] org.apache.spark.sql.Dataset.withAction
  [37] org.apache.spark.sql.Dataset.head
  [38] org.apache.spark.sql.Dataset.take
  [39] org.apache.spark.sql.Dataset.getRows
  [40] org.apache.spark.sql.Dataset.showString
  [41] sun.reflect.NativeMethodAccessorImpl.invoke0
  [42] sun.reflect.NativeMethodAccessorImpl.invoke
  [43] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [44] java.lang.reflect.Method.invoke
  [45] py4j.reflection.MethodInvoker.invoke
  [46] py4j.reflection.ReflectionEngine.invoke
  [47] py4j.Gateway.invoke
  [48] py4j.commands.AbstractCommand.invokeMethod
  [49] py4j.commands.CallCommand.execute
  [50] py4j.GatewayConnection.run
  [51] java.lang.Thread.run
  [52] [tid=16146]

--- 1551158166473915 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.buildLocalVariableMap
  [10] org.codehaus.janino.UnitCompiler.buildLocalVariableMap
  [11] org.codehaus.janino.UnitCompiler.compile
  [12] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [13] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [14] org.codehaus.janino.UnitCompiler.compile2
  [15] org.codehaus.janino.UnitCompiler.compile2
  [16] org.codehaus.janino.UnitCompiler.access$400
  [17] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [18] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [19] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [20] org.codehaus.janino.UnitCompiler.compile
  [21] org.codehaus.janino.UnitCompiler.compileUnit
  [22] org.codehaus.janino.SimpleCompiler.cook
  [23] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [24] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [25] org.codehaus.janino.ClassBodyEvaluator.cook
  [26] org.codehaus.janino.SimpleCompiler.cook
  [27] org.codehaus.commons.compiler.Cookable.cook
  [28] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [29] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [30] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [31] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [32] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [33] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [34] org.spark_project.guava.cache.LocalCache$Segment.get
  [35] org.spark_project.guava.cache.LocalCache.get
  [36] org.spark_project.guava.cache.LocalCache.getOrLoad
  [37] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [38] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [39] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [40] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [41] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [42] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [43] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [44] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [45] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [46] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [47] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [48] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [49] org.apache.spark.sql.Dataset.withAction
  [50] org.apache.spark.sql.Dataset.head
  [51] org.apache.spark.sql.Dataset.take
  [52] org.apache.spark.sql.Dataset.getRows
  [53] org.apache.spark.sql.Dataset.showString
  [54] sun.reflect.NativeMethodAccessorImpl.invoke0
  [55] sun.reflect.NativeMethodAccessorImpl.invoke
  [56] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [57] java.lang.reflect.Method.invoke
  [58] py4j.reflection.MethodInvoker.invoke
  [59] py4j.reflection.ReflectionEngine.invoke
  [60] py4j.Gateway.invoke
  [61] py4j.commands.AbstractCommand.invokeMethod
  [62] py4j.commands.CallCommand.execute
  [63] py4j.GatewayConnection.run
  [64] java.lang.Thread.run
  [65] [tid=16146]

--- 1551158166576431 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.util.ClassFile.addConstantMethodrefInfo
  [10] org.codehaus.janino.UnitCompiler.writeConstantMethodrefInfo
  [11] org.codehaus.janino.UnitCompiler.invoke
  [12] org.codehaus.janino.UnitCompiler.invokeConstructor
  [13] org.codehaus.janino.UnitCompiler.compileGet2
  [14] org.codehaus.janino.UnitCompiler.access$9800
  [15] org.codehaus.janino.UnitCompiler$16.visitNewClassInstance
  [16] org.codehaus.janino.UnitCompiler$16.visitNewClassInstance
  [17] org.codehaus.janino.Java$NewClassInstance.accept
  [18] org.codehaus.janino.UnitCompiler.compileGet
  [19] org.codehaus.janino.UnitCompiler.compileGetValue
  [20] org.codehaus.janino.UnitCompiler.compile2
  [21] org.codehaus.janino.UnitCompiler.access$2800
  [22] org.codehaus.janino.UnitCompiler$6.visitReturnStatement
  [23] org.codehaus.janino.UnitCompiler$6.visitReturnStatement
  [24] org.codehaus.janino.Java$ReturnStatement.accept
  [25] org.codehaus.janino.UnitCompiler.compile
  [26] org.codehaus.janino.UnitCompiler.compileStatements
  [27] org.codehaus.janino.UnitCompiler.compile
  [28] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [29] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [30] org.codehaus.janino.UnitCompiler.compile2
  [31] org.codehaus.janino.UnitCompiler.compile2
  [32] org.codehaus.janino.UnitCompiler.access$400
  [33] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [34] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [35] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [36] org.codehaus.janino.UnitCompiler.compile
  [37] org.codehaus.janino.UnitCompiler.compileUnit
  [38] org.codehaus.janino.SimpleCompiler.cook
  [39] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [40] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [41] org.codehaus.janino.ClassBodyEvaluator.cook
  [42] org.codehaus.janino.SimpleCompiler.cook
  [43] org.codehaus.commons.compiler.Cookable.cook
  [44] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [45] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [46] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [47] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [48] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [49] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [50] org.spark_project.guava.cache.LocalCache$Segment.get
  [51] org.spark_project.guava.cache.LocalCache.get
  [52] org.spark_project.guava.cache.LocalCache.getOrLoad
  [53] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [54] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [55] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [56] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [57] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [58] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [59] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [60] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [61] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [62] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [63] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [64] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [65] org.apache.spark.sql.Dataset.withAction
  [66] org.apache.spark.sql.Dataset.head
  [67] org.apache.spark.sql.Dataset.take
  [68] org.apache.spark.sql.Dataset.getRows
  [69] org.apache.spark.sql.Dataset.showString
  [70] sun.reflect.NativeMethodAccessorImpl.invoke0
  [71] sun.reflect.NativeMethodAccessorImpl.invoke
  [72] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [73] java.lang.reflect.Method.invoke
  [74] py4j.reflection.MethodInvoker.invoke
  [75] py4j.reflection.ReflectionEngine.invoke
  [76] py4j.Gateway.invoke
  [77] py4j.commands.AbstractCommand.invokeMethod
  [78] py4j.commands.CallCommand.execute
  [79] py4j.GatewayConnection.run
  [80] java.lang.Thread.run
  [81] [tid=16146]

--- 1551158166676718 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.compileBoolean
  [10] org.codehaus.janino.UnitCompiler.compileGet2
  [11] org.codehaus.janino.UnitCompiler.access$8800
  [12] org.codehaus.janino.UnitCompiler$16.visitConditionalExpression
  [13] org.codehaus.janino.UnitCompiler$16.visitConditionalExpression
  [14] org.codehaus.janino.Java$ConditionalExpression.accept
  [15] org.codehaus.janino.UnitCompiler.compileGet
  [16] org.codehaus.janino.UnitCompiler.compileGetValue
  [17] org.codehaus.janino.UnitCompiler.compile2
  [18] org.codehaus.janino.UnitCompiler.access$2700
  [19] org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement
  [20] org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement
  [21] org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept
  [22] org.codehaus.janino.UnitCompiler.compile
  [23] org.codehaus.janino.UnitCompiler.compileStatements
  [24] org.codehaus.janino.UnitCompiler.compile
  [25] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [26] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [27] org.codehaus.janino.UnitCompiler.compile2
  [28] org.codehaus.janino.UnitCompiler.compile2
  [29] org.codehaus.janino.UnitCompiler.access$700
  [30] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [31] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [32] org.codehaus.janino.Java$MemberClassDeclaration.accept
  [33] org.codehaus.janino.UnitCompiler.compile
  [34] org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes
  [35] org.codehaus.janino.UnitCompiler.compile2
  [36] org.codehaus.janino.UnitCompiler.compile2
  [37] org.codehaus.janino.UnitCompiler.access$400
  [38] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [39] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [40] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [41] org.codehaus.janino.UnitCompiler.compile
  [42] org.codehaus.janino.UnitCompiler.compileUnit
  [43] org.codehaus.janino.SimpleCompiler.cook
  [44] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [45] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [46] org.codehaus.janino.ClassBodyEvaluator.cook
  [47] org.codehaus.janino.SimpleCompiler.cook
  [48] org.codehaus.commons.compiler.Cookable.cook
  [49] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [50] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [51] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [52] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [53] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [54] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [55] org.spark_project.guava.cache.LocalCache$Segment.get
  [56] org.spark_project.guava.cache.LocalCache.get
  [57] org.spark_project.guava.cache.LocalCache.getOrLoad
  [58] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [59] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [60] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [61] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [62] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [63] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [64] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [65] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [66] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [67] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [68] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [69] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [70] org.apache.spark.sql.Dataset.withAction
  [71] org.apache.spark.sql.Dataset.head
  [72] org.apache.spark.sql.Dataset.take
  [73] org.apache.spark.sql.Dataset.getRows
  [74] org.apache.spark.sql.Dataset.showString
  [75] sun.reflect.NativeMethodAccessorImpl.invoke0
  [76] sun.reflect.NativeMethodAccessorImpl.invoke
  [77] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [78] java.lang.reflect.Method.invoke
  [79] py4j.reflection.MethodInvoker.invoke
  [80] py4j.reflection.ReflectionEngine.invoke
  [81] py4j.Gateway.invoke
  [82] py4j.commands.AbstractCommand.invokeMethod
  [83] py4j.commands.CallCommand.execute
  [84] py4j.GatewayConnection.run
  [85] java.lang.Thread.run
  [86] [tid=16146]

--- 1551158166779784 us
  [ 0] ClassVerifier::verify_method(methodHandle, Thread*)
  [ 1] Verifier::verify(instanceKlassHandle, Verifier::Mode, bool, Thread*)
  [ 2] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 3] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 4] InstanceKlass::initialize(Thread*)
  [ 5] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 6] org.apache.spark.sql.catalyst.expressions.GeneratedClass.generate
  [ 7] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [ 8] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create
  [ 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate
  [10] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [11] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [12] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [13] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [14] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [15] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [16] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [17] org.apache.spark.sql.Dataset.withAction
  [18] org.apache.spark.sql.Dataset.head
  [19] org.apache.spark.sql.Dataset.take
  [20] org.apache.spark.sql.Dataset.getRows
  [21] org.apache.spark.sql.Dataset.showString
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] py4j.reflection.MethodInvoker.invoke
  [27] py4j.reflection.ReflectionEngine.invoke
  [28] py4j.Gateway.invoke
  [29] py4j.commands.AbstractCommand.invokeMethod
  [30] py4j.commands.CallCommand.execute
  [31] py4j.GatewayConnection.run
  [32] java.lang.Thread.run
  [33] [tid=16146]

--- 1551158166889073 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen
  [10] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [11] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [12] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [13] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [14] org.apache.spark.rdd.RDDOperationScope$.withScope
  [15] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [16] org.apache.spark.sql.execution.SparkPlan.execute
  [17] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [18] org.apache.spark.sql.execution.SparkPlan.executeTake
  [19] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [20] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [21] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [22] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [23] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [24] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [25] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [26] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [27] org.apache.spark.sql.Dataset.withAction
  [28] org.apache.spark.sql.Dataset.head
  [29] org.apache.spark.sql.Dataset.take
  [30] org.apache.spark.sql.Dataset.getRows
  [31] org.apache.spark.sql.Dataset.showString
  [32] sun.reflect.NativeMethodAccessorImpl.invoke0
  [33] sun.reflect.NativeMethodAccessorImpl.invoke
  [34] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [35] java.lang.reflect.Method.invoke
  [36] py4j.reflection.MethodInvoker.invoke
  [37] py4j.reflection.ReflectionEngine.invoke
  [38] py4j.Gateway.invoke
  [39] py4j.commands.AbstractCommand.invokeMethod
  [40] py4j.commands.CallCommand.execute
  [41] py4j.GatewayConnection.run
  [42] java.lang.Thread.run
  [43] [tid=16146]

--- 1551158166993829 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.codehaus.janino.UnitCompiler.compile
  [10] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [11] org.codehaus.janino.UnitCompiler.compileDeclaredMethods
  [12] org.codehaus.janino.UnitCompiler.compile2
  [13] org.codehaus.janino.UnitCompiler.compile2
  [14] org.codehaus.janino.UnitCompiler.access$700
  [15] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [16] org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration
  [17] org.codehaus.janino.Java$MemberClassDeclaration.accept
  [18] org.codehaus.janino.UnitCompiler.compile
  [19] org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes
  [20] org.codehaus.janino.UnitCompiler.compile2
  [21] org.codehaus.janino.UnitCompiler.compile2
  [22] org.codehaus.janino.UnitCompiler.access$400
  [23] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [24] org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration
  [25] org.codehaus.janino.Java$PackageMemberClassDeclaration.accept
  [26] org.codehaus.janino.UnitCompiler.compile
  [27] org.codehaus.janino.UnitCompiler.compileUnit
  [28] org.codehaus.janino.SimpleCompiler.cook
  [29] org.codehaus.janino.SimpleCompiler.compileToClassLoader
  [30] org.codehaus.janino.ClassBodyEvaluator.compileToClass
  [31] org.codehaus.janino.ClassBodyEvaluator.cook
  [32] org.codehaus.janino.SimpleCompiler.cook
  [33] org.codehaus.commons.compiler.Cookable.cook
  [34] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile
  [35] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [36] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load
  [37] org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture
  [38] org.spark_project.guava.cache.LocalCache$Segment.loadSync
  [39] org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad
  [40] org.spark_project.guava.cache.LocalCache$Segment.get
  [41] org.spark_project.guava.cache.LocalCache.get
  [42] org.spark_project.guava.cache.LocalCache.getOrLoad
  [43] org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get
  [44] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile
  [45] org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1
  [46] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [47] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [48] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [49] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [50] org.apache.spark.rdd.RDDOperationScope$.withScope
  [51] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [52] org.apache.spark.sql.execution.SparkPlan.execute
  [53] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [54] org.apache.spark.sql.execution.SparkPlan.executeTake
  [55] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [56] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [57] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [58] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [59] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [60] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [61] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [62] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [63] org.apache.spark.sql.Dataset.withAction
  [64] org.apache.spark.sql.Dataset.head
  [65] org.apache.spark.sql.Dataset.take
  [66] org.apache.spark.sql.Dataset.getRows
  [67] org.apache.spark.sql.Dataset.showString
  [68] sun.reflect.NativeMethodAccessorImpl.invoke0
  [69] sun.reflect.NativeMethodAccessorImpl.invoke
  [70] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [71] java.lang.reflect.Method.invoke
  [72] py4j.reflection.MethodInvoker.invoke
  [73] py4j.reflection.ReflectionEngine.invoke
  [74] py4j.Gateway.invoke
  [75] py4j.commands.AbstractCommand.invokeMethod
  [76] py4j.commands.CallCommand.execute
  [77] py4j.GatewayConnection.run
  [78] java.lang.Thread.run
  [79] [tid=16146]

--- 1551158167100537 us
  [ 0] [tid=16146]

--- 1551158167203300 us
  [ 0] CodeHeap::find_start(void*) const
  [ 1] CodeCache::find_blob(void*)
  [ 2] frame::sender(RegisterMap*) const
  [ 3] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle, Thread*)
  [ 4] java_lang_Throwable::fill_in_stack_trace(Handle, methodHandle)
  [ 5] JVM_FillInStackTrace
  [ 6] Java_java_lang_Throwable_fillInStackTrace
  [ 7] java.lang.Throwable.fillInStackTrace
  [ 8] java.lang.Throwable.fillInStackTrace
  [ 9] java.lang.Throwable.<init>
  [10] java.lang.Exception.<init>
  [11] java.lang.ReflectiveOperationException.<init>
  [12] java.lang.NoSuchFieldException.<init>
  [13] java.lang.Class.getDeclaredField
  [14] java.io.ObjectStreamClass.getDeclaredSerialFields
  [15] java.io.ObjectStreamClass.getSerialFields
  [16] java.io.ObjectStreamClass.access$800
  [17] java.io.ObjectStreamClass$3.run
  [18] java.io.ObjectStreamClass$3.run
  [19] java.security.AccessController.doPrivileged
  [20] java.io.ObjectStreamClass.<init>
  [21] java.io.ObjectStreamClass.lookup
  [22] java.io.ObjectOutputStream.writeObject0
  [23] java.io.ObjectOutputStream.defaultWriteFields
  [24] java.io.ObjectOutputStream.writeSerialData
  [25] java.io.ObjectOutputStream.writeOrdinaryObject
  [26] java.io.ObjectOutputStream.writeObject0
  [27] java.io.ObjectOutputStream.defaultWriteFields
  [28] java.io.ObjectOutputStream.writeSerialData
  [29] java.io.ObjectOutputStream.writeOrdinaryObject
  [30] java.io.ObjectOutputStream.writeObject0
  [31] java.io.ObjectOutputStream.defaultWriteFields
  [32] java.io.ObjectOutputStream.writeSerialData
  [33] java.io.ObjectOutputStream.writeOrdinaryObject
  [34] java.io.ObjectOutputStream.writeObject0
  [35] java.io.ObjectOutputStream.defaultWriteFields
  [36] java.io.ObjectOutputStream.writeSerialData
  [37] java.io.ObjectOutputStream.writeOrdinaryObject
  [38] java.io.ObjectOutputStream.writeObject0
  [39] java.io.ObjectOutputStream.defaultWriteFields
  [40] java.io.ObjectOutputStream.writeSerialData
  [41] java.io.ObjectOutputStream.writeOrdinaryObject
  [42] java.io.ObjectOutputStream.writeObject0
  [43] java.io.ObjectOutputStream.writeArray
  [44] java.io.ObjectOutputStream.writeObject0
  [45] java.io.ObjectOutputStream.defaultWriteFields
  [46] java.io.ObjectOutputStream.writeSerialData
  [47] java.io.ObjectOutputStream.writeOrdinaryObject
  [48] java.io.ObjectOutputStream.writeObject0
  [49] java.io.ObjectOutputStream.writeObject
  [50] org.apache.spark.serializer.JavaSerializationStream.writeObject
  [51] org.apache.spark.serializer.JavaSerializerInstance.serialize
  [52] org.apache.spark.util.ClosureCleaner$.ensureSerializable
  [53] org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean
  [54] org.apache.spark.util.ClosureCleaner$.clean
  [55] org.apache.spark.SparkContext.clean
  [56] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply
  [57] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply
  [58] org.apache.spark.rdd.RDDOperationScope$.withScope
  [59] org.apache.spark.rdd.RDDOperationScope$.withScope
  [60] org.apache.spark.rdd.RDD.withScope
  [61] org.apache.spark.rdd.RDD.mapPartitionsWithIndex
  [62] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [63] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [64] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [65] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [66] org.apache.spark.rdd.RDDOperationScope$.withScope
  [67] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [68] org.apache.spark.sql.execution.SparkPlan.execute
  [69] org.apache.spark.sql.execution.InputAdapter.doExecute
  [70] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [71] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [72] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [73] org.apache.spark.rdd.RDDOperationScope$.withScope
  [74] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [75] org.apache.spark.sql.execution.SparkPlan.execute
  [76] org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs
  [77] org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute
  [78] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [79] org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply
  [80] org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply
  [81] org.apache.spark.rdd.RDDOperationScope$.withScope
  [82] org.apache.spark.sql.execution.SparkPlan.executeQuery
  [83] org.apache.spark.sql.execution.SparkPlan.execute
  [84] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [85] org.apache.spark.sql.execution.SparkPlan.executeTake
  [86] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [87] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [88] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [89] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [90] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [91] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [92] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [93] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [94] org.apache.spark.sql.Dataset.withAction
  [95] org.apache.spark.sql.Dataset.head
  [96] org.apache.spark.sql.Dataset.take
  [97] org.apache.spark.sql.Dataset.getRows
  [98] org.apache.spark.sql.Dataset.showString
  [99] sun.reflect.NativeMethodAccessorImpl.invoke0
  [100] sun.reflect.NativeMethodAccessorImpl.invoke
  [101] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [102] java.lang.reflect.Method.invoke
  [103] py4j.reflection.MethodInvoker.invoke
  [104] py4j.reflection.ReflectionEngine.invoke
  [105] py4j.Gateway.invoke
  [106] py4j.commands.AbstractCommand.invokeMethod
  [107] py4j.commands.CallCommand.execute
  [108] py4j.GatewayConnection.run
  [109] java.lang.Thread.run
  [110] [tid=16146]

--- 1551158167306829 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1.apply
  [10] org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1.apply
  [11] org.apache.spark.rdd.RDDOperationScope$.withScope
  [12] org.apache.spark.rdd.RDDOperationScope$.withScope
  [13] org.apache.spark.rdd.RDD.withScope
  [14] org.apache.spark.rdd.RDD.mapPartitionsInternal
  [15] org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd
  [16] org.apache.spark.sql.execution.SparkPlan.executeTake
  [17] org.apache.spark.sql.execution.CollectLimitExec.executeCollect
  [18] org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan
  [19] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [20] org.apache.spark.sql.Dataset$$anonfun$head$1.apply
  [21] org.apache.spark.sql.Dataset$$anonfun$53.apply
  [22] org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply
  [23] org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated
  [24] org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId
  [25] org.apache.spark.sql.Dataset.withAction
  [26] org.apache.spark.sql.Dataset.head
  [27] org.apache.spark.sql.Dataset.take
  [28] org.apache.spark.sql.Dataset.getRows
  [29] org.apache.spark.sql.Dataset.showString
  [30] sun.reflect.NativeMethodAccessorImpl.invoke0
  [31] sun.reflect.NativeMethodAccessorImpl.invoke
  [32] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [33] java.lang.reflect.Method.invoke
  [34] py4j.reflection.MethodInvoker.invoke
  [35] py4j.reflection.ReflectionEngine.invoke
  [36] py4j.Gateway.invoke
  [37] py4j.commands.AbstractCommand.invokeMethod
  [38] py4j.commands.CallCommand.execute
  [39] py4j.GatewayConnection.run
  [40] java.lang.Thread.run
  [41] [tid=16146]

--- 1551158167362203 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [ 9] org.apache.spark.MapOutputTrackerMaster.getNumAvailableOutputs
  [10] org.apache.spark.scheduler.ShuffleMapStage.numAvailableOutputs
  [11] org.apache.spark.scheduler.ShuffleMapStage.isAvailable
  [12] org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply
  [13] org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply
  [14] scala.collection.immutable.List.foreach
  [15] org.apache.spark.scheduler.DAGScheduler.visit$1
  [16] org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getMissingParentStages
  [17] org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$6.apply
  [18] org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$6.apply
  [19] org.apache.spark.internal.Logging$class.logInfo
  [20] org.apache.spark.scheduler.DAGScheduler.logInfo
  [21] org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted
  [22] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive
  [23] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [24] org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive
  [25] org.apache.spark.util.EventLoop$$anon$1.run
  [26] [tid=16222]

--- 1551158167378658 us
  [ 0] org.json4s.jackson.JValueSerializer.serialize
  [ 1] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [ 2] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [ 3] scala.collection.immutable.List.foreach
  [ 4] org.json4s.jackson.JValueSerializer.serialize
  [ 5] org.json4s.jackson.JValueSerializer$$anonfun$serialize$2.apply
  [ 6] org.json4s.jackson.JValueSerializer$$anonfun$serialize$2.apply
  [ 7] scala.collection.immutable.List.foreach
  [ 8] org.json4s.jackson.JValueSerializer.serialize
  [ 9] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [10] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [11] scala.collection.immutable.List.foreach
  [12] org.json4s.jackson.JValueSerializer.serialize
  [13] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [14] org.json4s.jackson.JValueSerializer$$anonfun$serialize$6.apply
  [15] scala.collection.immutable.List.foreach
  [16] org.json4s.jackson.JValueSerializer.serialize
  [17] org.json4s.jackson.JValueSerializer.serialize
  [18] com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue
  [19] com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue
  [20] com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString
  [21] org.json4s.jackson.JsonMethods$class.compact
  [22] org.json4s.jackson.JsonMethods$.compact
  [23] org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$1.apply
  [24] org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$1.apply
  [25] scala.Option.foreach
  [26] org.apache.spark.scheduler.EventLoggingListener.logEvent
  [27] org.apache.spark.scheduler.EventLoggingListener.onStageSubmitted
  [28] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [29] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [30] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [31] org.apache.spark.util.ListenerBus$class.postToAll
  [32] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [33] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [34] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [36] scala.util.DynamicVariable.withValue
  [37] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [38] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [39] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [40] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [41] [tid=16262]

--- 1551158174610298 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto.internalGetFieldAccessorTable
  [13] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [14] com.google.protobuf.GeneratedMessage.getAllFields
  [15] com.google.protobuf.TextFormat$Printer.print
  [16] com.google.protobuf.TextFormat$Printer.access$400
  [17] com.google.protobuf.TextFormat.shortDebugString
  [18] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [19] com.sun.proxy.$Proxy13.renewLease
  [20] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [21] sun.reflect.NativeMethodAccessorImpl.invoke0
  [22] sun.reflect.NativeMethodAccessorImpl.invoke
  [23] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [24] java.lang.reflect.Method.invoke
  [25] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [26] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [27] com.sun.proxy.$Proxy14.renewLease
  [28] org.apache.hadoop.hdfs.DFSClient.renewLease
  [29] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [30] org.apache.hadoop.hdfs.LeaseRenewer.run
  [31] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [32] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [33] java.lang.Thread.run
  [34] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16256]

--- 1551158174711705 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>
  [13] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto.internalGetFieldAccessorTable
  [14] com.google.protobuf.GeneratedMessage.getAllFieldsMutable
  [15] com.google.protobuf.GeneratedMessage.getAllFields
  [16] com.google.protobuf.TextFormat$Printer.print
  [17] com.google.protobuf.TextFormat$Printer.access$400
  [18] com.google.protobuf.TextFormat.shortDebugString
  [19] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [20] com.sun.proxy.$Proxy13.renewLease
  [21] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [22] sun.reflect.NativeMethodAccessorImpl.invoke0
  [23] sun.reflect.NativeMethodAccessorImpl.invoke
  [24] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [25] java.lang.reflect.Method.invoke
  [26] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [27] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [28] com.sun.proxy.$Proxy14.renewLease
  [29] org.apache.hadoop.hdfs.DFSClient.renewLease
  [30] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [31] org.apache.hadoop.hdfs.LeaseRenewer.run
  [32] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [33] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [34] java.lang.Thread.run
  [35] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16256]

--- 1551158174815711 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class(Thread*)
  [ 7] get_class_declared_methods_helper(JNIEnv_*, _jclass*, unsigned char, bool, Klass*, Thread*)
  [ 8] JVM_GetClassDeclaredMethods
  [ 9] java.lang.Class.getDeclaredMethods0
  [10] java.lang.Class.privateGetDeclaredMethods
  [11] java.lang.Class.privateGetMethodRecursive
  [12] java.lang.Class.getMethod0
  [13] java.lang.Class.getMethod
  [14] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.getReturnProtoType
  [15] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke
  [16] com.sun.proxy.$Proxy13.renewLease
  [17] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease
  [18] sun.reflect.NativeMethodAccessorImpl.invoke0
  [19] sun.reflect.NativeMethodAccessorImpl.invoke
  [20] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [21] java.lang.reflect.Method.invoke
  [22] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [23] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [24] com.sun.proxy.$Proxy14.renewLease
  [25] org.apache.hadoop.hdfs.DFSClient.renewLease
  [26] org.apache.hadoop.hdfs.LeaseRenewer.renew
  [27] org.apache.hadoop.hdfs.LeaseRenewer.run
  [28] org.apache.hadoop.hdfs.LeaseRenewer.access$700
  [29] org.apache.hadoop.hdfs.LeaseRenewer$1.run
  [30] java.lang.Thread.run
  [31] [LeaseRenewer:ec2-user@3.81.95.226:9000 tid=16256]

--- 1551158201343981 us
  [ 0] sun.nio.ch.SocketChannelImpl.readerCleanup
  [ 1] sun.nio.ch.SocketChannelImpl.read
  [ 2] io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes
  [ 3] io.netty.buffer.AbstractByteBuf.writeBytes
  [ 4] io.netty.channel.socket.nio.NioSocketChannel.doReadBytes
  [ 5] io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read
  [ 6] io.netty.channel.nio.NioEventLoop.processSelectedKey
  [ 7] io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized
  [ 8] io.netty.channel.nio.NioEventLoop.processSelectedKeys
  [ 9] io.netty.channel.nio.NioEventLoop.run
  [10] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [11] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [12] java.lang.Thread.run
  [13] [rpc-server-3-4 tid=16249]

--- 1551158206993879 us
  [ 0] [tid=16260]

--- 1551158237852119 us
  [ 0] frame::sender(RegisterMap*) const
  [ 1] JavaThread::nmethods_do(CodeBlobClosure*) [clone .part.89]
  [ 2] Threads::nmethods_do(CodeBlobClosure*)
  [ 3] NMethodSweeper::mark_active_nmethods()
  [ 4] SafepointSynchronize::do_cleanup_tasks()
  [ 5] SafepointSynchronize::begin()
  [ 6] VMThread::loop()
  [ 7] VMThread::run()
  [ 8] java_start(Thread*)
  [ 9] start_thread
  [10] [tid=16093]

--- 1551158326992611 us
  [ 0] __lock_text_start_[k]
  [ 1] try_to_wake_up_[k]
  [ 2] wake_up_q_[k]
  [ 3] futex_wake_[k]
  [ 4] do_futex_[k]
  [ 5] sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __pthread_cond_signal
  [ 9] sun.misc.Unsafe.unpark
  [10] java.util.concurrent.locks.LockSupport.unpark
  [11] java.util.concurrent.locks.AbstractQueuedSynchronizer.unparkSuccessor
  [12] java.util.concurrent.locks.AbstractQueuedSynchronizer.doReleaseShared
  [13] java.util.concurrent.locks.AbstractQueuedSynchronizer.releaseShared
  [14] scala.concurrent.impl.Promise$CompletionLatch.apply
  [15] scala.concurrent.impl.Promise$CompletionLatch.apply
  [16] scala.concurrent.impl.CallbackRunnable.run
  [17] scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1
  [18] scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp
  [19] scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply
  [20] scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply
  [21] scala.concurrent.BlockContext$.withBlockContext
  [22] scala.concurrent.BatchingExecutor$Batch.run
  [23] scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute
  [24] scala.concurrent.BatchingExecutor$class.execute
  [25] scala.concurrent.Future$InternalCallbackExecutor$.execute
  [26] scala.concurrent.impl.CallbackRunnable.executeWithValue
  [27] scala.concurrent.impl.Promise$DefaultPromise.tryComplete
  [28] scala.concurrent.Promise$class.trySuccess
  [29] scala.concurrent.impl.Promise$DefaultPromise.trySuccess
  [30] org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onSuccess$1
  [31] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$ask$1.apply
  [32] org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$ask$1.apply
  [33] scala.concurrent.impl.CallbackRunnable.run
  [34] org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute
  [35] scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute
  [36] scala.concurrent.impl.CallbackRunnable.executeWithValue
  [37] scala.concurrent.impl.Promise$DefaultPromise.tryComplete
  [38] scala.concurrent.Promise$class.complete
  [39] scala.concurrent.impl.Promise$DefaultPromise.complete
  [40] scala.concurrent.Promise$class.success
  [41] scala.concurrent.impl.Promise$DefaultPromise.success
  [42] org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send
  [43] org.apache.spark.rpc.netty.NettyRpcCallContext.reply
  [44] org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse
  [45] org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp
  [46] org.apache.spark.rpc.netty.Inbox.safelyCall
  [47] org.apache.spark.rpc.netty.Inbox.process
  [48] org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run
  [49] java.util.concurrent.ThreadPoolExecutor.runWorker
  [50] java.util.concurrent.ThreadPoolExecutor$Worker.run
  [51] java.lang.Thread.run
  [52] [tid=16165]

--- 1551158422587785 us
  [ 0] sun.nio.ch.SocketChannelImpl.writerCleanup
  [ 1] sun.nio.ch.SocketChannelImpl.write
  [ 2] org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf
  [ 3] org.apache.spark.network.protocol.MessageWithHeader.transferTo
  [ 4] io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion
  [ 5] io.netty.channel.nio.AbstractNioByteChannel.doWrite
  [ 6] io.netty.channel.socket.nio.NioSocketChannel.doWrite
  [ 7] io.netty.channel.AbstractChannel$AbstractUnsafe.flush0
  [ 8] io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0
  [ 9] io.netty.channel.AbstractChannel$AbstractUnsafe.flush
  [10] io.netty.channel.DefaultChannelPipeline$HeadContext.flush
  [11] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [12] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [13] io.netty.channel.AbstractChannelHandlerContext.flush
  [14] io.netty.channel.ChannelOutboundHandlerAdapter.flush
  [15] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [16] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [17] io.netty.channel.AbstractChannelHandlerContext.flush
  [18] io.netty.channel.ChannelDuplexHandler.flush
  [19] io.netty.channel.AbstractChannelHandlerContext.invokeFlush0
  [20] io.netty.channel.AbstractChannelHandlerContext.invokeFlush
  [21] io.netty.channel.AbstractChannelHandlerContext.access$1500
  [22] io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write
  [23] io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run
  [24] io.netty.util.concurrent.AbstractEventExecutor.safeExecute
  [25] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks
  [26] io.netty.channel.nio.NioEventLoop.run
  [27] io.netty.util.concurrent.SingleThreadEventExecutor$5.run
  [28] io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run
  [29] java.lang.Thread.run
  [30] [rpc-server-3-5 tid=16252]

--- 1551158542396021 us
  [ 0] ConstMethod::checked_exceptions_start() const
  [ 1] ClassFileParser::parse_methods(bool, AccessFlags*, bool*, bool*, Thread*)
  [ 2] ClassFileParser::parseClassFile(Symbol*, ClassLoaderData*, Handle, KlassHandle, GrowableArray<Handle>*, TempNewSymbol&, bool, Thread*)
  [ 3] ClassLoader::load_classfile(Symbol*, Thread*)
  [ 4] SystemDictionary::load_instance_class(Symbol*, Handle, Thread*)
  [ 5] SystemDictionary::resolve_instance_class_or_null(Symbol*, Handle, Handle, Thread*)
  [ 6] SystemDictionary::resolve_or_fail(Symbol*, Handle, Handle, bool, Thread*)
  [ 7] ConstantPool::klass_at_impl(constantPoolHandle, int, Thread*)
  [ 8] ConstantPool::klass_ref_at(int, Thread*)
  [ 9] LinkResolver::resolve_field_access(fieldDescriptor&, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [10] InterpreterRuntime::resolve_get_put(JavaThread*, Bytecodes::Code)
  [11] java.text.DecimalFormat.subformat
  [12] java.text.DecimalFormat.format
  [13] java.text.DecimalFormat.format
  [14] java.text.NumberFormat.format
  [15] org.apache.spark.sql.execution.metric.SQLMetrics$.stringValue
  [16] org.apache.spark.sql.execution.ui.SQLAppStatusListener$$anonfun$12.apply
  [17] org.apache.spark.sql.execution.ui.SQLAppStatusListener$$anonfun$12.apply
  [18] scala.collection.TraversableLike$$anonfun$map$1.apply
  [19] scala.collection.TraversableLike$$anonfun$map$1.apply
  [20] scala.collection.immutable.HashMap$HashMap1.foreach
  [21] scala.collection.immutable.HashMap$HashTrieMap.foreach
  [22] scala.collection.TraversableLike$class.map
  [23] scala.collection.AbstractTraversable.map
  [24] org.apache.spark.sql.execution.ui.SQLAppStatusListener.org$apache$spark$sql$execution$ui$SQLAppStatusListener$$aggregateMetrics
  [25] org.apache.spark.sql.execution.ui.SQLAppStatusListener$$anonfun$onExecutionEnd$1.apply
  [26] org.apache.spark.sql.execution.ui.SQLAppStatusListener$$anonfun$onExecutionEnd$1.apply
  [27] scala.Option.foreach
  [28] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onExecutionEnd
  [29] org.apache.spark.sql.execution.ui.SQLAppStatusListener.onOtherEvent
  [30] org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent
  [31] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [32] org.apache.spark.scheduler.AsyncEventQueue.doPostEvent
  [33] org.apache.spark.util.ListenerBus$class.postToAll
  [34] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll
  [35] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp
  [36] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [37] org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply
  [38] scala.util.DynamicVariable.withValue
  [39] org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch
  [40] org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp
  [41] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [42] org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run
  [43] [tid=16260]

--- 1551158542410273 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 7] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 8] InstanceKlass::initialize(Thread*)
  [ 9] InterpreterRuntime::_new(JavaThread*, ConstantPool*, int)
  [10] org.apache.spark.sql.SparkSession.catalog$lzycompute
  [11] org.apache.spark.sql.SparkSession.catalog
  [12] sun.reflect.NativeMethodAccessorImpl.invoke0
  [13] sun.reflect.NativeMethodAccessorImpl.invoke
  [14] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [15] java.lang.reflect.Method.invoke
  [16] py4j.reflection.MethodInvoker.invoke
  [17] py4j.reflection.ReflectionEngine.invoke
  [18] py4j.Gateway.invoke
  [19] py4j.commands.AbstractCommand.invokeMethod
  [20] py4j.commands.CallCommand.execute
  [21] py4j.GatewayConnection.run
  [22] java.lang.Thread.run
  [23] [tid=16146]

--- 1551158542481246 us
  [ 0] jshort_arraycopy
  [ 1] java.lang.Object.<init>
  [ 2] scala.collection.AbstractIterator.<init>
  [ 3] scala.collection.LinearSeqLike$$anon$1.<init>
  [ 4] scala.collection.LinearSeqLike$class.iterator
  [ 5] scala.collection.immutable.List.iterator
  [ 6] scala.StringContext.standardInterpolator
  [ 7] scala.StringContext.s
  [ 8] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$unpersist$1.apply
  [ 9] org.apache.spark.broadcast.TorrentBroadcast$$anonfun$unpersist$1.apply
  [10] org.apache.spark.internal.Logging$class.logDebug
  [11] org.apache.spark.broadcast.TorrentBroadcast$.logDebug
  [12] org.apache.spark.broadcast.TorrentBroadcast$.unpersist
  [13] org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast
  [14] org.apache.spark.broadcast.BroadcastManager.unbroadcast
  [15] org.apache.spark.ContextCleaner.doCleanupBroadcast
  [16] org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$1.apply
  [17] org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$1.apply
  [18] scala.Option.foreach
  [19] org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp
  [20] org.apache.spark.util.Utils$.tryOrStopSparkContext
  [21] org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning
  [22] org.apache.spark.ContextCleaner$$anon$1.run
  [23] [tid=16258]

--- 1551158542548660 us
  [ 0] java.util.Vector.size
  [ 1] org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders
  [ 2] org.apache.log4j.Category.callAppenders
  [ 3] org.apache.log4j.Category.forcedLog
  [ 4] org.apache.log4j.Category.log
  [ 5] org.slf4j.impl.Log4jLoggerAdapter.log
  [ 6] org.apache.commons.logging.impl.SLF4JLocationAwareLog.debug
  [ 7] org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run
  [ 8] [tid=16273]

--- 1551158542606313 us
  [ 0] Method::make_jmethod_id(ClassLoaderData*, Method*)
  [ 1] InstanceKlass::get_jmethod_id(instanceKlassHandle, methodHandle)
  [ 2] JvmtiEnv::GetClassMethods(oopDesc*, int*, _jmethodID***)
  [ 3] jvmti_GetClassMethods
  [ 4] VM::loadMethodIDs(_jvmtiEnv*, _jclass*)
  [ 5] InstanceKlass::link_class_impl(instanceKlassHandle, bool, Thread*)
  [ 6] InstanceKlass::initialize_impl(instanceKlassHandle, Thread*)
  [ 7] InstanceKlass::initialize(Thread*)
  [ 8] LinkResolver::resolve_static_call(CallInfo&, KlassHandle&, Symbol*, Symbol*, KlassHandle, bool, bool, Thread*)
  [ 9] LinkResolver::resolve_invokestatic(CallInfo&, constantPoolHandle, int, Thread*)
  [10] LinkResolver::resolve_invoke(CallInfo&, Handle, constantPoolHandle, int, Bytecodes::Code, Thread*)
  [11] InterpreterRuntime::resolve_invoke(JavaThread*, Bytecodes::Code)
  [12] org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto.newBuilder
  [13] org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rename
  [14] sun.reflect.NativeMethodAccessorImpl.invoke0
  [15] sun.reflect.NativeMethodAccessorImpl.invoke
  [16] sun.reflect.DelegatingMethodAccessorImpl.invoke
  [17] java.lang.reflect.Method.invoke
  [18] org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod
  [19] org.apache.hadoop.io.retry.RetryInvocationHandler.invoke
  [20] com.sun.proxy.$Proxy14.rename
  [21] org.apache.hadoop.hdfs.DFSClient.rename
  [22] org.apache.hadoop.hdfs.DistributedFileSystem.rename
  [23] org.apache.spark.scheduler.EventLoggingListener.stop
  [24] org.apache.spark.SparkContext$$anonfun$stop$8$$anonfun$apply$mcV$sp$6.apply
  [25] org.apache.spark.SparkContext$$anonfun$stop$8$$anonfun$apply$mcV$sp$6.apply
  [26] scala.Option.foreach
  [27] org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp
  [28] org.apache.spark.util.Utils$.tryLogNonFatalError
  [29] org.apache.spark.SparkContext.stop
  [30] org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp
  [31] org.apache.spark.util.SparkShutdownHook.run
  [32] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [33] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [34] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [35] org.apache.spark.util.Utils$.logUncaughtExceptions
  [36] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp
  [37] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [38] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [39] scala.util.Try$.apply
  [40] org.apache.spark.util.SparkShutdownHookManager.runAll
  [41] org.apache.spark.util.SparkShutdownHookManager$$anon$2.run
  [42] org.apache.hadoop.util.ShutdownHookManager$1.run
  [43] [tid=16633]

--- 1551158542723235 us
  [ 0] dup_fd_[k]
  [ 1] copy_process.part.38_[k]
  [ 2] _do_fork_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __libc_vfork
  [ 6] Java_java_lang_UNIXProcess_forkAndExec
  [ 7] java.lang.UNIXProcess.forkAndExec
  [ 8] java.lang.UNIXProcess.<init>
  [ 9] java.lang.ProcessImpl.start
  [10] java.lang.ProcessBuilder.start
  [11] org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative
  [12] org.apache.spark.network.util.JavaUtils.deleteRecursively
  [13] org.apache.spark.network.util.JavaUtils.deleteRecursively
  [14] org.apache.spark.util.Utils$.deleteRecursively
  [15] org.apache.spark.SparkEnv.stop
  [16] org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp
  [17] org.apache.spark.util.Utils$.tryLogNonFatalError
  [18] org.apache.spark.SparkContext.stop
  [19] org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp
  [20] org.apache.spark.util.SparkShutdownHook.run
  [21] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp
  [22] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [23] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply
  [24] org.apache.spark.util.Utils$.logUncaughtExceptions
  [25] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp
  [26] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [27] org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply
  [28] scala.util.Try$.apply
  [29] org.apache.spark.util.SparkShutdownHookManager.runAll
  [30] org.apache.spark.util.SparkShutdownHookManager$$anon$2.run
  [31] org.apache.hadoop.util.ShutdownHookManager$1.run
  [32] [tid=16633]

--- 1551158543404591 us
  [ 0] [DestroyJavaVM tid=16064]

--- 1551158543504290 us
  [ 0] [DestroyJavaVM tid=16064]

--- 1551158543604010 us
  [ 0] [DestroyJavaVM tid=16064]

--- 1551158543703728 us
  [ 0] [DestroyJavaVM tid=16064]

